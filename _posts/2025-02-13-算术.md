---
title: 高性能计算-第六章-算术
date: 2025-2-13 12:00:00 +0800
categories: [笔记，高性能计算]
tags: [高性能计算]     # TAG names should always be lowercase
math: true
---
[原文](https://en.algorithmica.org/hpc/arithmetic/)

## 序 - 算术

正如我们在本书中提到的那样，了解指令集中的冷门指令可能会带来丰厚的回报，尤其是在 x86 这样的 CISC 平台上，目前它有大约 1000 到 4000 条不同的指令。

这些指令大多与算术相关，要想要高效地使用它们来优化算术操作，需要大量的知识、技能和创造力。因此，在本章中，我们将讨论数字表示及其在数值算法中的应用。

## 一 - 浮点数

使用浮点数来做算术的人，在对浮点数的了解程度上大概构成一个钟形曲线：

- 新手程序员会到处使用浮点数，就好像它是一个具有无限精度的神奇的数据类型一样。
- 然后，程序员发现 `0.1 + 0.2 != 0.3` 或者其它的一些奇怪现象。于是开始认为浮点数在每次计算中都会有随机的误差，而在很长时间内都拒绝了解浮点数的真正结构。
- 然后他们终于振作起来，阅读 IEEE-754 浮点数如何工作的规范，并开始适当地使用它们。

不幸的是，有太多程序员停留在阶段 2 了，对浮点运算产生了各种各样的误解——认为它从根本上说是不精确和不稳定的，而且比整数运算慢。

但这些都只是传言而已。由于有专门的指令，浮点运算通常比整数运算快。而且实数的表示是完全标准化的，在舍入方面遵循简单而确定的规则，允许我们可信地处理计算误差。

事实上，浮点数可靠到有一些高级编程语言根本没有整数类型，比方说 JavaScript，在里面只有 `number` 类型，底层是一个 64-bit 的 `double`。根据浮点数的计算规则，实际上所有在 `-2^53` 到 `2^53` 内的整数都可以被精确表示。所以从程序员的角度看，基本上没有什么使用整数类型的必要。

一个值得注意的例外是，当你需要对数字做逐位操作时，浮点数类型是不支持这样的操作的。在这种情况下，它们需要被转换为整数，这在支持 Javascript 的浏览器中使用得过于频繁，以至于 arm 添加了一个特殊的“FJCVTZS”指令，它代表“浮点 Javascript 转换为带符号的定点数（整数），并向零舍入”（Floating-point Javascript Convert to Signed fixed-point, rounding toward Zero）——这是一个有趣的软件-硬件反馈循环的例子。

不过，只要你不是专门用实数来行来模拟整数运算的 JavaScript 程序员，你大概率还是需要了解一下浮点数的运算规则。

### 实数的表示

如果我们需要处理实数（非整数），我们有一些可选的实现方法。在讨论浮点数前，我们可以先讨论一下可用的替代方案，以及其背后的动机——不然总会用不想用浮点数的人的。

#### 符号表示

第一种、同时也是最麻烦的方法是不存储结果值本身，而是存储它们的代数表达式。

有一个简单的例子。在一些应用中，比方说计算几何，除了加减乘除之外，还需要精确地进行除法，以生成一个有理数。我们可以用两个整数的比率精确表示它：

```cpp
struct r {
    int x, y;
};

r operator+(r a, r b) { return {a.x * b.y + a.y * b.x, a.y * b.y}; }
r operator*(r a, r b) { return {a.x * b.x, a.y * b.y}; }
r operator/(r a, r b) { return {a.x * b.x, a.y * b.y}; }
bool operator<(r a, r b) { return a.x * b.y < b.x * a.y; }
// ...and so on, you get the idea
```

这个分数应该是不可约的，这样就可以产生一个唯一的表示：

```cpp
struct r {
    int x, y;
    r(int x, int y) : x(x), y(y) {
        if (y < 0)
            x = -x, y = -y;
        int g = gcd(x, y);
        x /= g;
        y /= g;
    }
};
```

这就是计算机代数系统（如 WolframAlpha 或 SageMath）的工作原理：它们只对符号表达式进行操作，避免将任何东西计算为实数。

使用这种方法，我们可以获得绝对的精度，当我们只需要有理数的时候，它表现得非常好。但这需要很大的计算成本，因为在通常情况下，我们是需要以某种方式存储整个操作历史，并在每次执行新操作时都考虑它——随着操作历史的增长，这很快变得不可行。

#### 定点数

另一种方法是坚持使用整数，但将它们视为乘了一个固定的常数。这在本质上和为了更精确地测量而改变了测量单位是一样的。

由于有些值不能精确地表示，这使得计算不精确：我们需要将结果四舍五入到最接近的可表示值。

这种方法通常用于财务软件，在这些软件中，我们确实需要一种直接的方法来管理舍入误差，以便最终数字相加。例如，NASDAQ 在其股票清单中使用 $$\frac{1}{10000}$$ 美元作为基本单位，这意味着在所有交易中，逗号后的精确值为 4 位。

```cpp
struct money {
    uint v; // 1/10000th of a dollar
};

std::string to_string(money) {
    return std::format("$${0}.{1:04d}", v / 10000, v % 10000);
}

money operator*(money x, money y) { return {x.v * y.v / 10000}; }
```

除了引入了舍入误差之外，另一个问题是当常数设置错误时，整个定点数很容易变得没有用处。如果正在处理的数字太大，则内部的整数值将溢出，如果数字太小，则会被舍入为零。有趣的是，当 NASDAQ 的股价接近 $$\frac{2^{32} - 1}{10000} = 429,496.7295$$ 时，前一种情况曾经成为纳斯达克的一个问题，无法用无符号 32 位整数容纳。

这个问题使得定点数从根本上不适合需要同时使用小数字和大数字的应用，例如，计算某些物理方程 $$E=mc^2$$。$$m$$ 通常小到原子的量级（$$1.67 \cdot 10^{-27}$$ kg），而 $$c$$ 却是光速（$$3 \cdot 10^9$$ m/s）。

#### 浮点数

在大多数数值应用中，我们主要关心的是相对误差。我们希望我们的计算结果与事实的差异不超过 0.01%，而并不关心 0.01% 具体是多少。

浮点数通过存储一定数量的最高有效数字和数字的数量级来解决这个问题。更准确地说，它们用一个整数（称为有效数或尾数）表示，并使用固定基数的指数（通常是 2 或 10）进行缩放。例如:

$$
1.2345=\underbrace{12345}_{有效数}\times {\underbrace{10}_{基数}}^{\overbrace{-4}^{指数}}
$$

计算机只在固定长度的二进制字节上操作，所以为硬件设计浮点数标准时，我们希望使用固定的二进制格式，其中一些位专用于有效数（用于更高的精度），一些专用于指数（用于更大的范围）。

例如给一种非常基础的实现：

```cpp
struct fp {
    int m; // mantissa
    int e; // exponent
};
```

这样我们既可以使用 $$\pm m\times 2^e$$ 的格式来表示，其中 $$m$$ 和 $$e$$ 都是有界并且可能为负的整数——分别对应负数和小数。这些数字的分布非常不均匀，在 $$[0,1)$$ 之间的数字数量和 $$[1, +\infty)$$ 之间的数字数量几乎一样。

注意到在这种表示下，数字的表示并不是唯一的，例如，有：

$$
1\times 2^0 = 2 \times 2^{-1} = 256 \times 2^{-8} = \dots
$$

这对于某些应用（例如比较或者 hash）可能会有问题。为了解决这个问题，我们可以使用某种约定对这些表示进行规范化。在十进制中，标准形式总是在第一位非零的数字（6.022e23）后面加小数点，对于二进制，我们也可以这样做：

$$
42 = 10101_2 = 1.0101_2 \times 2^5
$$

注意到，根据这个规则，有效数的第一位总是1。显式地存储它是多余的，所以我们就假装它在那里，只存储其他的位。这样有效数对应于 $$[0,1)$$ 范围内的某个有理数，也是其被称为尾数的原因。可表示数的集合现在大致是：

$$
\{\pm (1+m)2^e | m = \frac{x}{2^{32}}, x \in [0, 2^{32}) \}
$$

$$m$$ 现在又变成非负值了，所以为了表示负数，我们还需要增加一个符号位：

```cpp
struct fp {
    bool s;     // sign: "0" for "+", "1" for "-" 
    unsigned m; // mantissa
    int e;      // exponent
};
```

现在，让我们尝试使用这个浮点数来实现一些算术运算——例如乘法。使用新的形式，结果应该是：

$$
\begin{aligned}
c =& a \cdot b \\
=& (s_a(1+m_a)2^{e_a})(s_b(1+m_b)2^{e_b}) \\
=& \underbrace{s_a s_b}_{s_c} \cdot (1+ \underbrace{m_a + m_b + m_a m_b}_{m_c}) \cdot 2^{\overbrace{e_a + e_b}^{e_c}}
\end{aligned}
$$

这样的分组计算看上去很容易，但有一些小细节：

- 新的尾数现在在 $$[0,3)$$ 的范围内。我们需要检查它是否大于 1，并通过以下公式将其规范化表示： 
$$
1+m = (1+1) + (m -1) = (1 + \frac{m-1}{2})\cdot 2
$$

- 由于精度不足，结果数字可能（且很可能）无法精确表示。我们需要两倍的比特位数来处理 $$m_a m_b$$ 项，而我们能做的最好事情是将其舍入到最接近的可表示数字。

由于我们需要一些额外的位来正确处理尾数溢出问题，因此我们将从 $$m$$ 中保留一位，从而将其限制在 $$[0, 2^{31})$$ 范围内。

```cpp
fp operator*(fp a, fp b) {
    fp c;
    c.s = a.s ^ b.s;
    c.e = a.e + b.e;
    
    uint64_t x = a.m, y = b.m; // 转换到更宽的类型
    uint64_t m = (x << 31) + (y << 31) + x * y; // 62 或 63 位的结果
    if (m & (1<<62)) { // 检查是否溢出（大于 1）
        m -= (1<<62); // m -= 1;
        m >>= 1;
        c.e++;
    }
    m += (1<<30); // 加 0.5 从而和之后的 floor 操作共同达成四舍五入的效果。
    c.m = m >> 31;
    
    return c;
}
```

许多需要更高精度的应用程序以类似的方式实现 _软件浮点运算_。但当然，我们不希望每次将两个实数相乘时都执行这段代码编译出的约 10 条指令序列，因此在现代 CPU 中，浮点运算是在硬件中实现的——由于其复杂性，通常作为单独的协处理器。

x86 的浮点单元（通常称为 x87）具有单独的寄存器及其自己的小型指令集，支持内存操作、基本算术、三角函数以及一些常见操作，如对数、指数和平方根。为了使这些操作能够正确协同工作，我们需要澄清浮点数表示的一些额外细节——这些将在下一节中讨论。

## 二 - IEEE 754

在我们实现自己的浮点数时，其实省略了许多重要的细节：

- 我们应该用多少位来表示尾数和指数？
- 符号位是 `0` 的时候表示正数，还是反过来比较好？
- 浮点数总共占用多少 bits？
- 我们要怎么表示 0？
- 舍入规则具体是什么？
- 除 0 的时候会发生什么？
- 对负数开根会发生什么？
- 超出最大表示上界时会发生什么？
- 我们能否检测出上述三种情况发生了？

大多数早期的计算机都不支持浮点运算，当供应商开始添加浮点协处理器时，他们对这些问题的答案的看法各有不同。不同的实现使得可靠和可移植地使用浮点运算变得异常困难——特别是对于那些开发编译器的人。

到了 1985 年，电气和电子工程师协会发布了一个标准（称为 IEEE 754），该标准提供了浮点数应该如何工作的正式规范，该标准很快被供应商采用，几乎在所有通用计算机中使用。

### 浮点数的格式

和我们手动实现的浮点数一样，硬件浮点数也使用一 bit 作为符号位，以及一些指数位和尾数位。例如，标准的 32-bit `float` 最高位是符号位，后 8 位是指数位，剩下的 23 位是尾数位。

![IEEE 754 的浮点数标准](/assets/float.svg)
_IEEE 754 的浮点数标准（在 blog 左下角把背景颜色切换成白色可以看得更清楚）_

使用这种格式的一个原因是，你可以用无符号整数的比较器来比较两个浮点数的大小，除了可能在其中一个数字为负时翻转一些位。

出于同样的原因，指数位实际上是有偏置的：实际值比存储的无符号整数小 127。在上图的例子中：

$$
(-1)^0 \times 2^{01111100_2 - 127} \times (1 + 2^{-2}) = 2^{-3} \times 1.25 = 0.15625
$$

IEEE 754 和随后的一些标准定义了不止一种，而是几种大小不同的表示，值得关注的一些是：

| 类型      | 符号位 | 指数位 | 尾数位 | 总位数 | 近似的有效数字位数（10 进制下） |
| --------- | :----: | :----: | :----: | :----: | :-----------------------------: |
| single    |   1    |   8    |   23   |   32   |              ~7.2               |
| double    |   1    |   11   |   52   |   64   |              ~15.9              |
| half      |   1    |   5    |   10   |   16   |              ~3.3               |
| extended  |   1    |   15   |   64   |   80   |              ~19.2              |
| quadruple |   1    |   15   |  112   |  128   |              ~34.0              |
| bfloat16  |   1    |   8    |   7    |   16   |              ~2.3               |

不同的芯片对它们的支持并不相同：

- 大部分 CPU 都支持单双精度类型，在 C 语言中体现为 `float` 和 `double` 类型。
- Extended 类型是 x86 独有的，在 C 语言中体现为 `long double` 类型，同时 `long double` 在 Arm CPU 上会只是双精度类型。64 位的尾数可以准确地表达每一个 `long long` 整数。类似的还有一个 40 位的浮点数类型，拥有 32 位尾数。
- Quadruple 和 256 位的 octuple 格式仅用于特定的科学计算，一般不受硬件的支持。
- Half-precision 只支持一小部分操作，通常用于机器学习等应用程序，尤其是神经网络，因为它们倾向于执行大量的计算，却不需要很高的精度。
- Half-precision 正逐渐被 bfloat 所取代，它交换了 3 个尾数位，变得与单精度有相同的表示范围，从而实现与单精度的互操作性。它主要被专用硬件所采用：tpu、fgpa 和 gpu。bfloat 名字的意思是“Brain float”。

低精度类型占用更少的内存带宽来移动它们，并且通常也消耗更少的周期来操作，这就是为什么在误差允许的情况下，它们会更受欢迎一些。

深度学习，作为一个非常出名的计算密集型领域，创造了对低精度矩阵乘法的巨大需求。这导致制造商甚至会开发单独的硬件，或者至少添加专门的指令来支持这些类型的计算——值得注意的是，谷歌开发了一种名为 TPU（张量处理单元）的定制芯片，专门用于给 128x128 的 bfloat 矩阵做乘法，而 NVIDIA 在其所有较新的 gpu 上都添加了“张量核”，能够一次执行 4x4 矩阵乘法。

除了它们的大小之外，所有浮点类型的大多数行为都是相同的，现在我们将讨论这一点。

### 处理边际情况

整数算术处理极端情况时（如除零）的默认方式是崩溃。

但有时，软件崩溃会导致真正的物理崩溃。1996 年，Ariane 5（欧空局用来将物体送入近地轨道的太空运载火箭）的首次飞行以灾难性的爆炸告终，原因是算术错误导致中断的策略。在这种情况下，浮点数到整数转换溢出导致导航系统认为它偏离了轨道并进行了大幅修正，最终导致了 2 亿美元火箭的解体。

有一种方法可以优雅地处理像这样的极端情况：硬件中断。当出现异常时，CPU 会做下面的事情：

- 中断目前运行的程序；
- 将所有相关信息打包到一个称为“中断向量”的数据结构中；
- 将它传递给操作系统，如果可能的话，操作系统反过来调用处理代码（“try-except”块），否则将终止程序。

这是一个非常复杂的机制，值得专门写一篇文章。但由于这是一本关于性能的书，我们只需要知道它相当慢，在诸如导航火箭之类的实时系统中并不理想。

### NaN，0，无穷

浮点运算通常用来处理嘈杂的真实数据。在这里的异常比整数情况下的异常要常见得多，因此，处理它们的默认行为是不同的。浮点数运算结果会被替换为一个特殊的值，而不会中断程序的执行（除非程序员明确地希望它这样做）。

这种值的第一种类型是两个无穷大：一个正无穷大和一个负无穷大。如果计算的结果不能在可表示的范围内，则会生成它们，并且在算术中也会这样处理它们。

$$
\begin{aligned}
-\infty < x < \infty \\
\infty + x = \infty \\
x / \infty = 0
\end{aligned}
$$

如果除零的时候会发生什么？事实上，在浮点数中是明确的，因为浮点数实际上有两个 0：一个正零，一个负零：

$$
\begin{aligned}
\frac{1}{+0}=+\infty \\
\frac{1}{-0}=-\infty
\end{aligned}
$$

一个有趣的事实是，`x+0.0` 不能被简化为 `x`，而 `x+(-0.0)` 却可以。因此从这方面来说，使用负零作为浮点数的初始化值可以帮助编译器更好地进行优化。`+0.0` 不能优化的主要原因是 IEEE 规定 `+0.0 + -0.0 == +0.0`，并不是加法的单位元，优化掉会使得 `x == -0.0` 时的结果是不正确的。两个零的存在经常引起这样的头痛问题——好消息是，如果想禁用此行为，可以向编译器传递 `fno-signed-zeros`。

零的编码方式是将除了符号位外的所有位设置为 0。无穷大的编码方式是将其所有指数位设置为 1，将所有尾数位设置为 0，并用符号位区分正无穷大和负无穷大。

另一种类型是“Not a Number”（NaN），它是由于数学上错误的操作而产生的：

$$
\log(-1), \arccos(1.01),\infty-\infty,-\infty+\infty,0\times \infty, 0 \div 0, \infty \div \infty
$$

NaN 有两种类型：信号型 NaN（signaling NaN）和静默型 NaN（quiet NaN）。信号型 NaN 会引发异常标志，根据 FPU 的配置，可能会导致硬件中断，而静默型 NaN 则会在几乎任何算术操作中传播，产生更多的 NaN。

在二进制中，两种 NaN 的指数位都设置为全 1，而尾数部分则不全为零（和无穷大区分开来）。请注意，NaN 有许多有效的编码方式。

### 扩展阅读

如果还想深入了解，可以阅读经典的“[每个计算机科学家都应该知道的浮点算术](https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf)”和[介绍 Grisu3 的论文](https://www.cs.tufts.edu/~nr/cs257/archive/florian-loitsch/printf.pdf)，这是当前用于打印浮点数的最新技术。

## 三 - 舍入误差

硬件中浮点数的舍入方式非常简单：当且仅当操作结果无法精确表示时，它才会发生，并且默认情况下会舍入到最近的可表示数字（在等距离的情况下，更喜欢以零结尾的数字（偶数））。

考虑如下代码块：

```cpp
float x = 0;
for (int i = 0; i < (1 << 25); i++)
    x++;
printf("%f\n", x);
```

实际上并不会输出 $$2^{25} = 33554432$$，而是会输出 $$2^{24} = 16777216$$。

当重复增加一个浮点数 $$x$$ 时，最终 $$x$$ 总会太大，以至于 $$(x+1)$$ 被舍入到 $$x$$。对于上面的问题，第一个数字就是 $$2^{24}$$，因为

$$
2^{24} + 1 = 2^{24} \times 1.\underbrace{0\dots0}_{\times 23}1
$$

和 $$2^{24}$$ 与 $$(2^{24} + 1)$$ 的距离完全相同，由于之前的规则，就会舍入到 $$2^{24}$$。

### 舍入误差和操作顺序

尽管浮点数在代数上可以按照计算规则随意调换顺序，但是在实际中浮点数运算的结果却取决于操作的顺序。

例如，浮点数的加法和乘法运算在数学上是有交换律和结合律的，但是它们的舍入误差却没有。对于三个浮点数 $$x,y,z$$，$$(x+y+z)$$ 的结果取决于求和的顺序。同样的，非交换性适用于大多数（或者全部）浮点运算。

编译器不能产生不符合规范的结果，因此这些细微的细节禁用了一些涉及重新排列算术操作的潜在优化。我们可以通过在 GCC 和 Clang 中启用 `-ffast-math` 标志来关闭这种严格的合规性。如果我们添加它并重新编译上面的代码片段，代码会运行得更快，并且也恰好输出了正确的结果 $$33554432$$（但是我们需要注意到编译器也有可能选择了一条精度较低的计算路径）。

### 舍入模式

除了默认模式（也被称为 _银行家舍入法_），我们还可以用其它 4 种模式设置舍入逻辑：

- 向最近舍入，但是距离相同的时候向远离零的方向舍入。
- 向上舍入，朝向 $$+\infty$$，负数的舍入结果永远向零舍入。
- 向下舍入，朝向 $$-\infty$$，负数的舍入结果永远远离零舍入。
- 向零舍入，直接截断二进制就会产生这个效果。

例如，如果在运行上述循环之前调用 `fesetround(FE_UPWARD)`，它输出的不是 $$2^{24}$$，但也不是 $$2^{25}$$，而是 $$67108864 = 2^{26}$$。这是因为当我们达到 $$2^{24}$$ 时，$$(x + 1)$$ 开始舍入到下一个最近的可表示数字 $$(x + 2)$$，我们只需要一半的迭代次数就能达到 $$2^{25}$$。之后 $$(x + 1)$$ 向上舍入到 $$(x + 4)$$，我们又开始以四倍的速度前进。

替换舍入方式的一个用处是验证数值算法的稳定性。如果算法的结果在向上舍入和向下舍入切换时，结果有非常大的误差，就说明算法对舍入误差具有敏感性。

这种测试一般比将浮点数切换到低精度后查看结果是否有较大变化更有效。因为默认的银行家舍入法会使得舍入在期望上是正确的：一半会被向上舍入，一半会被向下舍入，有时它们就会相互抵消。

### 衡量误差

令人惊讶的是，我们能够对执行复杂计算（如自然对数和平方根）的硬件衡量误差。实际上，我们可以保证从任何操作中获得最高的精度。这使得分析舍入误差变得非常容易，我们将在稍后看到这一点。

有两种常见的方法来测量计算误差：

- 创建硬件或符合规范的精确软件的工程师关心的是最后一位的单位（units in the last place, ulps），即在精确的实值和实际计算结果之间可以容纳多少个可表示的数字，作为两个数之间的距离。
- 从事数值算法工作的人关心的是相对精度，即误差的绝对值除以真实答案 $$\lvert \frac{v - v'}{v} \rvert$$。

无论哪种情况，分析误差的常用策略是假设最坏情况并地对其进行限制。

如果我们执行单个基本算术操作，那么最坏的情况是将结果舍入到最接近的可表示数字，这意味着误差不会超过 0.5 ulps。为了以相同的方式推理相对误差，我们可以定义一个称为机器 epsilon 的数字 $$\epsilon$$ ，它等于 1 与下一个可表示值之间的差值（这应该等于 2 的某个负幂次，幂次取决于尾数的位数）。

这意味着在单个算术操作后如果得到结果 $$x$$，那么真实值在以下范围内：

$$
[x\cdot (1 - \epsilon), x \cdot (1 + \epsilon)]
$$

在基于浮点计算结果做出离散的“是或否”决策时，记住误差的普遍存在尤为重要。例如，以下是检查相等性的正确方式：

```cpp
const float eps = std::numeric_limits<float>::epsilon; // ~2^(-23)
bool eq(float a, float b) {
    return abs(a - b) <= eps;
}
```

`eps` 的值应该取决于应用程序。上面的例子中使用了 `float` 的机器 epslion，这只适用于不超过一次的浮点操作。

### 区间运算

如果一个算法无论什么原因，其误差都不会在计算过程中变大，我们就称其为 _数值稳定（numerically stable）算法_。这种性质基本上只在问题本身是 _良态的（well-conditioned）_ 才有可能发生。即输入数据发生微小变化时，输出的变化也很小。

在分析数值算法时，我们可以采用与实验物理学中的方法：不再处理未知的实值，而是处理它们可能所在的区间。

例如，考虑一个操作链，我们连续地将一个变量乘以任意实数：

```cpp
float x = 1;
for (int i = 0; i < n; i++)
    x *= a[i];
```

在第一次乘法之后，$$x$$ 相对于实际乘积的值被限制在 $$(1 + \epsilon)$$ 范围内，而在每次乘法之后，这个上限会再乘以一个 $$(1 + \epsilon)$$。通过归纳法，经过 $$n$$ 次乘法后，计算值的上限为 $$(1 + \epsilon)^n = 1 + n\epsilon + O(\epsilon^2)$$，并且存在类似的下限。

这意味着相对误差为 $$O(n\epsilon)$$，这在某种程度上是可以接受的，因为通常来讲 $$n \ll \frac{1}{\epsilon}$$。

举一个数值不稳定计算的例子，考虑函数

$$
f(x, y) = x^2 - y^2
$$

假设 $$x > y$$，该函数可以返回的最大值大约为

$$
x^2 \cdot (1 + \epsilon) - y^2 \cdot (1 - \epsilon)
$$

对应的绝对误差为

$$
x^2 \cdot (1 + \epsilon) - y^2 \cdot (1 - \epsilon) - (x^2 - y^2) = (x^2 + y^2) \cdot \epsilon
$$

因此相对误差为

$$
\frac{x^2 + y^2}{x^2 - y^2} \cdot \epsilon
$$

如果 $$x$$ 和 $$y$$ 的数量级接近，误差就变成 $$O(\epsilon \cdot \lvert x\rvert)$$。（原文并没有描写是基于什么假设得到这个误差的，我也整理不出这个误差）

在上述的计算下，减法操作“放大”了平方运算的误差。但我们可以通过使用以下公式来修复：

$$
f(x, y) = x^2 - y^2 = (x + y) \cdot (x - y)
$$

在这个公式中，很容易证明误差被限制在 $$\epsilon \cdot \lvert x - y\rvert$$ 范围内（这个我也推导不出，但是比较好证明的是，绝对误差是$$O(3\epsilon(x^2 - y^2))$$，相对误差是 $$O(3\epsilon)$$）。同时它也更高效，因为它需要 2 次加法和 1 次乘法，而不是 1 次加法和 2 次乘法。

### Kahan 求和

从前面的例子中可以看出，长链操作并不是问题，但加减不同量级的数字却是一个问题。处理此类问题的一般方法是尽量将大数与大数一起处理，小数与小数一起处理。

考虑标准的求和算法：

```cpp
float s = 0;
for (int i = 0; i < n; i++)
    s += a[i];
```

由于我们做的是加法而不是乘法，它的相对误差虽然还是 $$O(n \cdot \epsilon)$$，但是实际值却非常依赖于输入。

考虑最夸张的情况，第一个值就是 $$2^{24}$$，而其它的值都是 $$1$$，那么无论 $$n$$ 是多少最终结果都是 $$2^{24}$$，可以通过如下代码验证这一点：

```cpp
const int n = (1<<24);
printf("%d\n", n);

float s = n;
for (int i = 0; i < n; i++)
    s += 1.0;

printf("%f\n", s);
```

这是因为 `float` 只有 23 位尾数，因此 $$2^{24} + 1$$ 是第一个无法精确表示而并必须向下舍入的整数，每次我们尝试将 $$1$$ 加进 $$s = 2^{24}$$ 时都会发生这种情况。误差确实是 $$O(n \cdot \epsilon)$$，在上面的例子中，它是 $$2$$。

显而易见的解决方案是切换到更大的类型，例如 `double`，但这并不是一个可扩展的方法。一个优雅的解决方案是将未加的部分存储在单独的变量中，然后将其添加到下一个变量中：

```cpp
float s = 0, c = 0;
for (int i = 0; i < n; i++) {
    float y = a[i] - c; 
    float t = s + y;    // s 可能很大，y 可能很小，此时就会丢失信息
    c = (t - s) - y;    // (t-s) 得到了丢失的那些数值，也就是0删去了 y 的高阶部分得到的结果
    s = t;
}
```

最终的 s 就是求和的结果。这个技巧被称为 _Kahan 求和_，它的相对误差是 $$2\epsilon + O(n\epsilon^2)$$。第一项来自于最后一次求和，第二项是因为我们每一步操作的误差都是 $$o(\epsilon)$$（小于 $$\epsilon$$）的。事实上这类似于后续提到的双浮点数算术，使用 c 保存了 s 不能表示的误差，从而扩展了表示的精度。

当然，更通用的方法是切换到更精确的数据类型，比如 `double`，也可以有效地将机器误差进行了平方。此外，它可以（某种程度上）通过将两个 `double` 捆绑在一起来更加放大表示：一个用于存储值，另一个用于存储其不可表示的错误，并用它们表示值 $$a+b$$。这种方法被称为 _双双精度算术（double-double arithmetic）_，它可以类似地推广到四双精度和更高精度的算术。

## 四 - 牛顿迭代法

在实际问题中，一般来讲我们并不需要最高精度的结果。真实世界的数据上，建模和测量导致的误差通常会比浮点数舍入的误差大好几个数量级。因此我们更乐意牺牲一些精度换取一个更快速的算法。

在本节中，我们将介绍这种近似数值算法中最重要的组成部分之一：牛顿迭代法。

### 牛顿迭代法

牛顿迭代法是求解实值函数近似根的一个简单而又有效的方法，它会找出函数的根，也就是下列方程的解：

$$
f(x) = 0
$$

对于函数 $$f$$，我们只需要假设 $$f$$ 在搜索区间内至少有一个根，并且 $$f(x)$$ 是可导的。这里会有[一些极端情况](https://en.wikipedia.org/wiki/Newton%27s_method)，但它们在实际中几乎从未出现过。这里我们非正式地称其它不是极端情况的可导函数是“好的”。

算法的主要思想是从一个初始解 $$x_0$$ 开始，然后在 $$(x_i,f(x_i))$$ 处画出函数的切线，切线和 $$x$$ 轴的交点就是下一个 $$x_{i+1}$$。直观的感觉是，如果 $$f$$ 是“好的”，并且 $$x_i$$ 足够接近根，那么 $$x_{i+1}$$ 就会更加接近根。

![牛顿迭代法](/assets/Arithmetic/newton.png)

为了获得 $$x_{i+1}$$，我们需要求解下列方程：

$$
f(x_i) + (x_{i+1} - x_i) f'(x_i) = 0
$$

解得：

$$
x_{i+1} = x_i = \frac{f(x_i)}{f'(x_i)}
$$

牛顿迭代法非常重要，它是科学和工程中的优化求解器的一个基本部分。

### 平方根

考虑求解如下问题：

$$
x = \sqrt{n} \Leftrightarrow x^2 = n \Leftrightarrow f(x) = x^2 - n = 0
$$

我们可以得到迭代式：

$$
x_{i+1} = x_i - \frac{x_i^2 - n}{2x_i} = \frac{x_i + n / x_i}{2}
$$

在实践中，我们同时希望算法足够接近正确答案时停止，可以在每次迭代后简单地进行检查：

```cpp
const double EPS = 1e-9;

double sqrt(double n) {
    double x = 1;
    while (abs(x * x - n) > eps)
        x = (x + n / x) / 2;
    return x;
}
```

该算法对许多函数收敛，虽然它只对其中的某个子集（例如凸函数）可靠且可证明地收敛。但还有一个问题是，如果它收敛的话，收敛的速度有多快？

### 收敛的速度

从 $$x_0 = 1 $$ 开始求 $$\sqrt{2}$$，每轮迭代后正确的数字位数如下：

- **1**.0000000000000000000000000000000000000000000000000000000000000
- **1**.5000000000000000000000000000000000000000000000000000000000000
- **1.41**66666666666666666666666666666666666666666666666666666666675
- **1.41421**56862745098039215686274509803921568627450980392156862745
- **1.41421356237**46899106262955788901349101165596221157440445849057
- **1.41421356237309504880168**96235025302436149819257761974284982890
- **1.41421356237309504880168872420969807856967187537**72340015610125
- **1.4142135623730950488016887242096980785696718753769480731766796**

可以看到，每次迭代后有效数位大概翻倍，这种结果并不是凑巧的。

为了定量地分析收敛速度，我们需要考虑第 $$i$$ 次迭代上的相对误差 $$\delta_i$$，并确定下一次迭代时的误差 $$\delta_{i+1}$$ 有多小：

$$
\delta_i = \frac{|x_i - x|}{x}
$$

我们可以将 $$x_i$$ 表示成 $$x\cdot (1+\delta_i)$$，带入到牛顿迭代式中，并将两侧都除以 $$x$$：

$$
1+\delta_{i+1} = \frac{1}{2}\left(1+\delta_i + \frac{1}{1+\delta_i}\right) = \frac{1}{2}\left( 1+ \delta_i + 1 - \delta_i + \delta_i^2 + o(\delta_i^2) \right) = 1 + \frac{\delta_i^2}{2} + o(\delta_i^2)
$$

这里我们使用了泰勒展开，并假设 $$\delta_i$$ 足够小。重新整理式子，我们有：

$$
\delta_{i+1} = \frac{\delta_{i}^2}{2} + o(\delta_i^2)
$$

这意味着一旦我们接近解，每次迭代的误差大约会平方（并减半）。由于对数 $$(-\log_{10} \delta_i)$$ 大致是 $$x_i$$ 中准确的有效数字的数量，因此相对误差的平方恰好对应于我们观察到的有效数字数量的翻倍。

这被称为 _二次收敛_，事实上，这不仅限于求平方根。可以证明，一般来说：

$$
|\delta_{i+1}| = \frac{|f''(x_i)|}{2|f'(x_i)|} \delta_i^2
$$

当 $$f'(x)$$ 不为零，$$f''(x)$$ 是连续的时候，结果就是二次收敛的。

### 扩展阅读

[Introduction to numerical methods at MIT.](https://ocw.mit.edu/courses/18-330-introduction-to-numerical-analysis-spring-2012/pages/lecture-notes/)

## 五 - 快速平方根倒数

在计算机图形学及许多模拟场景中，经常需要归一化向量，此时需要计算平方根倒数（例如计算入射角和出射角从而模拟光线反射）。

$$
\hat{v} = \frac{\vec{v}}{\sqrt{\vec{v}^T \vec{v}}}
$$

直接计算平方根是非常慢的，因为平方根操作和取倒数都是很慢的操作，尽管它们是由硬件实现的。

但有一个非常好的近似算法，它利用了浮点数在内存中的存储方式。实际上，它太好了以至于直接[被实现在硬件中](https://www.felixcloutier.com/x86/rsqrtps)，因此算法本身和软件工程师就无关了。但是由于其独特的美感和巨大的学习意义，我们还是要了解一下。

除了算法本身，它的历史也很有意思。最早是由 id Software 在它们 1999 年的游戏 Quake III Arena 中实现的。虽然这估摸着也是一长串“我从别人那抄的，他也抄的别人的”。如果想要更加了解这个算法的历史的话，可以查看 [wiki](https://en.wikipedia.org/wiki/Fast_inverse_square_root)。

在 2005 年左右，他们开源了游戏的源码，然后这个算法就出名了。以下是相关的代码，并带有注释：

```cpp
float Q_rsqrt(float number) {
    long i;
    float x2, y;
    const float threehalfs = 1.5F;

    x2 = number * 0.5F;
    y  = number;
    i  = * ( long * ) &y;                       // evil floating point bit level hacking
    i  = 0x5f3759df - ( i >> 1 );               // what the fuck? 
    y  = * ( float * ) &i;
    y  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration
//  y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed

    return y;
}
```

我们将一步一步讨论这个算法，但首先，我们得看点别的。

### 近似对数

在计算机（或者计算器）成为日常物品前，人们使用对数表计算乘法和相关运算——查找 $$a$$ 和 $$a$$ 的对数，将它们相加，然后找到结果的反对数。

$$
a\times b = 10^{\log a + \log b} = \log^{-1}(\log a + \log b)
$$

我们也可以用相同的方法计算平方根倒数：

$$
\log \frac{1}{\sqrt{x}} = -\frac{1}{2} \log x
$$

快速的平方根反比算法就是基于这个恒等式的，所以它需要非常快速地计算出 $$x$$ 的对数。事实证明，这可以通过将 32 位浮点数重新解释为整数来近似。

回想一下，浮点数是由符号位（这里假设为正数），指数位 $$e_x$$ 和尾数位 $$m_x$$ 组成的：

$$
x = 2^{e_x} (1 + m_x)
$$

其对数为：

$$
\log_2 x = e_x + \log_2(1 + m_x)
$$

由于 $$m_x \in [0,1)$$，我们可以用对数的近似值来替换它：

$$
\log_2 (1+m_x) \approx m_x
$$

这个近似在区间两端都是精确的，但是为了考虑平均情况，我们引入误差项：

$$
\log_2 x = e_x + \log_2(1 + m_x) \approx e_x + m_x + \sigma
$$

现在让我们看看直接将其解释为整数是什么情况。令 $$L=2^{23}$$（尾数的位数）且 $$B=127$$（指数的偏置），我们有：

$$
\begin{aligned}
I_x = & L(e_x + B) + Lm_x \\
=& L(e_x + m_x + \sigma + B - \sigma) \\
\approx& L\log_2 (x) + L(B - \sigma)
\end{aligned}
$$

如果我们调整 $$\sigma$$ 尝试最小化均方误差，那么：

![对数近似](/assets/Arithmetic/approx.svg)
_横坐标为浮点数，纵坐标为整数表示。蓝线是直接重新解释为整数，灰线是通过计算得到的整数_

那么我们可以用整数表示来近似浮点数的对数：

$$
\log x \approx \frac{I_x}{L} - (B - \sigma)
$$

### 近似结果

为了计算 $$y = \frac{1}{\sqrt{x}}$$ 我们需要计算 $$\log y = -\frac{1}{2}\log x$$，将之前的近似带入可得：

$$
\frac{I_y}{L} - (B - \sigma) \approx -\frac{1}{2}(\frac{I_x}{L} - (B - \sigma))
$$

解得：

$$
I_y \approx \frac{3}{2}L(B - \sigma) - \frac{1}{2} I_x
$$

这表明我们甚至不需要计算对数：上面的公式只是一个常数减去二分之一的整数。它在代码中写为：

```cpp
i = * ( long * ) &x;
i = 0x5f3759df - ( i >> 1 );
```

我们在第一行中将 $$x$$ 重新解释为整数，然后将其代入第二行的公式，其中第一项是魔法数字 $$\frac{3}{2}L(B-\sigma)=\text{0x5f3759df}$$，而第二项使用二进制移位代替除法。

### 使用牛顿迭代法迭代

最后，我们使用牛顿迭代法解决 $$f(y)=\frac{1}{y^2} - x$$ 的根即可：

$$
y_{i+1} = \frac{y_i(3-xy_i^2)}{2}
$$

在代码中写为：

```cpp
x2 = number * 0.5F;
y  = y * ( threehalfs - ( x2 * y * y ) );
```

最开始的近似是如此之好，以至于对于游戏开发目的来说，只需一次牛顿迭代就足够了。在第一次迭代后，它在正确答案的 99.8% 以内，并且可以进一步重复迭代以提高精度——这就是硬件中所做的：x86 指令执行一点迭代并保证相对误差不超过 $$1.5\times 2^{-12}$$。

### 扩展阅读

建议阅读 [Wikipedia article on fast inverse square root](https://en.wikipedia.org/wiki/Fast_inverse_square_root)。其中还提到了魔法数字的设置以及目前的研究现状。
