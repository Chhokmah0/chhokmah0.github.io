---
title: 高性能计算-第八章-外存
date: 2025-4-3 12:00:00 +0800
categories: [笔记, 高性能计算]
tags: [高性能计算]     # TAG names should always be lowercase
math: true
---
[原文](https://en.algorithmica.org/hpc/external-memory/)

## 序

对两个数求和需要花多长时间？作为最常使用的指令之一，只要数据已经被保存在寄存器里，那么 `add` 只需要一个周期。

但是在 `*c = *a + *b` 这种情况中，我们却需要先从内存中取出相应的操作数：

```nasm
mov eax, DWORD PTR [rsi]
add eax, DWORD PTR [rdi]
mov DWORD PTR [rdx], eax
```

从内存中获得任何数据时，总会产生数据传输的延迟。因此，内存请求并不会直接跑到它的目的地，而是先跑到一个复杂的地址转换和缓存层来减少延迟。

所以这个问题最准确的答案是：对两个数求和的时间取决于操作数的位置：

- 如果存储在 RAM 中，大约会需要 100ns，或者说用 200 个周期获取它，再用 200 个周期写回去。
- 如果操作数最近被访问过，那么它大概率已经被缓存并且很快就可以获取。不同的缓存层级（访问时间）具有不同的访问速度，最慢的可能要 ~50 个周期，最快的可能只有 4-5 个周期。
- 数据也可能存储在 _外存_ 上，此时可能需要花费 5ms，大约 $$10^7$$ 个时钟周期来访问它。

这样高的性能差异主要是因为存储硬件和 CPU 芯片并不遵循相同的增长规律。就算五十年前他们的性能差不多，现在也已经差的很远了。

![内存和 CPU 硬件的增长](/assets/External_Memory/memory-vs-compute.png)

为了减少这一点，现代内存系统变得越来越层级化，高层级的缓存会以容量为代价换取更少的延迟。容量和延迟在不同的层级可能发生指数级的变化——尤其是外存——因此对于许多内存密集型的算法，优化它们的 IO 局部性变得尤为重要。

这促使我们构建一个新的数学模型，称为 _外存模型_。在这里，唯一的内存开销是块读取和块写入，对于涉及已经读取到有限的本地内存的数据，IO 都是零开销的。它催生了一个新的关于 _外存算法_ 的领域，也是我们之后要学习的东西。

## 一 - 内存层级

现代计算机内存系统是高度层级化的。较高的层会缓冲较低的层中被经常访问的数据以减少延迟，每层的性能和成本往往有着数量级的差异。

抽象化地说，不同的存储设备都可以被表述为一个存储容量 $$W$$ 和每次读写块数据的块大小 $$B$$。每种内存都有以下比较重要的特征：

- 总大小 $$M$$；
- 块大小 $$B$$；
- 延迟，也就是获得数据所花费的时间；
- 带宽，有些时候可能会高于块大小除以延迟，这意味着 I/O 操作可以并行执行；
- 成本，各种意义上的均摊成本，包含芯片价格、能源消耗、维护开销等。

这里有 2021 年的一些硬件信息：

| Type | $$M$$      | $$B$$ | Latency | Bandwidth  | $$/GB/mo |
| :--- | :--------- | ----- | ------- | ---------- | :------- |
| L1   | 10K        | 64B   | 2ns     | 80G/s      | -        |
| L2   | 100K       | 64B   | 5ns     | 40G/s      | -        |
| L3   | 1M/core    | 64B   | 20ns    | 20G/s      | -        |
| RAM  | GBs        | 64B   | 100ns   | 10G/s      | 1.5      |
| SSD  | TBs        | 4K    | 0.1ms   | 5G/s       | 0.17     |
| HDD  | TBs        | -     | 10ms    | 1G/s       | 0.04     |
| S3   | $$\infty$$ | -     | 150ms   | $$\infty$$ | 0.02     |

### 易失性内存

RAM 以及更高层的内存被成为易失性内存，因为它们的数据会在断电时丢失。但是它足够快，这也就是为什么会被用于在计算机通电时保存数据。

从最快到最慢的易失性存储器有：

- CPU 寄存器：CPU 会使用寄存器来读取它的操作数和保存计算结果，可以说 CPU 主要就是和寄存器执行交互，访问它们是零延迟的。他们的数量十分有限（例如，只有 16 个通用的寄存器）。
- CPU 缓存：现代 CPU 往往会使用很多层缓存（L1、L2、L3，有时候甚至会到达 L4）。L1、L2 缓存往往是 CPU 核独享的，到了 L3 就变为不同核之间共享的。
- 随机访问存储器：这是第一种可扩展的存储器。如今你甚至可以在公有云上租到 TB 级别 RAM 的机器。

CPU 缓存系统中有个非常重要的概念是 _缓存行 (cache line)_，它是从 CPU 和 RAM 之间进行数据传输的基本数据单元。在大多数机器上，缓存行的大小都是 64 字节。也就是说内存被按照 64 字节大小分块，每次获取某一字节的数据时，机器都会把同一缓存行中的其它 63 字节的数据拿过来，而不管你要不要它们。

在 CPU 上的缓存行根据它的最近访问时间来确定自己的缓存等级。当被访问时会被放到最低的缓存层，随后随着时间流逝，如果没有被访问就会被驱逐到更高一层的缓存。程序员无法控制这个过程，但是了解这些工作的细节也是有益的。我们将在下一张讨论这些。

### 非易失性内存

CPU 缓存和 RAM 都只存储少量电子（这些电子在不通电时很容易泄露），而在非易失性存储器中，存储单元会保存几百个电子。这使得数据可以在不通电的情况下保存更长的时间，但代价则是性能和耐用性——几百个电子更容易和硅原子发生碰撞。

有很多方式可以持久地保存数据，但从程序员视角来看，主要有这些：

- SSD (Solid state drives, 固态硬盘)：SSD 一般有着较小的延迟 (0.1ms)，但是它们的成本也很高，因为它们的每个存储单元寿命有限，且只能写入有限的次数。不过 SSD 是固定的且没有活动部件，因此被经常使用于移动设备。
- HHD (Hard disk drives, 机械硬盘)：HHD 是一个带有读写头的物理上的旋转磁盘。为了读取一个数据，你需要等磁盘转动到指定位置，然后非常精确地将磁头指向目标位置。这导致了一个非常奇怪的延迟：随机读取一个字节可能和顺序读取 1MB 的数据花费相同的时间。由于硬盘是除了冷却系统外唯一运动的部分，HHD 经常会损坏（数据中心的 HHD 寿命大约是 3 年）。
- NAS (Network-attached storage, 网络附属存储)：NAS 使用网络设备来存储数据。现实中基本上有两种类型，第一种是 NFS (Network File System, 网络文件系统)，这是一种通过网络挂载另一台计算机的文件系统的协议。另外一种是基于 API 的分布式存储系统，最著名的是 Amazon S3，它由公有云的存储机器提供支持，内部经常会使用廉价的 HDD 或者什么奇特的存储设备。尽管 NFS 在同一个数据中心时有些时候会比 HDD 更快，但是公有云上的对象存储往往还是会有 50-100ms 的延迟。它们通常是高度分发和复制的，从而获得更好地可用性。

由于 SDD/HDD 显著的比 RAM 慢，因此这一层以及更低的层级都被称为 _外存_。

和 CPU 缓存不同，外存可以被显示控制，在有些时候非常有用。但是大多数情况下程序员只想将其抽象出来，并将其当作主存的扩展使用。在操作系统中会使用 _虚拟内存_ 实现这一点。

## 二 - 虚拟内存

早期的操作系统允许每个进程自由地读取和修改任何内存区域，包括那些分配给其他进程的内存区域。虽然这挺简约的，但是带来了很多问题：

- 如果一个进程是有 bug 或者有恶意的呢？我们如何放置进程修改其他进程的内存，同时又允许它们通过内存进行同步？
- 我们如何处理内存碎片？例如我们有 4MB 内存，进程 A 分配了 1MB，进程 B 分配了 2MB，随后进程 A 结束并释放了内存，此时进程 C 申请 2MB 的连续内存——这里就报错了因为我们只有两个分开的 1MB 内存。重启进程 B 或者停止它并把它所有的数据和指针挪动 1MB 也不是什么好方法。
- 我们如何访问非 RAM 的存储结构？我们怎么插入一个闪存然后从里面读取一个文件？

这些问题对于一些专门的计算系统不是那么重要（比方说 GPU，通常一次只有一个计算任务，拥有对计算的完整控制），但它们对于现代多任务操作系统来说是绝对必要的——操作系统使用一种叫做虚拟内存的技术解决了所有这些问题。

### 内存分页

虚拟内存给每个进程一个控制了连续空间存储的错觉，这些存储空间实际上会被映射到物理内存上很多更小的块——主存或者外存都可以。

![虚拟内存和物理内存的映射](/assets/External_Memory/virtual-memory.jpg)

为了实现这一点，内存物理空间被划分为 _页_（通常为 4KB 大小），也是程序可以通过操作系统申请的最小内存单元。存储系统会维护一个特殊的数据结构 _页表_，包含了虚拟页到物理页的映射。当一个进程使用它的虚拟地址访问数据时，存储系统就会计算页号（将虚拟地址下除 4096，或者说右移 12 位），然后在页表中查找它的物理地址，将读或写请求转发到实际存储数据的位置。

由于每次内存请求都需要内存转换，同时内存页的编号也有很多（16GB / 4KB = 4M），地址转换本身就是一个难题。一种加速方式是给页表设置一个专用的 cache，translation lookaside buffer (TLB)，另一种是增加页的大小，通过减少页的粒度来减少页的数量。

### 映射到外存

虚拟内存的机制也允许相当直接地使用外部内存类型。现代操作系统支持内存映射，它允许你打开一个文件并使用它的内容，就好像它们在主内存中一样：

```cpp
// open a file containing 1024 random integers for reading and writing
int fd = open("input.bin", O_RDWR);
// map it into memory      size  allow reads and writes  write changes back to the file
int* data = (int*) mmap(0, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
// sort it like if it was a normal integer array
std::sort(data, data + 1024);
// changes are eventually propagated to the file
```

这里映射了一个 4K 的文件，我们可以用一个内存页保存。但当我们打开一个更大的文件时，读入会在我们需要的时候惰性完成，写入也会被缓冲下来直到需要将页写回（通常是程序终止或者系统没有 RAM 空间了）。

另一种技术有相同的原理，但却有相反的目的，它被称为 _交换文件_。它允许操作系统在没有足够的实际 RAM 时自动使用 SSD 或 HDD 的一部分作为主存的扩展。这让内存空间较小的系统只会经历夸张的减速而不是直接崩溃。

这种主存和外存的无缝集成方式本质上把 RAM 变成了外存的“L4缓存”，从算法设计的角度来看，是一种很便捷的思考方式。

## 三 - 外存模型

为了解释内存瓶颈算法的性能，我们需要一个对块 I/O 操作更加敏感的计算模型。

### 缓存感知模型 (Cache-Aware Model)

在标准 RAM 模型中，我们忽略了基本运算需要不同的时间来完成的这一事实。更重要的是，它没有区分在不同类型存储器上的操作，从 RAM 读取需要 ~50ns，而从 HDD 中读取需要 ~5ms，中间相差了 $$10^5$$ 倍。

考虑到这一点，在 _外存模型_ 中，我们只考虑 I/O 操作的开销。准确地说，我们只考虑一层缓存，并且假设它有如下性质：

- 输入数据的大小为 $$N$$，被存储在 _外存_ 上，我们只能同时读取 $$B$$ 个连续的元素。（读取一个块和块中的一个元素花费相同的时间）
- 我们可以在内存中存储至多 $$M$$ 个元素，也就是至多存储 $$\left\lfloor \frac{M}{B} \right\rfloor$$ 个块。
- 我们只关心 I/O 操作，任何其它操作都假设为零开销。
- 额外认为 $$N \gg M \gg B$$。

在这个模型当中，一个算法的性能由它的高层次 _I/O 操作_ 或者 IOPS 决定——换句话说，在整个运行过程中从外存中读取或写入块的数量。

虽然我们将要讨论的底层分析技术适用于缓存层次结构中的任何一层，但我们最关心的是将 RAM 当作内存，将 SSD 或者 HDD 当作外存的模型。在这个设置下，块大小 $$B$$ 大约是 1MB，内存大小 $$M$$ 大约是几 GB，而外存大小 $$N$$ 可以达到几 TB。

（话说最常用的居然不是将 Cache 当作内存，RAM 当作外存的模型吗？）

### 数组扫描 (Array Sacn)

作为最简单的例子，当我们想要计算一个数组的总和式，至少需要便利每个元素一次，我们可以每次加载 $$O(B)$$ 个元素然后求和。因此在外存模型中，求和或者其它数组扫描算法的复杂度为：

$$
SCAN(N) \triangleq O\left(\left\lfloor \frac{N}{B} \right\rfloor\right) 
$$

我们跳过具体的代码实现。注意，在很多情况下操作系统会自动执行 buffer 操作。即使数据只是从普通文件重定向到标准输入，操作系统也会通过缓冲流并以约 4KB 的块（默认情况下）读取它。

## 四 - 外存排序

现在让我们使用外存模型来设计一个高效算法。外存排序往往基于标准的归并排序算法，因此先来看一下简单的构件。

### 归并

**问题**。给定两个有序数组 $$a$$，$$b$$，给出一个包含它们的所有元素的有序的数组。

标准的双指针计数可以这样实现：

```rust
fn merge<T: Ord + Clone>(left: &[T], right: &[T]) -> Vec<T> {
    let mut merged = Vec::with_capacity(left.len() + right.len());
    let mut iter_left = left.iter().peekable();
    let mut iter_right = right.iter().peekable();
    
    while iter_left.peek().is_some() && iter_right.peek().is_some() {
        if iter_left.peek() <= iter_right.peek() {
            merged.push(iter_left.next().unwrap().clone());
        } else {
            merged.push(iter_right.next().unwrap().clone());
        }
    }
    while iter_left.peek().is_some() {
        merged.push(iter_left.next().unwrap().clone());
    }
    while iter_right.peek().is_some() {
        merged.push(iter_right.next().unwrap().clone());
    }

    merged
}
```

考虑内存操作，我们只是线性读取 $$a$$ 和 $$b$$ 并线性写入 $$c$$。由于读和写操作都可以被缓冲（从而实现块读取和块写入），因此可以在 $$SCAN(N+M)$$ 的 I/O 操作内完成。

到目前为止，这些例子都很简单，除了我们将最终答案除以了块大小 $$B$$，分析它们与标准的 RAM 模型没有太大的不同。但我们将要考虑一个特别的情况。

**k-路归并**。考虑归并算法的一个修改版本，不再合并两个数组，而是同时合并 $$k$$ 个数组，这 $$k$$ 个数组的总长为 $$N$$。同样的，可以选择 $$k$$ 个数组中最小的元素写入 $$c$$，并推进对应数组的迭代器。

在标准 RAM 模型中，随着 $$k$$ 的增加时间复杂度也会增加，因为我们需要 $$O(k)$$ 的时间（最简单的实现）来找到最小的元素。但在外存模型中，我们在内存上所做的任何操作都是零开销的，因此只要我们可以在内存中存入 $$(k+1)$$ 个块（$$k$$ 个块用于读取，$$1$$ 个块用于写入），那么复杂度就不会增加。也就是说 $$k=O\left(\frac{M}{B}\right)$$ 时都不会导致 IO 数增加。

还记得我们之前假设了 $$M \gg B$$ 吗？其更加形式化的描述方式是存在一个 $$\varepsilon > 0$$ 使得 $$M \geq B^{1+\varepsilon}$$，也被称为 _高缓存假设_。其作用在于使得 $$\frac{M}{B} \geq B^\varepsilon$$，确保内存中可以存储多项式级别的块。在外存算法中，往往会需要在内存中存储亚多项式数量的块，例如 $$O(\log N)$$，高缓存假设可以保证这些外存算法的复杂性。

### 归并排序

普通的归并排序的时间复杂度为 $$O(N\log_2 N)$$（外存算法似乎比较关系对数的底数，尽管它实际上可以通过换底公式以常数代价换成另一个底数）。在 $$O(\log_2 N)$$ 层迭代中，每一层都会需要遍历 $$N$$ 个元素进行两两归并。

在外存模型中，对于大小为 $$B$$ 的块，我们可以零开销的排序内部元素，因为这样不会产生新的 I/O。我们可以认为排序 $$O(B)$$ 个元素的时间复杂度是 $$O(1)$$ 的。这样我们可以把整个数组分成 $$O(\frac{N}{B})$$ 个块，把它们当作基础元素进行归并。最底层我们使用 k-路归并，因为这是零开销的，。

![k-路归并](/assets/External_Memory/k-way.png)

此时需要 $$O(\log_2 \frac{N}{M})$$ 层归并操作（注意并不是 $$O(\log_2 \frac{N}{B})$$，这是因为合并 $$\frac{M}{B}$$ 个块的 I/O 复杂度是 $$O(1)$$ 的，因为它们全部都在内存当中）。每层的归并则是一个 $$SCAN$$ 操作，因此总的 I/O 次数是：

$$
O\left(\frac{N}{B} \log_2 \frac{N}{M}\right)
$$

这相当快。如果我们有 1GB 的内存和 10GB 的数据，这实际上意味着我们需要比单纯的 $$SCAN$$（单纯的读取数据）多 3 倍的努力。有趣的是，我们可以做得更好。

### k-路归并

如果我们每一层都使用 $$k$$-路归并的话，会产生什么结果？

每一层仍然需要进行一次 $$SCAN$$ 来进行 $$k$$-路归并，但是此时 $$k=\frac{M}{B}$$，层数将会大大减少，变为 $$\log_{\frac{M}{B}} \frac{N}{M}$$，整体的复杂度变为：

$$
SORT(N) \triangleq O\left(\frac{N}{B} \log_{\frac{M}{B}} \frac{N}{M} \right)
$$

### 真实实现

在真实场景下，相较于使用 $$\log_{\frac{M}{B}} \frac{N}{M}$$ 层归并，我们可以只使用两层：一层用于多次合并 $$\frac{M}{B}$$ 个大小为 $$B$$ 的块，一层用于合并得到最终结果。通过这种方法，我们可以只读取数据集两次。在 GB 级别的 RAM 和 1MB 的块大小下，这种方式可以排序 TB 级别的数据。（$$SORT$$ 中对数的结果等于 1 时）

具体的实现请查看[原文](https://en.algorithmica.org/hpc/external-memory/)，因为我没有数据，也不想搓这些数据占用硬盘资源。不过总体来讲还是比较简单的。

### Joining（连接）

排序主要被用于其它操作的步骤之一，外存排序的一个比较重要的应用是 joining（比方说 SQL join），经常出现在数据库或者其它数据处理应用。

**问题**。给定 $$(x_i,a_{x_i})$$ 和 $$(y_i, b_{y_i})$$ 两个列表，输出一个列表 $$(k,a_{x_i}, b_{y_j})$$ 满足所有的元素都有 $$k=x_i=y_j$$。（博主：我觉得原文写的有点问题，这里改了下）

最优的方法是将两个列表排序，然后使用标准的双指针方法进行合并。IPOS 和 $$SORT$$ 一致，如果已经排好了序那么就只需要 $$SCAN$$ 的 I/O。这也是为什么许多数据处理软件（数据库，MapReduce）都喜欢让表保持部分有序。

**其它方法**。不过上述分析只适用于我们不能将所有数据读入内存。否则在现实当中，我们有一些其它更快的方法。

最简单的方式就是 _哈希 join_，用 python 来写的话大概就长这样：

```python
def join(a, b):
    d = dict(a)
    for x, y in b:
        if x in d:
            yield d[x]
```

在外存中，使用哈希表 join 两个列表基本是不可能的。因为最坏情况下相邻的一些元素会被 hash 的很远，导致我们需要读取 $$O(N)$$ 次块。

还有一种方法就是使用基数排序。在实际中，基数排序会需要 $$O(\frac{N}{B}\cdot w)$$ 次块读写。其中 $$w$$ 是在某个基数下 $$N$$ 个键中最大值的位数，也就是基数排序所需要的轮数。如果基数（桶的数量）是 $$O(\frac{M}{B})$$ 的，那么每轮基数排序就需要 $$SCAN$$ 的时间进行一轮读和写。因此总时间是 $$O(\frac{N}{B}\cdot w)$$ 的。当数据集足够大且都是小键值时，基数排序可能会更快。

## 五 - 列表排序 (List Ranking)

在本节中，我们会使用外存排序和 join 解决一个看上没啥用，但实际上用于许多外存算法或者并行算法中的问题。

**问题**。给一个和单链表，计算其中每个元素的 rank。这里将 rank 定义为到最后一个元素的距离。（博主：怎么不是第一个？）

![LiskRank 例子](/assets/External_Memory/list-ranking.png)
_List Ranking 的一个例子（博主认为上半部分图中大于 0 的数字应该减一，毕竟是从 0 开始编号的）_

这个问题在 RAM 模型中可以简单地解决：只需要带个计数器遍历整个列表。但是这种指针跳转在外存模型下不能很好地工作，因为列表节点是随机存储的。在最坏的情况下，读取每个新节点都可能需要读取一个新的块。

### 算法

考虑这个问题的一个更一般的版本。现在每个元素除了其地址 $$i$$ 外，还有一个权重 $$w_i$$，而我们需要计算它的前缀和（按照在链表上的顺序、**不包含**当前元素的前缀和）放置到其对应的地址上。如果想要解决解决原始问题，只需要让每个 $$w_i=1$$。

原文写得非常奇怪。这篇博客里的内容是我从其它地方总结得到的，我并不能保证正确性。

该算法的主要思想是去除部分元素，将自身的权重留给前一个元素，然后递归求解。最后再使用被删除的元素和权重重建原问题的解。

假设我们有三个链表上连续的元素 $$x,y,z$$，我们删去 $$y$$ 并将 $$w_y$$ 合并至 $$w_x$$ 中。做完前缀和后，你会发现 $$rw_x, rw_z$$ 就是最终的结果。因此我们在恢复时只需要加入 $$y$$，并计算 $$rw_y = rw_x + w_y$$。

在这里，每次删除元素时必须保证被删除的元素不能是在链表上连续的，否则的话在递归和恢复时会产生不必要的麻烦。最理想的情况下我们希望将链表拆分成偶数和奇数位置的两个链表，但是这样子做的难度并不比解决原问题简单多少。因此有一种处理方式是随机，我们给每一个元素投掷一个硬币，如果链表上的上一个元素的硬币为正，同时当前元素的硬币为反，则可以删除掉这个元素。这样的随机过程在期望上会删除掉链表中 $$\frac{1}{4}$$ 的元素。

假设链表中的每个元素是三元组 $$(i, succ_i, w_i)$$，则整个算法的流程可以被总结成这样：

1. 投硬币，并令链表开头的元素投出来的必须是正面，防止其被删除。随后通过前述的方式将链表进行压缩得到 $$(i, succ'_i, w'_i)$$。
2. 递归解决压缩后链表的结果，得到 $$(i, rw_i)$$。
3. 将被删除的元素重新加入到链表中。

当链表的长度小于 $$M$$ 时，可以使用 $$O(M/B)$$ 次 IO 直接读入到内存中解决。我们重点讨论如何实现步骤 1 和步骤 3。

对于步骤一，每个元素现在是四元组 $$(i, succ_i, w_i, p_i)$$，其中 $$p_i$$ 表示投硬币的结果，同时有 $$p_0 = 0$$。我们将其以 $$succ_i$$ 为键进行外部排序，外部排序后的结果可以被解释为 $$(prev_i, i, w_{prev_i}, p_{prev_i})$$。我们之后将对这两个序列归并地按照键 $$i$$ 进行 join 操作，构建保留下来的数组 $$L$$ 和被删除的 $$D$$。

- 如果 $$p_{prev_i} = 1$$ 且 $$p_i = 0$$，则当前元素需要被删除，向 $$L$$ 中添加三元组 $$(prev_i,succ_i,w_{prev_i} + w_i)$$，向 $$D$$ 中添加 $$(i, prev_i, w_i)$$。
- 否则，当前元素需要被保留，向 $$L$$ 中添加三元组 $$(prev_i, i, w_{prev_i})$$。

递归地处理 $$L$$ 数组，假设已经得到了 $$(i, rw_i)$$。

将 $$D$$ 按照 $$prev_i$$ 作为键进行外部排序，并将三元组重新解释为 $$(succ_i, i, w_{succ_i})$$。将之前的结果数组 $$(i, rw_i)$$ 和 $$D$$ 进行归并的 join 操作，同时保留 $$(i, rw_i)$$ 的所有值（貌似被叫做 Left Join？）。如果有相同的 $$i$$，则额外向结果数组中插入 $$(succ_i, rw_i + w_{succ_i})$$。

这个算法的 IO 复杂度为：

$$
T(N)=T(\frac{3}{4} N) + SORT(N)= SORT(N)
$$

可以使用主方法得到最后一个等号。

### 应用

List ranking 在图论算法中非常有用。

例如我们可以在外存中获取一颗树的欧拉路径（dfs 路径），然后在这个链表上执行 List Ranking，就可以得到每个结点的 $$tin_v$$（time_in，即第一次访问到点 $$v$$ 的时间。

原文的剩余部分无法理解其意图，先跳过。

## 六 - 淘汰策略 (Eviction Policies)

虽然模型允许我们手动控制 I/O 操作，但多数情况下我们还是回使用自动的缓冲和缓存机制，可能是懒，也可能是计算环境受限。

但是自动缓存也有它的挑战。当一个非程序使用完内存后，再次从外存中读取数据时就需要决定将哪个块的数据淘汰掉。决定淘汰掉那个块的策略就是淘汰策略：

- First in first out (FIFO)：将最早加入到内存中的块淘汰掉。
- Least recently used (LRU)：将最长没有使用过的块淘汰掉。
- Last in first out (LIFO) 和 most recently used (MRU)：与前两个正好相反。这两个策略看上去并不好，毕竟为什么要删除热点块？然而这些策略在某些情况下确实最优的，比如循环读取一些文件，刚刚读取的数据其实很长时间不会被再次使用。
- Least-frequently used (LFU)：通过统计每个块的访问次数，淘汰掉使用频率最低的块。部分改进的算法会考虑访问模式的时间变化特性，例如统计最近 $$n$$ 次访问的时间窗口，或者使用指数加权平均来让近期访问有更高的权重。
- Random replacement (RR)：随机丢弃一个块。其优点是不需要维护任何数据结构。

缓存淘汰策略的精度和实现时复杂度带来的开销之间，存在着天然的权衡。对于 CPU 缓存而言，需要采用能够用硬件实现、几乎零延迟的简单淘汰策略；而在节奏更慢、可规划性更强的场景中—例如 Netflix 决定将电影存储在哪些数据中心，或 Google Drive 优化用户数据的存储位置——则更适合使用复杂的策略，甚至引入机器学习来预测数据的下一次访问时间。

### 最优缓存

对于理论上的最优策略，我们将其表示为 $$OPT$$ 或者 $$MIN$$。“最优”意味着对于一个查询序列，Cache miss 的次数最小的淘汰策略。

最优策略其实可以用一个“简单”的贪心策略 _Bélády algorithm_ 来完成：淘汰掉未来最晚被使用的块。可以证明这样做总是最优解之一。然而为了使用这种策略，你要么提前知道有那些查询（离线调度），要么就能够预知未来。

不过好消息是，从渐进复杂度的角度而言，使用哪种策略并不重要。Sleator 和 Tarjan 的[论文](https://www.cs.cmu.edu/~sleator/papers/amortized-efficiency.pdf) 表明，在大多数情况下，$$LRU$$ 和 $$OPT$$ 之间的差距只有一个常数因子。

**定理**。令 $$LRU_M$$ 和 $$OPT_M$$ 表示在内存为 $$M$$ 的机器上，使用两个对应策略所产生的 Cache Miss 的次数，则：

$$
LRU_M \leq 2\cdot OPT_{M/2}
$$

证明的主要思想是考虑最坏情况。对于 $$LRU_M$$，最坏情况是重复的循环读取 $$M/B$$ 个块，此时每个块都会 Cache Miss。然而，$$OPT_{M/2}$$ 会一致保留其中的一半块，这样一半的块会命中缓存，而另一半会 Cache Miss，Cache Miss 率降到了 50%。

这是一个非常令人满意的结果。这意味着，对于渐近 I/O 复杂度，我们可以假设我们的淘汰策略是 LRU 或者 OPT——哪个有利于分析就用哪个——然后将分析结果迁移到另一个策略上。

### 实现缓存

在合理的时间内找到应该被淘汰的块并不总是容易的事。CPU 缓存通常是由硬件实现的（往往是 LRU 的某种变体），更高层的淘汰策略则应该由软件实现，并且应该存储和维护数据结构来加快淘汰的过程。

考虑一下实现 LRU 策略需要什么。假设我们正在存储一些中等大小的对象——比方说我们正在开发一个数据库的缓存，其中存储的都是中等长度的字符串。这样我们所设计的数据结构相对比较小，但也并非可以忽略。

首先，我们需要一个哈希表来查找数据本身。由于我们存储的是大型可变长度字符串，因此使用查询的哈希值作为键，并使用指向堆中字符串的指针作为值。

为了实现 LRU，最简单的方式是构建一个队列，当我们访问数据时，向其中推入当前时间和所访问数据的 key。同时对于每个对象我们需要存储最后访问的时间。（不一定是时间戳，使用任何自增的计数器都可以）

现在，当我们需要释放空间时，我们可以通过从队列的前面弹出元素来找到最近最少使用的对象。但是这时不能直接删除它们，因为它们可能在被添加到队列后又被访问了。因此，我们需要检查将它们放入队列的时间与它们最后一次被访问的时间是否相匹配，然后才能释放内存。

现在的问题是，我们每次访问数据都需要往队列中添加数据，而只有在缓存未命中时才删除条目，并开始从前面弹出它们，直到找到匹配项。这可能会导致队列溢出，为了减轻这种情况，我们可以在缓存命中时立即将其移动到队列的末尾。

为了实现这一点，我们需要实现一个双向链表来表示队列。每个块除了保存最后一次被访问的时间，还应该保存一个指向链表中表示自己结点的指针。当我们缓存命中时，我们通过指针从双向链表中删除表示自己的结点，然后加入到队列的尾部。这样，在任何时间点，队列中的节点数量与我们拥有的块数量一样多，并且保证每个缓存条目的内存开销是恒定的。

作为练习，可以考虑如何实现其它淘汰策略。

## 七 - 缓存无关算法 (Cache-Oblivious Algorithms)

在外存模型的语境下，存在两种高效算法：

- _缓存感知算法_ 对于确定的 $$B$$ 和 $$M$$ 可以设计出高效的算法。
- _缓存无关算法_ 对于任意 $$B$$ 和 $$M$$ 都是高效的算法。

例如外部排序就是缓存感知的，并不是缓存无关算法。我们需要知道确切的内存大小和块大小，才能计算出 $$k$$-路归并中正确的 $$k$$ 值。

缓存无关算法的优势在于其对任意级别的缓存都是有效的。一个缓存无关算法会充分利用所有层级的缓存，而不只是对某一层重点设计的结果。

### 矩阵转置

假设我们有一个 $$N\times N$$ 的方阵 $$A$$，而我们想要转置它，那么最基础的方式是：

```rust
pub fn matrix_transpose<T>(matrix: &mut [T], n: usize) {
    for i in 0..n {
        for j in 0..i {
            matrix.swap(i * n + j, j * n + i);
        }
    }
}
```

这里将方阵存储在一维数组中以保证内存的连续性。

该算法的 I/O 复杂度是 $$O(N^2)$$ 的，因为在一个方向上连续的读取，在另一个方向上就是不连续的。即使是交换了变量的枚举顺序也是如此。

#### 算法

缓存无关算法依赖于以下分块矩阵恒等式：

$$
\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}^T=
\begin{pmatrix}
A^T & C^T \\
B^T & D^T
\end{pmatrix}
$$

据此可以用一个分而治之算法递归解决原问题：

1. 将输入的矩阵分为四部分。
2. 递归转置每个矩阵。
3. 交换次对角线上的分块矩阵。

在矩阵上实现分而治之比较复杂，但主要的方法是一致的。为了减少复制子矩阵的开销，我们最好是使用一个“视图”，这一理念在许多编程语言中都有体现。例如 c++ 就有 `std::string_view`。说回正题，当我们的数据可以被塞入 L1 缓存时就没有必要再次细分，虽然说该算法是缓存无关的，但是如果可以知道最底层的缓存大小，可以减少许多不必要的开销。我们同时还需要小心的处理奇数，因为此时不能将数据等分为 4 个大小相同的矩阵。

```rust
fn matrix_transpose_cache_oblivious_rec<T>(matrix: *mut T, n: usize, tot_n: usize) {
    if n <= 32 {
        for i in 0..n {
            for j in 0..i {
                unsafe {
                    std::ptr::swap(matrix.add(i * tot_n + j), matrix.add(j * tot_n + i));
                }
            }
        }
    } else {
        let mid = n / 2;
        unsafe {
            matrix_transpose_cache_oblivious_rec(matrix, mid, tot_n);
            matrix_transpose_cache_oblivious_rec(matrix.add(mid), mid, tot_n);
            matrix_transpose_cache_oblivious_rec(matrix.add(mid * tot_n), mid, tot_n);
            matrix_transpose_cache_oblivious_rec(matrix.add(mid * tot_n + mid), mid, tot_n);
        }
        for i in 0..mid {
            for j in 0..mid {
                unsafe {
                    std::ptr::swap(
                        matrix.add(i * tot_n + j + mid),
                        matrix.add((i + mid) * tot_n + j),
                    );
                }
            }
        }
        if n % 2 == 1 {
            let last_row = n - 1;
            for i in 0..n - 1 {
                unsafe {
                    std::ptr::swap(
                        matrix.add(last_row * tot_n + i),
                        matrix.add(i * tot_n + last_row),
                    );
                }
            }
        }
    }
}

pub fn matrix_transpose_cache_oblivious<T>(matrix: &mut [T], n: usize) {
    matrix_transpose_cache_oblivious_rec(matrix.as_mut_ptr(), n, n);
}
```

这个缓存无关算法的 I/O 复杂度似乎为 $$O(\frac{N^2}{B}\log_2 \frac{N}{M})$$，我认为此处原文中存在错误。因为整个算法会迭代 $$O(\log_2 \frac{N}{M})$$ 层，每层中移动副对角矩阵的时候都需要移动整个矩阵中一半的元素 $$O(N^2)$$，但是由于缓存连续，因此可以得到 $$O(\frac{N^2}{B}\log_2 \frac{N}{M})$$ 的算法。

其它论文中的正确算法如下：

```rust
fn matrix_transpose_cache_oblivious_fast_rec<T>(
    matrix: *mut T,
    x0: usize,
    y0: usize,
    x1: usize,
    y1: usize,
    tot_n: usize,
) {
    if x1 - x0 <= 16 && y1 - y0 <= 16 {
        for i in x0..x1 {
            for j in y0..y1.min(i) {
                unsafe {
                    std::ptr::swap(matrix.add(i * tot_n + j), matrix.add(j * tot_n + i));
                }
            }
        }
        return;
    }
    if (x1 - x0) > (y1 - y0) {
        let mid_x = x0.midpoint(x1);
        matrix_transpose_cache_oblivious_fast_rec(matrix, x0, y0, mid_x, y1, tot_n);
        matrix_transpose_cache_oblivious_fast_rec(matrix, mid_x, y0, x1, y1, tot_n);
        return;
    }

    let mid_y = y0.midpoint(y1);
    matrix_transpose_cache_oblivious_fast_rec(matrix, x0, y0, x1, mid_y, tot_n);
    matrix_transpose_cache_oblivious_fast_rec(matrix, x0, mid_y, x1, y1, tot_n);
}

pub fn matrix_transpose_cache_oblivious_fast<T>(matrix: &mut [T], n: usize) {
    matrix_transpose_cache_oblivious_fast_rec(matrix.as_mut_ptr(), 0, 0, n, n, n);
}
```

虽然交换时仍然是在整个矩阵上的对角线做交换，内存空间上相隔的非常远，但是实际进行交换操作时的矩阵大小保证能够全部放入内存中，并由递归算法保证在递归至上一层时，访问的数据是邻近的。

#### Benchmark

基准测试表示在转置 $$2000\times 2000$$ 的矩阵时，基本的算法用时 7.56ms，而原文的缓存无关的算法用时 6.16ms，修正后的缓存无关算法用时 3.09ms。

### 矩阵乘法

接下来我们考虑更复杂的矩阵乘法，考虑 $$C=AB$$，有：

$$
c_{ij} = \sum_k a_{ik} b_{kj}
$$

最简单的算法就是直接按照定义去计算，为了方便，我们定义一个 `Matrix<T>` 类：

```rust
pub struct Matrix<T> {
    data: Vec<T>,
    n: usize,
    m: usize,
}

impl<T: Default + Clone> Matrix<T> {
    pub fn new(n: usize, m: usize) -> Self {
        let size = n * m;
        let data = vec![T::default(); size];
        Matrix { data, n, m }
    }

    pub fn from_vec(data: Vec<T>, n: usize, m: usize) -> Self {
        assert_eq!(data.len(), n * m);
        Matrix { data, n, m }
    }
}

impl<T: std::ops::Mul<Output = T> + std::ops::Add<Output = T> + Default + Clone> Matrix<T> {
    pub fn simple_mul(&self, other: &Matrix<T>) -> Matrix<T> {
        assert_eq!(self.m, other.n);
        let mut result = Matrix::new(self.n, other.m);
        for i in 0..self.n {
            for j in 0..other.m {
                let mut sum = T::default();
                for k in 0..self.m {
                    sum = sum + self.get(i, k).clone() * other.get(k, j).clone();
                }
                result.set(i, j, sum);
            }
        }
        result
    }
}
```

这样的算法总共需要 $$O(N^3)$$ 次块 I/O，理由和之前的一样。

一个比较知名的优化方法是先将 $$B$$ 进行转置：

```rust
pub fn transpose_mul(&self, other: &Matrix<T>) -> Matrix<T> {
    assert_eq!(self.m, other.n);
    let other_transposed = other.transpose();
    let mut result = Matrix::new(self.n, other.m);
    for i in 0..self.n {
        for j in 0..other.m {
            let mut sum = T::default();
            for k in 0..self.m {
                sum = sum + self.get(i, k).clone() * other_transposed.get(j, k).clone();
            }
            result.set(i, j, sum);
        }
    }
    result
}
```

此时整个算法会在 $$O(N^3/B)$$ 的 I/O 次数内结束。

看上去似乎已经达到了最优，但是我们还能更进一步。

#### 算法

缓存无关的矩阵乘法和矩阵转置的思路相同。我们需要将数据不断的分治，知道它的大小可以被放入到最小的缓存中 ($$N^2 \leq M$$)。对于矩阵乘法，在分块矩阵下的结果为：

$$
\begin{pmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22} \\
\end{pmatrix} \begin{pmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22} \\
\end{pmatrix} = \begin{pmatrix}
A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\
A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\
\end{pmatrix}
$$

实现起来略微有点困难，因为我们现在需要递归八次矩阵乘法。不过我们仍然可以每次只进行二分的分块：

```rust
fn mul_rec(
        &self,
        ax0: usize,
        ax1: usize,
        ay0: usize,
        ay1: usize,
        other: &Matrix<T>,
        by0: usize,
        by1: usize,
        result: &mut Matrix<T>,
        cx0: usize,
        cx1: usize,
    ) {
    if ax1 - ax0 <= 16 && ay1 - ay0 <= 16 && by1 - by0 <= 16 {
        for i in ax0..ax1 {
            for j in by0..by1 {
                let mut sum = result.get(i, j).clone();
                for k in ay0..ay1 {
                    sum = sum + self.get(i, k).clone() * other.get(k, j).clone();
                }
                result.set(i, j, sum);
            }
        }
        return;
    }

    if ay1 - ay0 > 16 {
        let mid_y = ay0.midpoint(ay1);
        self.mul_rec(ax0, ax1, ay0, mid_y, other, by0, by1, result, cx0, cx1);
        self.mul_rec(ax0, ax1, mid_y, ay1, other, by0, by1, result, cx0, cx1);
        return;
    }

    if ax1 - ax0 > 16 {
        let mid_x = ax0.midpoint(ax1);
        self.mul_rec(ax0, mid_x, ay0, ay1, other, by0, by1, result, cx0, cx1);
        self.mul_rec(mid_x, ax1, ay0, ay1, other, by0, by1, result, cx0, cx1);
        return;
    }
    // if by1 - by0 > 16 {
    let mid_x = cx0.midpoint(cx1);
    self.mul_rec(ax0, ax1, ay0, ay1, other, by0, mid_x, result, cx0, mid_x);
    self.mul_rec(ax0, ax1, ay0, ay1, other, mid_x, by1, result, mid_x, cx1);
}

pub fn mul(&self, other: &Matrix<T>) -> Matrix<T> {
    assert_eq!(self.m, other.n);
    let mut result = Matrix::new(self.n, other.m);
    self.mul_rec(
        0,
        self.n,
        0,
        self.m,
        other,
        0,
        other.m,
        &mut result,
        0,
        other.m,
    );
    result
}
```

#### 分析

这个算法的 I/O 复杂性由如下递归式确定：

$$
T(N) = \begin{cases}
O(\frac{N^2}{B}) & N \leq \sqrt M & \text{(we only need to read it)} \\
8 \cdot T(N/2) + O(\frac{N^2}{B}) & \text{otherwise}
\end{cases}
$$

假设最简单的那个情形花费 $$O(1)$$ 时间，这个算法会在 $$O((\frac{N}{\sqrt{M}})^3)$$ 内结束，因此原本的总 I/O 复杂度为：

$$
T(N) = O\left(\frac{(\sqrt{M})^2}{B} \cdot \left(\frac{N}{\sqrt M}\right)^3\right) = O\left(\frac{N^3}{B\sqrt{M}}\right)
$$

比 $$O(N^3/B)$$ 要更好一些。

#### Benchmark

基准测试表明，在计算 $$1024\times 1024$$ 大小的两个 `f64` 矩阵乘法时，最简单的矩阵乘法花费 3.39s，转置后乘法花费 1.39s，而缓存无关的算法花费 0.95s。

### Strassen Algorithm

和之前类似，在将矩阵分块时原本需要进行 8 次乘法。但是实际上对于大小都为 $$\frac{n}{2}$$ 的八个方阵，可以只进行 7 次乘法就能得到原本矩阵乘法的结果。这使得分治算法会在 $$O(n^{\log_2 7})\approx O(n^{2.81})$$ 的时间内结束。同时在外存模型下也有类似的渐进 I/O 复杂度。

当我们执行以下乘法时：

$$
\begin{pmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22} \\
\end{pmatrix}
=\begin{pmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22} \\
\end{pmatrix}
\begin{pmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22} \\
\end{pmatrix}
$$

Strassen Algorithm 给出：

$$
\begin{aligned}
   M_1 &= (A_{11} + A_{22})(B_{11} + B_{22})   & C_{11} &= M_1 + M_4 - M_5 + M_7
\\ M_2 &= (A_{21} + A_{22}) B_{11}             & C_{12} &= M_3 + M_5
\\ M_3 &= A_{11} (B_{21} - B_{22})             & C_{21} &= M_2 + M_4
\\ M_4 &= A_{22} (B_{21} - B_{11})             & C_{22} &= M_1 - M_2 + M_3 + M_6
\\ M_5 &= (A_{11} + A_{12}) B_{22}
\\ M_6 &= (A_{21} - A_{11}) (B_{11} + B_{12})
\\ M_7 &= (A_{12} - A_{22}) (B_{21} + B_{22})
\end{aligned}
$$

如果你想的话可以自行带换一下。

原文中提到尽管现在有一个[实现](https://arxiv.org/pdf/1605.01078)可以在矩阵大于 $$2000$$ 的时候有比较好的效率，但是目前没有主流线性代数库使用 Strassen algorithm。

而且理论上这个技术也被扩展到将矩阵分解为更小的子矩阵。到 2020 年位置，目前最好的渐进复杂度是 $$O(n^{2.3728596})$$。能否在 $$O(n^2)$$ 或者 $$O(n^2 \log^k n)$$ 内计算矩阵乘法仍然是一个开放性问题。

### 扩展阅读

要获得扎实的理论观点，请考虑阅读 Erik Demaine 的 [Cache-Oblivious Algorithms and Data Structures](https://erikdemaine.org/papers/BRICS2002/paper.pdf)。


## 八 - 时空局部性

为了准确地评估一种算法在内存操作方面的性能，我们需要考虑缓存系统的多个特征：缓存层的数量、每一层的内存和块大小、每一层用于缓存淘汰的确切策略，有时甚至是内存分页机制的细节。

在设计算法的第一阶段，不考虑这些小细节，而是从中抽象出来是很有帮助的。比起分析理论上的缓存命中率，用更定性的术语来描述缓存的性能通常更有意义。

在这个语境下，我们可以从两种方面来讨论缓存的重用率：

- _时间局部性_ 指的是在相对较短的时间内访问相同的数据，这样这些数据在两次访问之间更容易保留在缓存当中。
- _空间局部性_ 指的是使用在内存上相互接近的元素，这样它们更有可能在一个数据块中被一次读取到内存。

换句话说，时间局部性是指有多大可能很快再次请求同一内存位置的时间，而空间局部性是指有多大可能立即访问当前请求附近的位置的时间。

### 深度优先 vs. 宽度优先

考虑一个分治算法，比如归并排序，我们有两种方式实现它。

- 我们可以递归地实现它，或者叫深度优先搜索，也是最常使用的方法。
- 我们也可以迭代地实现它，或者叫宽度优先搜索。先处理最底层，然后向上合并，每次合并两个有序列表中的元素。

第二种看上去有些麻烦，但貌似会更快一些，因为递归总是偏慢的，不是吗？

一般来讲，递归确实会慢一些，但是在许多分治算法中却并不是。虽然迭代方法的优点是只进行顺序的 I/O，但递归方法具有更好的时间局部性：当一个段被完全放入缓存时，它会留在缓存并被用于所有更低的递归层，从而在以后获得更好的访问时间。

换句话说，DFS 只需要分割数组 $$O(\log \frac{N}{M})$$ 次后，剩下的递归层就会在 $$O(\frac{N}{B})$$ 的时间内完成，总的 I/O 复杂度为 $$O(\frac{N}{B} \log \frac{N}{M})$$。而迭代时总共需要 $$O(\log N)$$ 轮合并，每轮都需要执行一次 $$SCAN$$，总的 I/O 复杂度为 $$O(\frac{N}{B} \log N)$$。也就是说使用 DFS 就可以加速约 $$O(\frac{\log N}{\log N - \log M})$$。

在实际中，递归仍然会产生一些额外开销，这也是为什么我们会更常使用混合算法。我们不会一直通过分治缩小问题规模，而是递归到较小的规模上后切换为使用迭代。

### 动态规划

类似的推理可以用于动态规划。考虑经典的背包问题，给你 $$N$$ 个物品，每个物品有 $$c_i$$ 的 cost，找出一个子集使得它们的 cost 之和最大但又不超过 $$W$$。

解决这个问题的方式是使用状态 $$f[n,w]$$，表示考虑从前 $$n$$ 个元素和 cost 最大不超过 $$w$$ 时的子问题的解。python 有一个方便的 `lru_cache` 装饰器来进行记忆化搜索：

```python
@lru_cache
def f(n, w):
    # check if we have no items to choose
    if n == 0:
        return 0
    
    # check if we can't pick the last item (note zero-based indexing)
    if c[n - 1] > w:
        return f(n - 1, w)
    
    # otherwise, we can either pick the last item or not
    return max(f(n - 1, w), c[n - 1] + f(n - 1, w - c[n - 1]))
```

当计算 $$f[N,W]$$ 时，递归最多需要访问 $$O(NW)$$ 个状态，从渐近复杂度上来看是高效的，但在实际中却相当缓慢。即使在消除了 Python 递归的开销和 LRU 缓存工作所需的所有哈希表查询之后，它仍然很慢，因为它在执行过程中大部分都是随机 I/O。

我们可以做的是为动态规划创建一个二维数组，并用一个嵌套的循环替换递归，就像这样：

```cpp
int f[N + 1][W + 1] = {0}; // this zero-fills the array

for (int n = 1; n <= N; n++)
    for (int w = 0; w <= W; w++)
        f[n][w] = c[n - 1] > w ?
                  f[n - 1][w] :
                  max(f[n - 1][k], c[n - 1] + f[n - 1][w - c[n - 1]]);
```

这样的算法的 I/O 复杂性为 $$O(\frac{NW}{B})$$。由于每次迭代都使用上一层的结果，因此我们也可以使用滚动数组来压缩空间：

```cpp
bool f[W + 1] = {0};
f[0] = 1;
for (int n = 0; n < N; n++)
    for (int x = W - c[n]; x >= 0; x--)
        f[x + c[n]] |= f[x];
```

另外，我们还可以使用 `bitset` 来进一步优化空间占用，从而提高 cache 性能：

```cpp
std::bitset<W + 1> b;
b[0] = 1;
for (int n = 0; n < N; n++)
    b |= b << c[n];
```

特别的，这个问题仍然有提升的空间，我们会在之后提到。

### ST 表

_ST 表_ 是一种解决 _静态 RMQ_ 问题的数据结构。其保存一个二维数据，其中预计算了从位置 $$i$$ 往后 $$2^k$$ 个所有元素的 $$RMQ$$ 的结果。这样的数组可以使得我们在 $$O(1)$$ 时间内查询任意段的 $$RMQ$$ 结果，因为我们总是可以将其拆为两个相互之间可能重叠的段，使得它们的长度是 2 的幂次。

这意味着只要很少的计算开销就能得到任意段的 $$RMQ$$：

```cpp
int rmq(int l, int r) { // half-interval [l; r)
    int t = __lg(r - l);
    return min(mn[t][l], mn[t][r - (1 << t)]);
}
```

由于表种的长度都是 2 的幂次，因此可以通过倍增的方式来构建 ST 表。然而我们有 4 种方式可以构建这个 ST 表。`logn` 和 `maxn` 哪个维度在前？`for` 循环时先循环哪个维度？实际上，最优秀的便是如下构建方式：

```cpp
int mn[logn][maxn];

memcpy(mn[0], a, sizeof a);

for (int l = 0; l < logn - 1; l++)
    for (int i = 0; i + (2 << l) <= n; i++)
        mn[l + 1][i] = min(mn[l][i], mn[l][i + (1 << l)]);
```

这个内存布局和迭代顺序可以带来约 3 倍的加速。其 I/O 复杂度为 $$O(\frac{N\log N}{B})$$。

如果改变内存布局，则 I/O 复杂度会上升为 $$O(\frac{N\log N}{B / \log N})= O(\frac{N\log^2 N}{B})$$。

如果改变迭代顺序，此时同时需要将 `i` 改为从大到小便利。则 I/O 复杂度会变为 $$O(N\log N)$$。（如果 $$\frac{M}{B}>\log N$$ 的话，或许是 $$O(\frac{N\log N}{B})$$ 的 I/O 复杂度？）

如果两个同时改变，则 I/O 复杂度为 $$O(\frac{N\log N}{B})$$。

### 结构体数组和数组结构体 (Array-of-Structs vs. Struct-of-Arrays)

假设我们想要实现一个二叉树：

```cpp
int left_child[maxn], right_child[maxn], key[maxn], size[maxn];
```

这样的方式被成为数组结构体 (struct-of-arrays, SoA)。在大部分情况下，访问一个节点都需要访问其所有数据，这种方法会导致你不得不登台最慢的一个被获取。

然而，相比之下，如果它被存储为结构体数组（AoS），则将需要少 4 倍的块读取，因为一个节点的所有数据都存储在同一个块中并一次获取：

```cpp
struct Node {
    int left_child, right_child, key, size;
};

Node t[maxn];
```

AoS 布局通常是数据结构的首选，但 SoA 仍然有很好的用途：虽然它在搜索算法上比较差，但在线性扫描方面要好得多。

这种设计上的差异在数据处理应用中很重要。例如，数据库可以是面向行 (row-oriented) 或面向列的 (column-oriented, 也称为列式存储 columnar)：

- 当我们需要在大型数据集中搜索有限数量的对象和/或获取它们的全部或大部分字段时，使用面向行的存储格式。例如：PostgreSQL， MongoDB。
- 列式存储格式用于大数据处理和分析，也就是需要扫描所有内容以计算某些统计数据的时候。例如：ClickHouse、Hbase。

列式存储还有一个额外的优点就是可以只读取需要的字段，因为不同的字段被单独地存储在外部内存区域中。
