---
title: 高性能计算-第八章-外存
date: 2025-4-3 12:00:00 +0800
categories: [笔记, 高性能计算]
tags: [高性能计算]     # TAG names should always be lowercase
math: true
---
[原文](https://en.algorithmica.org/hpc/external-memory/)

## 序

对两个数求和需要花多长时间？作为最常使用的指令之一，只要数据已经被保存在寄存器里，那么 `add` 只需要一个周期。

但是在 `*c = *a + *b` 这种情况中，我们却需要先从内存中取出相应的操作数：

```nasm
mov eax, DWORD PTR [rsi]
add eax, DWORD PTR [rdi]
mov DWORD PTR [rdx], eax
```

从内存中获得任何数据时，总会产生数据传输的延迟。因此，内存请求并不会直接跑到它的目的地，而是先跑到一个复杂的地址转换和缓存层来减少延迟。

所以这个问题最准确的答案是：对两个数求和的时间取决于操作数的位置：

- 如果存储在 RAM 中，大约会需要 100ns，或者说用 200 个周期获取它，再用 200 个周期写回去。
- 如果操作数最近被访问过，那么它大概率已经被缓存并且很快就可以获取。不同的缓存层级（访问时间）具有不同的访问速度，最慢的可能要 ~50 个周期，最快的可能只有 4-5 个周期。
- 数据也可能存储在 _外存_ 上，此时可能需要花费 5ms，大约 $$10^7$$ 个时钟周期来访问它。

这样高的性能差异主要是因为存储硬件和 CPU 芯片并不遵循相同的增长规律。就算五十年前他们的性能差不多，现在也已经差的很远了。

![内存和 CPU 硬件的增长](/assets/External_Memory/memory-vs-compute.png)

为了减少这一点，现代内存系统变得越来越层级化，高层级的缓存会以容量为代价换取更少的延迟。容量和延迟在不同的层级可能发生指数级的变化——尤其是外存——因此对于许多内存密集型的算法，优化它们的 IO 局部性变得尤为重要。

这促使我们构建一个新的数学模型，称为 _外存模型_。在这里，唯一的内存开销是块读取和块写入，对于涉及已经读取到有限的本地内存的数据，IO 都是零开销的。它催生了一个新的关于 _外存算法_ 的领域，也是我们之后要学习的东西。

## 一 - 内存层级

现代计算机内存系统是高度层级化的。较高的层会缓冲较低的层中被经常访问的数据以减少延迟，每层的性能和成本往往有着数量级的差异。

抽象化地说，不同的存储设备都可以被表述为一个存储容量 $$W$$ 和每次读写块数据的块大小 $$B$$。每种内存都有以下比较重要的特征：

- 总大小 $$M$$；
- 块大小 $$B$$；
- 延迟，也就是获得数据所花费的时间；
- 带宽，有些时候可能会高于块大小除以延迟，这意味着 I/O 操作可以并行执行；
- 成本，各种意义上的均摊成本，包含芯片价格、能源消耗、维护开销等。

这里有 2021 年的一些硬件信息：

| Type | $$M$$      | $$B$$ | Latency | Bandwidth  | $$/GB/mo |
| :--- | :--------- | ----- | ------- | ---------- | :------- |
| L1   | 10K        | 64B   | 2ns     | 80G/s      | -        |
| L2   | 100K       | 64B   | 5ns     | 40G/s      | -        |
| L3   | 1M/core    | 64B   | 20ns    | 20G/s      | -        |
| RAM  | GBs        | 64B   | 100ns   | 10G/s      | 1.5      |
| SSD  | TBs        | 4K    | 0.1ms   | 5G/s       | 0.17     |
| HDD  | TBs        | -     | 10ms    | 1G/s       | 0.04     |
| S3   | $$\infty$$ | -     | 150ms   | $$\infty$$ | 0.02     |

### 易失性内存

RAM 以及更高层的内存被成为易失性内存，因为它们的数据会在断电时丢失。但是它足够快，这也就是为什么会被用于在计算机通电时保存数据。

从最快到最慢的易失性存储器有：

- CPU 寄存器：CPU 会使用寄存器来读取它的操作数和保存计算结果，可以说 CPU 主要就是和寄存器执行交互，访问它们是零延迟的。他们的数量十分有限（例如，只有 16 个通用的寄存器）。
- CPU 缓存：现代 CPU 往往会使用很多层缓存（L1、L2、L3，有时候甚至会到达 L4）。L1、L2 缓存往往是 CPU 核独享的，到了 L3 就变为不同核之间共享的。
- 随机访问存储器：这是第一种可扩展的存储器。如今你甚至可以在公有云上租到 TB 级别 RAM 的机器。

CPU 缓存系统中有个非常重要的概念是 _缓存行 (cache line)_，它是从 CPU 和 RAM 之间进行数据传输的基本数据单元。在大多数机器上，缓存行的大小都是 64 字节。也就是说内存被按照 64 字节大小分块，每次获取某一字节的数据时，机器都会把同一缓存行中的其它 63 字节的数据拿过来，而不管你要不要它们。

在 CPU 上的缓存行根据它的最近访问时间来确定自己的缓存等级。当被访问时会被放到最低的缓存层，随后随着时间流逝，如果没有被访问就会被驱逐到更高一层的缓存。程序员无法控制这个过程，但是了解这些工作的细节也是有益的。我们将在下一张讨论这些。

### 非易失性内存

CPU 缓存和 RAM 都只存储少量电子（这些电子在不通电时很容易泄露），而在非易失性存储器中，存储单元会保存几百个电子。这使得数据可以在不通电的情况下保存更长的时间，但代价则是性能和耐用性——几百个电子更容易和硅原子发生碰撞。

有很多方式可以持久地保存数据，但从程序员视角来看，主要有这些：

- SSD (Solid state drives, 固态硬盘)：SSD 一般有着较小的延迟 (0.1ms)，但是它们的成本也很高，因为它们的每个存储单元寿命有限，且只能写入有限的次数。不过 SSD 是固定的且没有活动部件，因此被经常使用于移动设备。
- HHD (Hard disk drives, 机械硬盘)：HHD 是一个带有读写头的物理上的旋转磁盘。为了读取一个数据，你需要等磁盘转动到指定位置，然后非常精确地将磁头指向目标位置。这导致了一个非常奇怪的延迟：随机读取一个字节可能和顺序读取 1MB 的数据花费相同的时间。由于硬盘是除了冷却系统外唯一运动的部分，HHD 经常会损坏（数据中心的 HHD 寿命大约是 3 年）。
- NAS (Network-attached storage, 网络附属存储)：NAS 使用网络设备来存储数据。现实中基本上有两种类型，第一种是 NFS (Network File System, 网络文件系统)，这是一种通过网络挂载另一台计算机的文件系统的协议。另外一种是基于 API 的分布式存储系统，最著名的是 Amazon S3，它由公有云的存储机器提供支持，内部经常会使用廉价的 HDD 或者什么奇特的存储设备。尽管 NFS 在同一个数据中心时有些时候会比 HDD 更快，但是公有云上的对象存储往往还是会有 50-100ms 的延迟。它们通常是高度分发和复制的，从而获得更好地可用性。

由于 SDD/HDD 显著的比 RAM 慢，因此这一层以及更低的层级都被称为 _外存_。

和 CPU 缓存不同，外存可以被显示控制，在有些时候非常有用。但是大多数情况下程序员只想将其抽象出来，并将其当作主存的扩展使用。在操作系统中会使用 _虚拟内存_ 实现这一点。

## 二 - 虚拟内存

早期的操作系统允许每个进程自由地读取和修改任何内存区域，包括那些分配给其他进程的内存区域。虽然这挺简约的，但是带来了很多问题：

- 如果一个进程是有 bug 或者有恶意的呢？我们如何放置进程修改其他进程的内存，同时又允许它们通过内存进行同步？
- 我们如何处理内存碎片？例如我们有 4MB 内存，进程 A 分配了 1MB，进程 B 分配了 2MB，随后进程 A 结束并释放了内存，此时进程 C 申请 2MB 的连续内存——这里就报错了因为我们只有两个分开的 1MB 内存。重启进程 B 或者停止它并把它所有的数据和指针挪动 1MB 也不是什么好方法。
- 我们如何访问非 RAM 的存储结构？我们怎么插入一个闪存然后从里面读取一个文件？

这些问题对于一些专门的计算系统不是那么重要（比方说 GPU，通常一次只有一个计算任务，拥有对计算的完整控制），但它们对于现代多任务操作系统来说是绝对必要的——操作系统使用一种叫做虚拟内存的技术解决了所有这些问题。

### 内存分页

虚拟内存给每个进程一个控制了连续空间存储的错觉，这些存储空间实际上会被映射到物理内存上很多更小的块——主存或者外存都可以。

![虚拟内存和物理内存的映射](/assets/External_Memory/virtual-memory.jpg)

为了实现这一点，内存物理空间被划分为 _页_（通常为 4KB 大小），也是程序可以通过操作系统申请的最小内存单元。存储系统会维护一个特殊的数据结构 _页表_，包含了虚拟页到物理页的映射。当一个进程使用它的虚拟地址访问数据时，存储系统就会计算页号（将虚拟地址下除 4096，或者说右移 12 位），然后在页表中查找它的物理地址，将读或写请求转发到实际存储数据的位置。

由于每次内存请求都需要内存转换，同时内存页的编号也有很多（16GB / 4KB = 4M），地址转换本身就是一个难题。一种加速方式是给页表设置一个专用的 cache，translation lookaside buffer (TLB)，另一种是增加页的大小，通过减少页的粒度来减少页的数量。

### 映射到外存

虚拟内存的机制也允许相当直接地使用外部内存类型。现代操作系统支持内存映射，它允许你打开一个文件并使用它的内容，就好像它们在主内存中一样：

```cpp
// open a file containing 1024 random integers for reading and writing
int fd = open("input.bin", O_RDWR);
// map it into memory      size  allow reads and writes  write changes back to the file
int* data = (int*) mmap(0, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
// sort it like if it was a normal integer array
std::sort(data, data + 1024);
// changes are eventually propagated to the file
```

这里映射了一个 4K 的文件，我们可以用一个内存页保存。但当我们打开一个更大的文件时，读入会在我们需要的时候惰性完成，写入也会被缓冲下来直到需要将页写回（通常是程序终止或者系统没有 RAM 空间了）。

另一种技术有相同的原理，但却有相反的目的，它被称为 _交换文件_。它允许操作系统在没有足够的实际 RAM 时自动使用 SSD 或 HDD 的一部分作为主存的扩展。这让内存空间较小的系统只会经历夸张的减速而不是直接崩溃。

这种主存和外存的无缝集成方式本质上把 RAM 变成了外存的“L4缓存”，从算法设计的角度来看，是一种很便捷的思考方式。

## 三 - 外存模型

为了解释内存瓶颈算法的性能，我们需要一个对块 I/O 操作更加敏感的计算模型。

### 缓存感知模型 (Cache-Aware Model)

在标准 RAM 模型中，我们忽略了基本运算需要不同的时间来完成的这一事实。更重要的是，它没有区分在不同类型存储器上的操作，从 RAM 读取需要 ~50ns，而从 HDD 中读取需要 ~5ms，中间相差了 $$10^5$$ 倍。

考虑到这一点，在 _外存模型_ 中，我们只考虑 I/O 操作的开销。准确地说，我们只考虑一层缓存，并且假设它有如下性质：

- 输入数据的大小为 $$N$$，被存储在 _外存_ 上，我们只能同时读取 $$B$$ 个连续的元素。（读取一个块和块中的一个元素花费相同的时间）
- 我们可以在内存中存储至多 $$M$$ 个元素，也就是至多存储 $$\left\lfloor \frac{M}{B} \right\rfloor$$ 个块。
- 我们只关心 I/O 操作，任何其它操作都假设为零开销。
- 额外认为 $$N \gg M \gg B$$。

在这个模型当中，一个算法的性能由它的高层次 _I/O 操作_ 或者 IOPS 决定——换句话说，在整个运行过程中从外存中读取或写入块的数量。

虽然我们将要讨论的底层分析技术适用于缓存层次结构中的任何一层，但我们最关心的是将 RAM 当作内存，将 SSD 或者 HDD 当作外存的模型。在这个设置下，块大小 $$B$$ 大约是 1MB，内存大小 $$M$$ 大约是几 GB，而外存大小 $$N$$ 可以达到几 TB。

（话说最常用的居然不是将 Cache 当作内存，RAM 当作外存的模型吗？）

### 数组扫描 (Array Sacn)

作为最简单的例子，当我们想要计算一个数组的总和式，至少需要便利每个元素一次，我们可以每次加载 $$O(B)$$ 个元素然后求和。因此在外存模型中，求和或者其它数组扫描算法的复杂度为：

$$
SCAN(N) \triangleq O\left(\left\lfloor \frac{N}{B} \right\rfloor\right) IOPS
$$

我们跳过具体的代码实现。注意，在很多情况下操作系统会自动执行 buffer 操作。即使数据只是从普通文件重定向到标准输入，操作系统也会通过缓冲流并以约 4KB 的块（默认情况下）读取它。

## 四 - 外存排序

现在让我们使用外存模型来设计一个高效算法。外存排序往往基于标准的归并排序算法，因此先来看一下简单的构件。

### 归并

**问题**。给定两个有序数组 $$a$$，$$b$$，给出一个包含它们的所有元素的有序的数组。

标准的双指针计数可以这样实现：

```rust
fn merge<T: Ord + Clone>(left: &[T], right: &[T]) -> Vec<T> {
    let mut merged = Vec::with_capacity(left.len() + right.len());
    let mut iter_left = left.iter().peekable();
    let mut iter_right = right.iter().peekable();
    
    while iter_left.peek().is_some() && iter_right.peek().is_some() {
        if iter_left.peek() <= iter_right.peek() {
            merged.push(iter_left.next().unwrap().clone());
        } else {
            merged.push(iter_right.next().unwrap().clone());
        }
    }
    while iter_left.peek().is_some() {
        merged.push(iter_left.next().unwrap().clone());
    }
    while iter_right.peek().is_some() {
        merged.push(iter_right.next().unwrap().clone());
    }

    merged
}
```

考虑内存操作，我们只是线性读取 $$a$$ 和 $$b$$ 并线性写入 $$c$$。由于读和写操作都可以被缓冲（从而实现块读取和块写入），因此可以在 $$SCAN(N+M)$$ 的 I/O 操作内完成。

到目前为止，这些例子都很简单，除了我们将最终答案除以了块大小 $$B$$，分析它们与标准的 RAM 模型没有太大的不同。但我们将要考虑一个特别的情况。

**k-路归并**。考虑归并算法的一个修改版本，不再合并两个数组，而是同时合并 $$k$$ 个数组，这 $$k$$ 个数组的总长为 $$N$$。同样的，可以选择 $$k$$ 个数组中最小的元素写入 $$c$$，并推进对应数组的迭代器。

在标准 RAM 模型中，随着 $$k$$ 的增加时间复杂度也会增加，因为我们需要 $$O(k)$$ 的时间（最简单的实现）来找到最小的元素。但在外存模型中，我们在内存上所做的任何操作都是零开销的，因此只要我们可以在内存中存入 $$(k+1)$$ 个块（$$k$$ 个块用于读取，$$1$$ 个块用于写入），那么复杂度就不会增加。也就是说 $$k=O\left(\frac{M}{B}\right)$$ 时都不会导致 IO 数增加。

还记得我们之前假设了 $$M \gg B$$ 吗？其更加形式化的描述方式是存在一个 $$\varepsilon > 0$$ 使得 $$M \geq B^{1+\varepsilon}$$，也被称为 _高缓存假设_。其作用在于使得 $$\frac{M}{B} \geq B^\varepsilon$$，确保内存中可以存储多项式级别的块。在外存算法中，往往会需要在内存中存储亚多项式数量的块，例如 $$O(\log N)$$，高缓存假设可以保证这些外存算法的复杂性。

### 归并排序

普通的归并排序的时间复杂度为 $$O(N\log_2 N)$$（外存算法似乎比较关系对数的底数，尽管它实际上可以通过换底公式以常数代价换成另一个底数）。在 $$O(\log_2 N)$$ 层迭代中，每一层都会需要遍历 $$N$$ 个元素进行两两归并。

在外存模型中，对于大小为 $$B$$ 的块，我们可以零开销的排序内部元素，因为这样不会产生新的 I/O。我们可以认为排序 $$O(B)$$ 个元素的时间复杂度是 $$O(1)$$ 的。这样我们可以把整个数组分成 $$O(\frac{N}{B})$$ 个块，把它们当作基础元素进行归并。最底层我们使用 k-路归并，因为这是零开销的，。

![k-路归并](/assets/External_Memory/k-way.png)

此时需要 $$O(\log_2 \frac{N}{M})$$ 层归并操作（注意并不是 $$O(\log_2 \frac{N}{B})$$，这是因为合并 $$\frac{M}{B}$$ 个块的时间复杂度是 $$O(1)$$ 的，因为它们全部都在内存当中）。每层的归并则是一个 $$SCAN$$ 操作，因此总的 IOPS 是：

$$
O\left(\frac{N}{B} \log_2 \frac{N}{M}\right)
$$

这相当快。如果我们有 1GB 的内存和 10GB 的数据，这实际上意味着我们需要比单纯的 $$SCAN$$（单纯的读取数据）多 3 倍的努力。有趣的是，我们可以做得更好。

### k-路归并

如果我们每一层都使用 $$k$$-路归并的话，会产生什么结果？

每一层仍然需要进行一次 $$SCAN$$ 来进行 $$k$$-路归并，但是此时 $$k=\frac{M}{B}$$，层数将会大大减少，变为 $$\log_{\frac{M}{B}} \frac{N}{M}$$，整体的复杂度变为：

$$
SORT(N) \triangleq O\left(\frac{N}{B} \log_{\frac{M}{B}} \frac{N}{M} \right)
$$

### 真实实现

在真实场景下，相较于使用 $$\log_{\frac{M}{B}} \frac{N}{M}$$ 层归并，我们可以只使用两层：一层用于多次合并 $$\frac{M}{B}$$ 个大小为 $$B$$ 的块，一层用于合并得到最终结果。通过这种方法，我们可以只读取数据集两次。在 GB 级别的 RAM 和 1MB 的块大小下，这种方式可以排序 TB 级别的数据。（$$SORT$$ 中对数的结果等于 1 时）

具体的实现请查看[原文](https://en.algorithmica.org/hpc/external-memory/)，因为我没有数据，也不想搓这些数据占用硬盘资源。不过总体来讲还是比较简单的。

### Joining（连接）

排序主要被用于其它操作的步骤之一，外存排序的一个比较重要的应用是 joining（比方说 SQL join），经常出现在数据库或者其它数据处理应用。

**问题**。给定 $$(x_i,a_{x_i})$$ 和 $$(y_i, b_{y_i})$$ 两个列表，输出一个列表 $$(k,a_{x_i}, b_{y_j})$$ 满足所有的元素都有 $$k=x_i=y_j$$。（博主：我觉得原文写的有点问题，这里改了下）

最优的方法是将两个列表排序，然后使用标准的双指针方法进行合并。IPOS 和 $$SORT$$ 一致，如果已经排好了序那么就只需要 $$SCAN$$ 的 I/O。这也是为什么许多数据处理软件（数据库，MapReduce）都喜欢让表保持部分有序。

**其它方法**。不过上述分析只适用于我们不能将所有数据读入内存。否则在现实当中，我们有一些其它更快的方法。

最简单的方式就是 _哈希 join_，用 python 来写的话大概就长这样：

```python
def join(a, b):
    d = dict(a)
    for x, y in b:
        if x in d:
            yield d[x]
```

在外存中，使用哈希表 join 两个列表基本是不可能的。因为最坏情况下相邻的一些元素会被 hash 的很远，导致我们需要读取 $$O(N)$$ 次块。

还有一种方法就是使用基数排序。在实际中，基数排序会需要 $$O(\frac{N}{B}\cdot w)$$ 次块读写。其中 $$w$$ 是在某个基数下 $$N$$ 个键中最大值的位数，也就是基数排序所需要的轮数。如果基数（桶的数量）是 $$O(\frac{M}{B})$$ 的，那么每轮基数排序就需要 $$SCAN$$ 的时间进行一轮读和写。因此总时间是 $$O(\frac{N}{B}\cdot w)$$ 的。当数据集足够大且都是小键值时，基数排序可能会更快。

## 五 - 列表排序 (List Ranking)

在本节中，我们会使用外存排序和 join 解决一个看上没啥用，但实际上用于许多外存算法或者并行算法中的问题。

**问题**。给一个和单链表，计算其中每个元素的 rank。这里将 rank 定义为到最后一个元素的距离。（博主：怎么不是第一个？）

![LiskRank 例子](/assets/External_Memory/list-ranking.png)
_List Ranking 的一个例子（博主认为上半部分图中大于 0 的数字应该减一，毕竟是从 0 开始编号的）_

这个问题在 RAM 模型中可以简单地解决：只需要带个计数器遍历整个列表。但是这种指针跳转在外存模型下不能很好地工作，因为列表节点是随机存储的。在最坏的情况下，读取每个新节点都可能需要读取一个新的块。

### 算法

考虑这个问题的一个更一般的版本。现在每个元素除了其地址 $$i$$ 外，还有一个权重 $$w_i$$，而我们需要计算它的前缀和（按照在链表上的顺序、**不包含**当前元素的前缀和）放置到其对应的地址上。如果想要解决解决原始问题，只需要让每个 $$w_i=1$$。

原文写得非常奇怪。这篇博客里的内容是我从其它地方总结得到的，我并不能保证正确性。

该算法的主要思想是去除部分元素，将自身的权重留给前一个元素，然后递归求解。最后再使用被删除的元素和权重重建原问题的解。

假设我们有三个链表上连续的元素 $$x,y,z$$，我们删去 $$y$$ 并将 $$w_y$$ 合并至 $$w_x$$ 中。做完前缀和后，你会发现 $$rw_x, rw_z$$ 就是最终的结果。因此我们在恢复时只需要加入 $$y$$，并计算 $$rw_y = rw_x + w_y$$。

在这里，每次删除元素时必须保证被删除的元素不能是在链表上连续的，否则的话在递归和恢复时会产生不必要的麻烦。最理想的情况下我们希望将链表拆分成偶数和奇数位置的两个链表，但是这样子做的难度并不比解决原问题简单多少。因此有一种处理方式是随机，我们给每一个元素投掷一个硬币，如果链表上的上一个元素的硬币为正，同时当前元素的硬币为反，则可以删除掉这个元素。这样的随机过程在期望上会删除掉链表中 $$\frac{1}{4}$$ 的元素。

假设链表中的每个元素是三元组 $$(i, succ_i, w_i)$$，则整个算法的流程可以被总结成这样：

1. 投硬币，并令链表开头的元素投出来的必须是正面，防止其被删除。随后通过前述的方式将链表进行压缩得到 $$(i, succ'_i, w'_i)$$。
2. 递归解决压缩后链表的结果，得到 $$(i, rw_i)$$。
3. 将被删除的元素重新加入到链表中。

当链表的长度小于 $$M$$ 时，可以使用 $$O(M/B)$$ 次 IO 直接读入到内存中解决。我们重点讨论如何实现步骤 1 和步骤 3。

对于步骤一，每个元素现在是四元组 $$(i, succ_i, w_i, p_i)$$，其中 $$p_i$$ 表示投硬币的结果，同时有 $$p_0 = 0$$。我们将其以 $$succ_i$$ 为键进行外部排序，外部排序后的结果可以被解释为 $$(prev_i, i, w_{prev_i}, p_{prev_i})$$。我们之后将对这两个序列归并地按照键 $$i$$ 进行 join 操作，构建保留下来的数组 $$L$$ 和被删除的 $$D$$。

- 如果 $$p_{prev_i} = 1$$ 且 $$p_i = 0$$，则当前元素需要被删除，向 $$L$$ 中添加三元组 $$(prev_i,succ_i,w_{prev_i} + w_i)$$，向 $$D$$ 中添加 $$(i, prev_i, w_i)$$。
- 否则，当前元素需要被保留，向 $$L$$ 中添加三元组 $$(prev_i, i, w_{prev_i})$$。

递归地处理 $$L$$ 数组，假设已经得到了 $$(i, rw_i)$$。

将 $$D$$ 按照 $$prev_i$$ 作为键进行外部排序，并将三元组重新解释为 $$(succ_i, i, w_{succ_i})$$。将之前的结果数组 $$(i, rw_i)$$ 和 $$D$$ 进行归并的 join 操作，同时保留 $$(i, rw_i)$$ 的所有值（貌似被叫做 Left Join？）。如果有相同的 $$i$$，则额外向结果数组中插入 $$(succ_i, rw_i + w_{succ_i})$$。

这个算法的 IO 复杂度为：

$$
T(N)=T(\frac{3}{4} N) + SORT(N)= SORT(N)
$$
