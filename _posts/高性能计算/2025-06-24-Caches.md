---
title: 高性能计算-第九章-RAM & CPU 缓存
date: 2025-6-24 12:00:00 +0800
categories: [笔记, 高性能计算]
tags: [高性能计算]     # TAG names should always be lowercase
math: true
---
[原文](https://en.algorithmica.org/hpc/cpu-cache/)

## 序

在上一章中，我们讨论了外存模型及其理论分析。

虽然外存模型对于在 HDD 或者网络存储上的复杂度基本是精确的，因为此时内存中的计算操作确实可以忽视。然而在更低的层级比如 cache 上，这个就不太准确了，因为此时 I/O 和计算的耗时具有可比性。

为了更好地对内存算法进行优化，我们需要深入到 CPU 的缓存区系统。本文并不学习 Intel 枯燥的文档，而是在一些 benchmark 上进行试验。

### 实验设置

请查看原文。由于这一章节存在大量的绘图，因此很多会使用原文的 cpp 代码和图案。

### 扩展

本章的灵感来自 Igor Ostrovsky 的 [Gallery of Processor Cache Effects](https://igoro.com/archive/gallery-of-processor-cache-effects/) 和 Ulrich Drepper 的 [What Every Programmer Should Know About Memory](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf)，这两本书都可以作为很好的辅助读物。

## 一 - 内存带宽

在 CPU 寄存器和 RAM 之间，存在一个称为 _缓存_ 的层级，用于加速对高频数据的访问。这里的加速有两方面含义：

- 延迟：从开始读写到真正访问到数据的时间。
- 带宽：在每个单位时间内可处理的内存操作的数量。

对于许多算法来说，内存带宽是缓存系统最重要的特性。同时，它也是最容易测量的，所以我们就从它开始。

在实验中，我们创建一个大小为 $$N$$ 的数组并迭代 $$K$$，增加其中元素的值：

```cpp
int a[N];

for (int t = 0; t < K; t++)
    for (int i = 0; i < N; i++)
        a[i]++;
```

通过改变 $$N$$ 和调整 $$K$$ 使得操作总数保持不变，我们可以得到下图：

![缓存层大小](/assets/Caches/inc.svg)
_虚线表示缓存层 ($$L_1,L_2,L_3$$) 的大小_

我们可以观察到以下效应：

- 当整个数组可以放入最底的 Cache 时，瓶颈由 CPU 决定。此时随着数组的增大，载入第一个元素的耗时占比越低，因此会有一定的增长。
- 但在超过缓存后，性能就不断下降，直到超出 L3 缓存后稳定在 2 GFLOPS。

这种现象出现在许多轻量级循环当中。

### 频率缩放

所有的 CPU 缓存层都和处理器放在一起，因此带宽、延迟和所有其他特性都随时钟频率的变化而变化。另一方面，RAM 有它自己的固定时钟，它的特性会保持不变。我们可以通过重新运行相同的基准测试并打开 turbo boost （睿频，一种动态调整 CPU 时钟频率的技术）来观察到这一点：

![超频](/assets/Caches/boost.svg)

这一细节在比较不同的算法实现时比较重要，因为相同的算法在不同的 CPU 上可能带来不同的加速比。你可以看到图中的算法在数据超出 L3 后的性能是一致的，但是在能够放入 Cache 时则取决于 CPU 的时钟频率。

由于这个原因，在做 benchmark 时建议固定时钟频率。原文中由于睿频不够稳定，因此关闭了它并在 2GHz 的 CPU 上做实验。

### 定向访存

之前的循环需要同时执行读和写操作：读入数据，自增，写回数据。但在很多时候，我们也需要只执行读操作或者只执行写操作。

计算一个数组的和只需要读操作，而为一个数组初始化则只需要写操作：

```cpp
for (int i = 0; i < N; i++)
    s += a[i];

for (int i = 0; i < N; i++)
    a[i] = 0;
```

这两个循环都会被编译器向量化，而第二个循环实际上被 `memset` 取代，因此 CPU 并不是这里的瓶颈（除非数组可以完全放到 L1 缓存里面）。

![定向访存](/assets/Caches/directional.svg)

单向和双向访存的性能不同是因为它们公用同一个 Cache 和内存之间的总线和其它 CPU 设施。例如在 RAM 的情况下，这会导致纯读入和同时读写的两种算法产生约 2 倍的差距。但是在 L2 缓存内，此时的性能瓶颈并不是总线，因此性能只下降了约 15%。

另一个有意思的是，当超出 L2 缓存到达 L3 缓存时，纯读入的循环和同时读写的循环表现出相似的性能。这是因为 CPU 总会将访问的数据移动到最快的 Cache 上。当读入数据时这没有问题，但是当写入数据时，会产生一次隐式的读入——所以需要两倍的总线带宽。

### 绕过缓存

我们可以使用 _非临时内存访问 (non-temporal memory accesses)_ 来阻止 CPU 隐式地获取数据。要做到这一点，我们需要更直接地重新实现归零循环，而不依赖于编译器的自动向量化。

简单来讲，`memset` 和自动向量化就是通过 SIMD 指令以 32 字节的大小来移动数据：

```cpp
const __m256i zeros = _mm256_set1_epi32(0);

for (int i = 0; i + 7 < N; i += 8)
    _mm256_store_si256((__m256i*) &a[i], zeros);
```

Rust 中可以使用 `std::arch` 库来调用 SIMD：

```rust
use std::arch::x86_64::*;

#[target_feature(enable = "avx")]
unsafe fn zero_array_avx(a: &mut [i32]) {
    let zeros = _mm256_set1_epi32(0);

    // 主循环
    a.chunks_exact_mut(8).for_each(|chunk| {
        unsafe { _mm256_store_si256(chunk.as_mut_ptr() as *mut __m256i, zeros) };
    });

    // 剩余部分
    a.chunks_exact_mut(8).into_remainder().fill(0);
}
```

还可以再包装一层，从而防止不支持 SIMD：

```rust
fn zero_array(a: &mut [i32]) {
    if is_x86_feature_detected!("avx") {
        unsafe { zero_array_avx(a) };
    } else {
        a.fill(0);  // 回退到普通实现
    }
}
```

我们可以将其中的存储部分换成 _非临时_ 形式：

```rust
use std::arch::x86_64::*;

#[target_feature(enable = "avx")]
unsafe fn zero_array_avx(a: &mut [i32]) {
    let zeros = _mm256_set1_epi32(0);

    // 主循环
    a.chunks_exact_mut(8).for_each(|chunk| {
        unsafe { _mm256_stream_si256(chunk.as_mut_ptr() as *mut __m256i, zeros) };
    });

    // 剩余部分
    a.chunks_exact_mut(8).into_remainder().fill(0);
    _mm_sfence();
}
```

> SIMD 指令需要注意内存对齐，在本文的代码中并没有处理这一部分。另外 `_mm_sfence` 是流存储的写内存屏障，用于确保流存储操作完成，避免乱序执行的问题。
{: .prompt-tip }

非临时内存读或写告诉 CPU 我们在将来不需要刚刚访问的数据，因此在写入之后不需要将数据读回来。

![非临时读写](/assets/Caches/non-temporal.svg)

从图中可以看出，当数据足够小可以放到缓存里时，我们可以直接访问它们，因为我们确实需要立马访问相邻的数据。另一方面在数据足够大时，非临时访问的方式充分利用了总线的带宽，而不需要重新读回。

事实上，在 RAM 上使用非临时写的时候，甚至比只进行读入还要快两倍。这是因为：

- 存储器控制器不必在读和写模式之间切换总线的模式；
- 指令序列变得更简单，允许暂存更多的内存指令；
- 最重要的是，内存控制器可以简单地“触发并忘记”非临时写请求——而对于读请求，它需要记住一旦数据到达后应该如何处理数据（类似于网络软件中的连接句柄）。

从理论上讲，这两个请求应该使用相同的带宽：读请求发送一个地址并获取一个数据，而非临时写请求发送一个地址和一个数据，但什么也不获取。不考虑方向，我们传输了相同数量的数据，但读取周期将更长，因为它需要等待数据被获取。由于内存系统可以处理的并发请求数量是有实际限制的，因此读/写周期延迟的差异也会导致带宽的差异。

由于这些原因，单个 CPU 核心通常不能充分利用内存带宽。

同样的技术也适用于 `memcpy`：它也是使用 SIMD 加载/存储指令移动 32 字节块，并且可以类似地将其设置为非临时，从而将大型数组的吞吐量提高两倍。还有一个非临时加载指令 `_mm256_stream_load_si256`，用于在不污染缓存的情况下进行读取（例如，在 `memcpy` 之后并不需要原始数组）。

## 二 - 内存延迟

虽然带宽是一个有点复杂的概念，但它比延迟更容易观测和衡量：调度器可能会提前访问、重排和重叠指令，从而隐藏延迟。

为了测量 _延迟_，我们需要一种不让 CPU “作弊”的方法。一种方式就是生成一个长度为 $$N$$ 的排列，然后再遍历这个排列：

```cpp
int p[N], q[N];

// generating a random permutation
iota(p, p + N, 0);
std::mt19937 g(0);
std::shuffle(p, p + N, g);

// this permutation may contain multiple cycles,
// so instead we use it to construct another permutation with a single cycle
int k = p[N - 1];
for (int i = 0; i < N; i++)
    k = q[k] = p[i];

for (int t = 0; t < K; t++)
    for (int i = 0; i < N; i++)
        k = q[k];
```

和线性迭代比，这个迭代要慢得多——差几个数量级。SIMD 不再能够使用，还让管道停滞，所有的指令都得等待从内存中读取数据。

这种性能反模式被称为 Pointer Chasing，在数据结构中相当常见。尤其是一些高级语言，它们会使用大量的堆分配对象和指针来实现动态类型。

![Pointer Chasing](/assets/Caches/latency-throughput.svg)

在讨论延迟时，使用周期或纳秒比使用吞吐量单位更有意义，因此我们将此图替换为其倒数：

![latency](/assets/Caches/permutation-latency.svg)

注意到在这两个图上的性能断崖式下降没有带宽那么明显，这是因为即使在随机情况下，我们还是有可能访问到缓存中的数据。

### 理论延迟

形式化的，如果有 $$k$$ 层缓存，每层的大小和延迟为 $$s_i$$ 和 $$l_i$$，那么，我们可以得到每次访问大小为 $$N$$ 的数组的延迟期望为：

$$
E[L] = \frac{s_1 l_1 + (s_2 - s_1) l_2 + \dots + (N - s_k) l_{RAM}}{N}
$$

只保留最后一层的延迟，其它所有的常数都忽略并抽象化为 $$C$$，我们可以得到：

$$
E[L] = \frac{N l_{last} - C}{N} = l_{last} - \frac{C}{N}
$$

随着 $$N$$ 的增加，延迟的期望直接趋近于 $$l_{last}$$。如果你粗略地观察之前的吞吐量图，它应该大致上应该接近于一个双曲线图：

$$
\begin{aligned}
E[L]^{-1} =& \frac{1}{l_{last} - \frac{C}{N}} \\
=&\frac{N}{Nl_{last} -C} \\
=&\frac{1}{l_{last}}\left(\frac{1}{N\frac{l_{last}}{C} - 1} + 1 \right) \\
=& \frac{1}{k(x-x_0)} + y_0
\end{aligned}
$$

为了得到确切的延迟，我们可以使用第一种方式来计算，也可以直接查看图中在断崖右侧的数据。

还有更直接的方法来测量延迟，例如使用非临时读取，但是之前的基准测试更能代表实际的访问模式。

### 频率缩放

和带宽一样，CPU 缓存的延迟随时钟周期的增加成比例的减少，而 RAM 不会。我们可以从图中看到这一点：

![permutation-boost](/assets/Caches/permutation-boost.svg)

我们在下图中画出相对加速：

![permutation-boost-speedup](/assets/Caches/permutation-boost-speedup.svg)

在预期中，对于完全可以放入 CPU cache 的数组，超频可以带来 2x 的加速，而对于在 RAM 中的数据，应该是基本不变。但实际情况并非如此：即使对于 RAM 访问，也会有一个小的加速。

这是因为 CPU 在向 RAM 发送读查询之前首先要检查它的 cache，以便为可能需要它的其他进程节省 RAM 带宽。这一小点加速就是检查 cache 这一阶段所带来的加速。

内存延迟还受到虚拟内存的实现和特定于 RAM 的计时的一些细节的轻微影响，我们将在后面讨论这些细节。

## 三 - 缓存行

在 CPU cache 系统中传输的基本单位不是位或字节，而是 _缓存行 (cache lines)_。在大多数机器上，缓存行的大小是 64 字节，这意味着所有的内存被分为许多个大小为 64 字节的块。在读取和写入数据时，无论你是否需要，都会将相邻的 64 字节内的数据放入缓存。

为了演示，我们调整自增循环当中的步长：

```cpp
for (int i = 0; i < N; i += D)
    a[i]++
```

我们在 $$D=1$$ 和 $$D=16$$ 下运行代码：

![strided](/assets/Caches/strided.svg)

尽管 $$D=16$$ 时减少了 16 倍的工作量，但是随着问题规模增大，两种循环的耗时仍然逐渐接近。这是因为对于缓存行而言，这两个算法所需要获取的缓存行次数是完全相同的。

当数组可以完全放到 L1 缓存时，跨行访问的算法速度更快——但是并没有 16 倍的加速，而是大约 2 倍的加速。这是因为 $$D=1$$ 时并不需要对 16 个元素执行 `inc DWORD PTR [rdx]`，而是可以通过 SIMD 每次处理 8 个元素，只执行两次指令。两个算法都因为写回数据而成为瓶颈：Zen 2 cpu 每个周期只能写入一个字节，不论是一个整数还是八个整数。

当把 $$D$$ 调整为 $$8$$ 时，图像基本没有变化，因为每 16 个元素还是需要两个自增指令和两个写回指令：

![strided2](/assets/Caches/strided2.svg)

考虑到缓存行，我们可以使用如下方式最小化 cache 对测量延迟的 benchmark 的影响。只需要在每个元素后加入填充元素：

```cpp
struct padded_int {
    int val;
    int padding[15];
};

padded_int q[N / 16];

// constructing a cycle from a random permutation
// ...

for (int i = 0; i < N / 16; i++)
    k = q[k].val;
```

现在再进行循环时，每个元素就更有可能 cachemiss。

![permutation-padded](/assets/Caches/permutation-padded.svg)

在设计和分析内存算法时，计算缓存行的访问次数比统计整体的内存读写次数更加重要。

## 四 - 内存共享

从内存的某个层级开始，内存就变得在多个 CPU 之间共享，这减少了芯片的总面积，也方便了在同一个片上添加更多的核，但也带来了“嘈杂的邻居 (noisy neighbor)”问题。

在许多 CPU 上，只有最后一层 Cache 是共享的，但并不总是以统一的方式共享。在原文的机器上，有 8 个物理内核，其中 L3 缓存的大小是 8M，但它被分成两半：两组 4 个内核可以访问它们自己的 L3 缓存的 4M 空间，而不是全部的 L3。

除此之外也存在一些更复杂的拓扑结构，使得访问内存的不同区域所用的时间不同，这样的架构被称为非一致访存 (non-uniform memory access, NUMA)，对于可以安装多个 CPU 的多插槽系统也是如此。

在 Linux 上，内存系统的拓扑结构可以使用 `lstopo` 来查看：

![lstopo](/assets/Caches/lstopo.png)
_Ryzen 7 4700U 的缓存结构_

这对并行算法会有一定的影响，因为现在一个多线程内存访问的速度取决于具体是哪个核在运行哪个线程。

### CPU 亲和性 (Affinity)

相较于修改源码来运行多线程程序，我们可以使用 GNU 并行来简单地运行多个独立的进程。为了控制具体在哪个核上运行它们，我们需要使用 `taskset` 来设置程序的处理器亲和性 (processor affinity)。使用如下指令可以在前 4 个核上运行 4 个进程：

```bash
parallel taskset -c 0,1,2,3 ./run ::: {0..3}
```

下图是改变进程数量的运行结果。

![parallel](/assets/Caches/parallel.svg)

从图中可以看到，当数组大小超过 L2 缓存时（进入到共享内存阶段），性能开始产生了下降。

在之前的实验中，我们特地将进程运行在前 4 个核上，因为它们有统一的 L3 内存。如果我们在另外一半的核上调度线程，就会有更少的竞争。操作系统不监视这些活动——进程干了些什么事情是它的隐私——所以默认情况下，操作系统会只考虑内核负载地将线程分配给内核，而不关心缓存关联。

我们查看一下另一个基准测试，现在会在相同和不同的 L3 内存的 4 核组上运行程序：

```bash
parallel taskset -c 0,1 ./run ::: {0..1}  # L3 cache sharing
parallel taskset -c 0,4 ./run ::: {0..1}  # no L3 cache sharing
```

![affinity](/assets/Caches/affinity.svg)

它的性能更好——就好像有两倍的 L3 缓存和 RAM 带宽可用。

这个问题尤其难以处理，尤其是在基准测试和测量并行算法时，会是一个很大的噪声源。

### 饱和带宽

查看本节的第一张图的 RAM 部分，你会发现在多线程下，处理器的吞吐量会除以线程数倍，而总的带宽保持不变。但这实际上是不正确的：尽管竞争非常有害，但是单个 CPU 核往往不能够充分利用 RAM 的所有带宽。

如果我们更仔细地绘制它，我们会看到总带宽实际上随着内核数量的增加而增加——虽然并不是成比例的——并最终接近其理论最大值 ~42.4 GB/s：

![parallel-bandwidth](/assets/Caches/parallel-bandwidth.svg)

注意在这张图中仍然保持了特别的处理器亲和性：前 k 个进程运行在前 k 个核上。这也是为什么会在 4 到 5 个核的时候会有一次比较大的性能增长。

总的来讲，为了能够获得最大的带宽，你应该始终对称地分割应用程序的线程。

## 五 - 内存级并行

内存请求可以在时间上重叠：当等待读请求完成时，我们可以发送一些其他请求，这些请求将与读请求并发执行。这就是为什么线性迭代比指针跳转快得多的主要原因：CPU 知道下一步需要获取哪个内存位置，并提前发送内存请求。

可以并发的内存操作数量很多但仍然是有限的，而且对于不同种类的内存也有不同的值。在设计算法尤其是数据结构时，这个数字限制了计算的并行上线。

如果想要找到某种内存的这个限制，我们可以将延迟（获取缓存行的时间）乘以带宽（每秒获取缓存行的数量），从而得到正在进行的内存操作的平均数量：

![latency-bandwidth](/assets/Caches/latency-bandwidth.svg)

L1/L2 缓存的延迟很小，因此不需要很长的流水线。而对于较大的内存类型，它们可以支持多达 25-40 个并发读操作。

### 直接访存实验

让我们修改指针追逐的基准测试，通过同时遍历 $$D$$ 个不同的环来测试内存读取的并行性。

```cpp
const int M = N / D;
int p[M], q[D][M];

for (int d = 0; d < D; d++) {
    iota(p, p + M, 0);
    random_shuffle(p, p + M);
    k[d] = p[M - 1];
    for (int i = 0; i < M; i++)
        k[d] = q[d][k[d]] = p[i];
}

for (int i = 0; i < M; i++)
    for (int d = 0; d < D; d++)
        k[d] = q[d][k[d]];
```

令 $$N$$ 为常数，调整 $$D$$ 的大小，我们可以得到如下结果：

![mlp](/assets/Caches/permutation-mlp.svg)

从图中可以看到，L2 缓存大约可以并行 6 个操作。但是更大的内存类型却在 13 到 17 的时候达到最好的性能。

这是因为可以并行的操作时也被逻辑寄存器的数量限制，当指针追逐中环的数量小于寄存器的数量是，我们可以给每个环分配一个寄存器：

```nasm
dec     edx
movsx   rdi, DWORD PTR q[0+rdi*4]
movsx   rsi, DWORD PTR q[1048576+rsi*4]
movsx   rcx, DWORD PTR q[2097152+rcx*4]
movsx   rax, DWORD PTR q[3145728+rax*4]
jne     .L9
```

而当数量到了 15 左右时，就不得不使用临时内存存储。

```nasm
mov     edx, DWORD PTR q[0+rdx*4]
mov     DWORD PTR [rbp-128+rax*4], edx
```

虽然我们并不总是能够充分利用内存并行，但是对于大多数应用来说，10 几个并发请求就够用了。

## 六 - 预取

上一节我们提到硬件本身具有并发能力，为了充分利用硬件中空闲的并发能力，我们可以对接下来很可能被访问的数据进行预取。当流水线中不存在数据冒险，这很容易做到——CPU 只需提前运行指令流，乱序执行内存操作即可。

但有时目标的内存地址并不存在于指令流中，不过仍能以高概率被预测。此时可通过其他方式实现预取：

- 显示预取：通过单独读取下一个数据字或同一缓存行中的任意字节，使其在缓存层级中上移。
- 隐式预取：利用线性迭代等简单访问模式，这些访问模式会被内存硬件自动检测并触发预取。

隐藏内存延迟对性能提升至关重要，因此本节我们将深入探讨数据预取技术。

### 硬件预取

让我们调整指针追逐测试来观察硬件预取的性能。现在我们伸成一种特殊的排列方式：遍历这个排列时，CPU 会按照顺序请求缓存行，但每个缓存行内部的元素访问顺序仍然随机：

```cpp
int p[15], q[N];

iota(p, p + 15, 1);

for (int i = 0; i + 16 < N; i += 16) {
    random_shuffle(p, p + 15);
    int k = i;
    for (int j = 0; j < 15; j++)
        k = q[k] = i + p[j];
    q[k] = i + 16;
}
```

此时绘制性能曲线已无意义——无论数组大小如何，延迟始终稳定在3纳秒。尽管指令调度器仍无法预判后续的访存操作，但内存预取器仅通过观察内存访问模式就能提前加载下一个缓存行，从而有效减少内存延迟。

硬件预取机制足以应对大多数常规场景，但其模式识别能力仅限于简单规律。例如支持同时向前/向后遍历多个数组，或处理中小幅度的跨步访问，仅此而已。面对更复杂的访问模式时，预取器将无能为力，此时就需要我们手动介入优化。

### 软件预取

实现软件预取最简单的方式是用 `mov` 或其他内存指令加载缓存行中的任意字节，不过 CPU 专门提供了预取指令——该指令仅将缓存行提前加载，而不进行任何实际处理。虽然这条指令不属于 C/C++ 标准，但大多数编译器都通过 `__builtin_prefetch` 内置函数提供了支持：

```cpp
__builtin_prefetch(&a[k]);
```

要设计一个能表现其价值的简单案例并不容易。为了让指针追逐测试从软件预取中受益，我们需要构造一种特殊的排列：既要能遍历整个数组，又要让硬件预取器无法预测，同时还需确保下一个地址易于计算。

幸运的是，线性同余生成器（LCG）具有这样的特性：当模数 n 为质数时，生成器的周期恰好为 n。因此，如果我们用 LCG 生成排列（以当前索引作为状态），就能满足上述所有需求：

```cpp
const int n = find_prime(N); // largest prime not exceeding N

for (int i = 0; i < n; i++)
    q[i] = (2 * i + 1) % n;
```

在这个排列上运行基准测试时，性能和随机排列时没什么差别。但是现在我们有了可以预测未来的能力：

```cpp
int k = 0;

for (int t = 0; t < K; t++) {
    for (int i = 0; i < n; i++) {
        __builtin_prefetch(&q[(2 * k + 1) % n]);
        k = q[k];
    }
}
```

计算下一个地址时有一定的开销，但是对于足够大的数组来说，这可以带来约两倍的加速：

![sw-prefetch](/assets/Caches/sw-prefetch.svg)

有趣的是，我们可以提前预取不止一个元素，在 LCG 函数下可以得到：

$$
f^k(x) = 2^k x + (2^k - 1)
$$

因此为了加载从当前往后的第 $$D$$ 个元素，我们可以：

```cpp
__builtin_prefetch(&q[((1 << D) * k + (1 << D) - 1) % n]);
```

若在每次迭代中都执行此预取请求，理论上我们就能平均提前预取 $$D$$ 个元素，使吞吐量提升 $$D$$ 倍（暂不考虑过大 $$D$$ 值导致的整数溢出等问题）。此时平均延迟可无限趋近于计算下一个索引的时间成本（本例中主要取决于取模运算的开销）。

![sw-prefetch-others](/assets/Caches/sw-prefetch-others.svg)

需注意这是个理想化案例，实际工程中强行加入软件预取往往适得其反。这主要是因为额外的内存指令会与其他指令争夺资源。而硬件预取则完全无害——它仅在内存和缓存总线空闲时才会被激活。

软件预取还允许指定数据预取至特定缓存层级（`_mm_prefetch` 函数的第二个参数可以接受一个整型）。当不确定数据是否会立即使用，又希望避免污染 L1 缓存时，这个特性尤为实用。将该机制与非临时加载/存储指令配合使用时效果最佳。

## 七 - 对齐和打包

由于内存被按照 64B 缓存行来划分区域，对跨越缓存行边界的数据进行操作就变得十分困难。当我们保存一个原始数据类型，比方说 i32 时，基本上是想要把它放到一个缓存行中的——否则我们就不得不读取两个缓存行，这会占用更多的带宽，带来更大的延迟。

这一点严重影响着算法设计，以及编译器如何选择数据结构的内存布局。

### 对齐分配

默认情况下，当我们申请某个原始类型的数组时，会保证所有数据的地址都是其大小的某个整数倍，从而保证它们只生成在一个缓存行上。例如，i32 数组的第一个元素和其它元素的地址都是 4 字节的倍数。

有些时候我们也需要增加这个对齐的数据，例如在 SIMD 中，往往是以 32 字节来读写数据，因此将这 32 个字节的数据放在同一个缓存行里是非常重要的一件事。在 c++ 中，我们可以使用 `alignas` 说明符来设置一个静态数组变量的对齐步长：

```cpp
alignas(32) float a[n];
```

而如果要动态申请内存对齐的数组，我们可以使用 `std::alighed_alloc`，第一个参数表示对齐步长的字节数，第二个参数表示申请多少字节内存：

```cpp
void *a = std::aligned_alloc(32, 4 * n);
```

我们还可以对齐到比缓存行还要大的步长，唯一的要求是第二个参数必须是第一个参数的整数倍。在 rust 中，如果不想操作 unsafe，则有 `aligned_vec` 库可以用来定义动态分配内存对齐的 `Vec<T>`。

我们也可以在定义 `struct` 时使用 `alignas` 说明符（rust 中为 `#[repr(align(n))]` 修饰）：

```cpp
struct alignas(64) Data {
    // ...
};
```

当给 `Data` 的实例分配内存时，它的地址总会是某个缓存行的开头。但这样做的缺点是结构体的大小会被上升到一个 64 字节的倍数。（如果结构体的大小不是 64 字节的倍数，在分配 `Data` 的数组时会导致第一个以外的 `Data` 实例可能不是对齐的。）

### 结构体对齐

在需要分配一组非均匀元素（例如结构体）时，这个问题会变得更加复杂。与其像玩俄罗斯方块那样试图重新排列结构体成员（试图让每个成员都完整落在单个缓存行内——这往往无法实现，因为结构体本身并不总是从缓存行起始地址开始存储），大多数 C/C++ 编译器都更依赖于内存对齐机制。

结构体对齐通过以下方式确保其所有基本类型成员的地址都是其自身大小的整数倍，从而自动保证每个成员仅跨越单个缓存行：

- 按需填充：在必要时为每个结构体成员添加可变长度的空白字节，以满足下一个成员的对齐要求；
- 整体对齐：将结构体本身的对齐要求设置为其成员类型对齐要求的最大值。这样当分配该结构体数组，或将其作为成员嵌套在其他结构体中时，能确保所有基本类型成员的对齐要求得到满足。

做个测试，考虑如下示例：

```cpp
struct Data {
    char a;
    short b;
    int c;
    char d;
};
```

如果直接存储，这个结构体需要 $$1+2+4+1=8$$ 个字节。假设这个结构体是 4 字节对齐的（对齐要求最大的成员变量），此时 `a` 和 `d` 不会跨越缓存行，剩下由于没有对齐，都有可能跨越缓存行。

为了解决这个问题，编译器会在其中插入一些匿名变量来保证下一个元素在正确的对齐位置上：

```cpp
struct Data {
    char a;    // 1 byte
    char x[1]; // 1 byte 填充，从而让之后的 short 类型是对齐的
    short b;   // 2 bytes 
    int c;     // 4 bytes 正好对齐因此无需填充，同时作为最大的成员变量，会使得整个 Data 的内存对齐也是 4 bytes
    char d;    // 1 byte
    char y[3]; // 3 bytes 从而保证整个 Data 的大小是 12 bytes (也就是 4 bytes 的倍数，因为 Data 的内存对齐是 4 bytes)
};

// sizeof(Data) = 12
// alignof(Data) = alignof(int) = sizeof(int) = 4
```

这可能会浪费空间，但会节省大量的 CPU 周期。这种权衡在很多时候都是有益的，因此大多数编译器都默认启用结构体对齐。

### 优化变量顺序

填充的主要目的就是保证成员变量和结构体整体满足对齐要求。通过调整成员变量的顺序，我们我可以减少填充的字节数量。

考虑之前的例子，我们可以重排整个结构体中的变量：

```cpp
struct Data {
    int c;
    short b;
    char a;
    char d;
};
```

此时不需要填充，每个成员变量也是对齐的。因此这个结构体的大小就是 8 字节了。虽然结构体的大小（进而影响其性能）竟然取决于定义成员变量的顺序感觉很怪，但为了保持二进制兼容性，这种做法是必要的。

在经验上，定义结构体时按照数据类型从大到小的顺序排列成员就可以得到很好的对齐结果——除非你使用了一些不是 2 的幂次大小的数据，例如 10 字节 (long double[^long-double])。

[^long-double]: 80 比特的 `long double` 至少需要 10 字节，但实际上取决于编译器。`long double` 可能会被填充到 12 或 16 字节从而减少对齐问题（64 位的 GCC 和 Clang 编译器都默认使用 16 字节。我们也可以通过设置 `-mlong-double-64/80/128` 或者 `-m96/128bit-long-double` 来覆盖它）。

### 结构体打包

如果你知道自己在做什么，我们也可以禁用结构体对齐并尽可能紧凑地打包数据。

这里必须要求编译器来做，因为这样的功能既不是 C 也不是 c++ 标准的一部分。在 GCC 和 Clang 中，这是通过打包属性完成的：

```cpp
struct __attribute__ ((packed)) Data {
    long long a;
    bool b;
};
```

在 rust 中则是 `#[repr(packed(n))]`，其中 n 是 2 的幂次。它强制要求整个结构体的对齐**不超过** n。`#[repr(packed)]` 等价于 ``#[repr(packed(1))]`，也就是对齐恒为 1。

这使得 `Data` 的实例只占用 9 字节，而不是结构体对齐后所需的 16 字节，代价则是可能需要读取两条缓存行来读取其成员。

### 位段

位域允许我们显式地指定成员的大小。我们可以将其与打包结合使用：

```cpp
struct __attribute__ ((packed)) Data {
    char a;     // 1 byte
    int b : 24; // 3 bytes
};
```

> rust 不支持位段。如果需要，请写几个 get 和 set 函数手动操作二进制。当然这个需求也已经有了类似的库 `modular-bitfield`。
{: .prompt-info }

这个结构体在紧凑打包时占 4 字节，而结构体对齐则会占用 8 字节。结构体成员的比特数不必是 8 的倍数，整个结构体的大小也不必按字节对齐。在存储 Data 结构体的数组时，若存在非整数字节的情况，相邻元素的内存空间将会"合并"。该特性还允许设置超出基础类型位宽的字段（此时会作为填充位使用）——尽管编译器会对此发出警告。

这种位域特性之所以未能普及，是因为 CPU 并不支持 3 字节算术之类的操作，在数据加载时不得不进行低效的逐字节转换变成 4 字节：

```cpp
int load(char *p) {
    char x = p[0], y = p[1], z = p[2];
    return (x << 16) + (y << 8) + z;
}
```

如果甚至不是一个完整的字节会更加麻烦——我们不得不使用移位操作和与操作。

这个过程可以通过加载一个 4 字节的int，然后使用掩码丢弃其最高位来优化。

```cpp
int load(int *p) {
    int x = *p;
    return x & ((1<<24) - 1);
}
```

编译器通常不会这样做，因为这在技术上是不合法的：因为第 4 个字节可能位于程序不拥有的内存页上。即使我们打算读取后立即丢弃它，操作系统也不会让你的程序加载它。

## 八 - 指针替换

在指针追逐测试中，我们使用了相对于数组基址的便宜而非实际的指针来访问元素：

```cpp
for (int i = 0; i < N; i++)
    k = q[k];
```

由于 x86 的优化，寻址操作可以将计算和读取融合起来：

```cpp
mov rax, DWORD PTR q[0+rax*4]
```

不过虽然这两个操作已经完全融合在了一起，额外的计算步骤仍然导致了一点点内存延迟。L1 缓存的读取是 4 或 5 个周期——如果我们需要复杂的计算则是后者。由于这个原因，我们的基准测试中每次跳转需要 3ns 或 6 个周期：4+1 个周期用于读取和地址计算，1 个周期用于移动结果到正确的寄存器上。

### 指针

我们可以将假指针（偏移量）替换为实际的指针，从而实现轻微的加速。

```cpp
struct node { node* ptr; };

node* k = q + p[N - 1];

for (int i = 0; i < N; i++)
    k = k->ptr = q + p[i];

for (int i = 0; i < N; i++)
    k = k->ptr;
```

对于可以放入 L1 缓存的数组，这段代码现在以 2ns / 4 个周期运行。为什么不是 $$4+1=5$$ 呢？因为 Zen 2 有一个有趣的特性，允许通过地址访问数据时的零延迟重用，所以这里的“移动”是零开销的，从而节省了整整两个周期。

不幸的是，这种做法在 64 位系统上有一个问题。因为指针变得两倍大，与使用 32 位索引相比，L1 缓存中能够包含的数组大小会减少一半。延时与数组大小的关系图中就会向左移动 2 倍：

![p64](/assets/Caches/permutation-p64.svg)

将编译器切换到 32 位可以消除这个影响。

![p32](/assets/Caches/permutation-p32.svg)

### 位域

在大规模数据上，算法的性能被内存而不是 CPU 限制。因此我们还可以使用位域来使用更少的内存存储索引：

```cpp
struct __attribute__ ((packed)) node { int idx : 24; };

int k = p[N - 1];

for (int i = 0; i < N; i++) {
    k = q[k].idx = p[i];

for (int i = 0; i < N; i++) {
    k = q[k].idx;
```

对于 L 1缓存，此代码测量为 6.5ns。由于编译器选择的默认转换过程不是最优的，因此还有一些改进的空间。我们可以手动加载一个 4 字节的整数并截断它（我们还需要向 q 数组中添加一个元素，以确保我们拥有额外的 1 字节内存，防止操作系统阻止我们的程序）：

```cpp
k = *((int*) (q + k));
k &= ((1<<24) - 1);
```

现在就会在 4ns 内运行，并生成下图：

![bf-custom](/assets/Caches/permutation-bf-custom.svg)

如果我们放的足够大（图片是 `svg` 格式），我们可以看到指针在足够小的数组上具有很高的性能，而随着数组到达 L2-L3 缓存的边界，位域的性能会更好。而对于更大的数组，选择哪个方式的性能都是类似的，因为此时缓存带来的提升并不明显。

这并不是一个可以带来 5x 加速的方法，但是当其它资源耗尽时，也是一种可以尝试的优化策略。

## 九 - 缓存相关性

考虑一个在 $$N=2^{21}$$ 大小的数组上进行的步长为 256 的循环：

```cpp
for (int i = 0; i < N; i += 256)
    a[i]++;
```

然后考虑一个步长为 257 的循环：

```cpp
for (int i = 0; i < N; i += 257)
    a[i]++;
```

哪一个完成的更快？

- 简单的想法是两个算法应该差不多，或者 257 更快，因为它少访问了一些元素。
- 或者 256 是一个“整数”，可能会由编译器或者内存系统优化，所以 256 会更快。

但实际上，257 会更快——而且快了 10 倍。

不仅仅 256 是一个糟糕的步长。对于 2 的大幂次的所有步长，性能都会下降：

![strides-small](/assets/Caches/strides-small.svg)
_数组大小进行了改变，以保证迭代的总次数是恒定的_

这里并没有向量化或者什么别的东西影响，而只是因为内存系统，也就是被称为 _缓存关联性_ 的性质，是因为通过硬件实现 CPU 缓存时导致的结果。

### 硬件缓存

当我们从理论上研究内存系统时，讨论了在软件中实现缓存淘汰策略的不同方法。其中吗，我们关注的一个特殊策略是最近最少使用（LRU）策略。该策略简单有效，但仍然需要一些并不“简单”的数据操作。

在硬件环境下，这种策略被称为 _全相联缓存 (fully associative cache)_：主存中的任意缓存行都可以映射到缓存中的任意缓存行，当产生争用时，将最长时间未被访问的那个块踢出。

![cache1](/assets/Caches/cache1.png)
_全相联缓存 (fully associative cache)_

全相联缓存的实现困难之处在于“从数百万个缓存行中找到最老的那个”。我们可以处理 16 个缓存行的全相联缓存，但是对于几百个缓存行就变得既昂贵又有可能变慢。

我们可采用另一种更简单的方法：把 RAM 中的每 64 字节映射到缓存中的特定缓存行上。假如主存中有 4096 个块，缓存中有 64 个缓存行，那么每个缓存行就会对应 $$\frac{4096}{64} = 64$$ 个不同的块。

![cache2](/assets/Caches/cache2.png)
_直接映射缓存 (Direct-mapped cache)_

直接映射缓存非常容易实现，因为它并不需要存储任何额外的辕信息，只需要存储它的 tag（缓存行中存储的是哪个缓存块）。缺点就是一个缓存可能会被立马踢出去——比方说我们一直在两个映射到相同缓存行的块上跳来跳去。

处于这个原因，我们选择了一种介于直接映射和全相联的缓存方式：组相联。他将内存地址空间划分成相同大小的组，每个组内使用全相联缓存。

![cache3](/assets/Caches/cache3.png)
_组相联缓存 (Set-associative cache)，每个组内有两个缓存行 (2-way associative)_

相联度 (associativity) 指这些组的大小，或者说每个组可以映射到多少个不同的缓存行。更高的相联度能提升缓存利用率，但也会增加硬件成本。

例如在原文作者的 CPU 中，L3 缓存是 16 路组相联的，每个核有 4MB 的可用空间。这意味着总共有 $$\frac{2^{22}}{2^{6}} = 2^{16}$$ 个缓存行，会被划分成 $$\frac{2^{16}}{2^4} = 2^{12}$$ 个组，每个组内则用全相联策略管理 $$\frac{1}{2^{12}}$$ 主存大小的数据。

大多数 CPU 缓存（包括非数据类缓存，如指令缓存和TLB）同样采用组相联结构。唯一的例外是那些仅容纳 64 项或更少条目的小型专用缓存——这类缓存通常采用全相联设计。

### 地址转换

现在只剩下最后一个问题了：缓存行映射具体怎么做？

如果采用软件实现组相联缓存，我们会先计算内存块地址的哈希值，再将其作为缓存行索引。但在硬件层面，这种方法因速度过慢而不可行：以L1缓存为例，其延迟要求仅为 4 到 5 个时钟周期，而即便是取模运算就已经需要 10-15 个周期，更复杂的计算更无法满足时效要求。

因此，硬件采用了一种简捷方案：将待访问的内存地址从低位到高位划分为三部分：

- **偏移量**（offset）：64B 缓存行内的字索引（$$\log_2 64 = 6$$ 位）；
- **索引**（index）：缓存行组的编号（12 位，因为 L3 缓存包含 $$2^{12}$$ 个缓存行）；
- **标签**（tag）：内存地址的剩余部分，用于区分缓存行中存储的是哪个内存块。

换言之，所有具有相同"中间部分"的内存地址都会映射到同一组。

这种设计虽然降低了缓存系统的实现复杂度和成本，但也使其容易受到特定低效访问模式的影响。

### 病态映射

现在回到我们的核心问题：为什么 256 整数步长的迭代会导致如此严重的性能下降？

当我们以256个整数为跨度跳跃访问时，指针每次增加 $$1024=2^{10}$$ 字节，导致地址的最后 10 位始终保持不变。由于缓存系统使用低 6 位作为偏移量，接下来的 12 位作为缓存组索引，实际上我们只利用了L3缓存中 $$2^{12 - (10 - 6)} = 2^8$$ 个组而非全部的 $$2^{12}$$ 个组，这相当于将 L3 缓存容量压缩至原有的 $$2^4=16$$ 分之一。当数组大小（$$N=2^{21}$$）超出缩水后的L3缓存容量时，数据就会被挤入速度慢一个数量级的主内存，从而引发性能骤降。

由缓存相联度引发的性能问题在算法中会经常出现，原因在于我们总是偏爱用二次幂作为数组索引：

- 让多位数组的最后一维的长度是 2 的幂次，获取地址时就可以直接进行移位运算来获得低几位的地址。
- 对二次幂取模可以通过一个 `and` 操作完成。
- 在分治算法中使用 2 的幂次是经常的，甚至是必要的。
- 作为最小的整数指数，二次幂序列是内存密集型算法进行基准测试的常用选择。
- 而更符合现实生活的十次幂数值，因为 2 是 10 的因子，其中也包含较大的二次幂因子。

这种情况在采用固定内存布局的隐式数据结构中尤为常见。例如：在 $$2^{20}$$ 大小的数组上二分查找每次查询耗时约 360ns，而 $$(2^{20} + 123)$$ 大小的数组仅需 300ns。当数组尺寸是较大二次幂的倍数时，前几次迭代访问的“最热”元素索引往往也包含大二次幂因子，导致这些元素被映射到同一缓存行相互驱逐，造成约 20% 的性能损失。

好消息是这些情况都是特殊情况而非一般情况。解决方案也通常很简单：避免使用二次幂步长迭代、将多维数组的末维调整为非规整尺寸、在内存布局中插入“空隙”、或建立数组索引与实际存储位置的随机双射关系。

## 十 - 内存分页

再次考虑循环增长问题：

```cpp
const int N = (1 << 13);
int a[D * N];

for (int i = 0; i < D * N; i += D)
    a[i] += 1;
```

我们调整迭代的步长 $$D$$ 而保持迭代的次数 $$N$$ 不变。对于 $$D\geq 16$$，此时每次内存访问都会跨越缓存行，因此我们会需要获取恰好 $$N$$ 个缓存行，也就是 $$64 N$$ 字节。无论步长如何，这些内存都可以被放到 L2 缓存里。

![stride](/assets/Caches/strides.svg)

然而，从图中可以看到当步长超出 256 左右时，性能就开始产生下降。这种异常也是由缓存系统造成的，不过并不是 L1-L3 缓存，而是虚拟内存。尤其是虚拟内存的 _转译后备缓冲器 (Translation Lookaside Buffer, TLB)_ 造成的，它是一个用于检索虚拟内存对应的物理地址的缓存。

在原文作者的 CPU 上，有两级 TLB：

- L1 TLB 可以存储 64 个对象，如果分页大小是 4K，那么其中就可以存储 512KB 的内存。
- L2 TLB 可以存储 2048 个对象，如果分页大小是 4K，那么其中就可以存储 8MB 的内存。

当 $$D$$ 大于等于 256 时，$$8K\times 256 \times 4B = 8MB$$，超出了 L2 TLB 可以保存的长度。当 $$D$$ 大于 256 时，一些请求开始被直接分发到主页表，这会带来很大的延迟和非常有限的吞吐量，从而称为整个计算的瓶颈。

### 修改页面大小

无减速的内存只有 8MB 似乎是一个非常大的限制。虽然我们不能改变硬件的特性来提升它，但是我们可以通过增加页面大小，反过来减少 TLB 容量的压力。

现代操作系统允许我们全局或者单独设置页面大小。CPU 只支持一些页面大小，例如在原文作者的 CPU 上只能使用 4K 或 2M 作为页面大小。还有一种常见的页面大小是 1G ——通常只适用于有几百个 GB 的 RAM 的服务器级别的硬件上使用。任何超过默认的 4K 的页面大小，在 Linux 中称为 huge pages，在 Windows 中称为 large pages。

在 Linux 上，有一个特殊的系统文件来管理大页面的分配。下面是如何让内核在每次分配时给你 huge pages：

```shell
echo always > /sys/kernel/mm/transparent_hugepage/enabled
```

像这样全局启用大页面并不总是一个好主意，因为它降低了内存粒度，提高了一个进程消耗的最小内存——在某些环境中，进程的数量可能超过可用内存的 MB。所以，除了 `always` 和 `never` 之外，还有第三个选项：

```shell
$$ cat /sys/kernel/mm/transparent_hugepage/enabled
always [madvise] never
```

`madvise` 是一个特殊的系统调用，它让程序来通知内核是否使用大页面，这样就可以按需分配大页面。启用后，可以像这样在 c++ 中使用它：

```cpp
#include <sys/mman.h>

void *ptr = std::aligned_alloc(page_size, array_size);
madvise(ptr, array_size, MADV_HUGEPAGE);
```

只能在对齐的内存区域上设置大页面。

Windows 也有类似的功能，它的 API 将分配内存和设置页面结合到了一起：

```cpp
#include "memoryapi.h"

void *ptr = VirtualAlloc(NULL, array_size,
                         MEM_RESERVE | MEM_COMMIT | MEM_LARGE_PAGES, PAGE_READWRITE);
```

无论是哪种情况 `array_size` 都应该是 `page_size` 的倍数，以实现内存对齐。

### 大页面的影响

以上两种方式都可以解决图形当中的性能下降：

![hugepages](/assets/Caches/strides-hugepages.svg)

对于无法放入 L2 的数组，启用大页面还可以将延迟减少 10-15%：

![permutation-hugepages](/assets/Caches/permutation-hugepages.svg)

一般而言，对于任何稀疏读取，开启大页面都是一个好主意。它经常会带来提升并且几乎（例外情况会在下一节讨论）不会损害性能。

不过，如果可能的话，算法不应该依赖于大页面，由于硬件或计算环境的限制，大页面并不总是可用的。

在空间上将数据访问分组可能是有益的，这可以自动解决分页问题，还有许多其他原因。最好还是将相邻空间的数据访问放置在一起，从而自动解决分页的问题。

## 十一 - AoS 和 SoA 布局

一般来讲，将需要同时获取的数据放置在一起是有益的，最好能在同一个缓存行上，不行的话就在相邻的缓存行上。这可以提高访存时的空间局部性，从而提升瓶颈在内存上的算法的性能。

为了演示这样做的潜在影响，我们可以修改指针追逐的基准测试，以使用可变数量的字段（$$D$$ 个）而不是一个字段来计算下一个指针。

### 实验

第一种方法将这些字段作为二维数组的行一起获取。我们将把这种变体称为结构体数组 (array of structures)：

```cpp
const int M = N / D; // # of memory accesses
int p[M], q[M][D];

iota(p, p + M, 0);
random_shuffle(p, p + M);

int k = p[M - 1];

for (int i = 0; i < M; i++)
    q[k][0] = p[i];

    for (int j = 1; j < D; j++)
        q[i][0] ^= (q[j][i] = rand());

    k = q[k][0];
}

for (int i = 0; i < M; i++) {
    int x = 0;
    for (int j = 0; j < D; j++)
        x ^= q[k][j];
    k = x;
}
```

在第二种方法中，我们将把它们分开放置：

```cpp
int q[D][M];
//^-^
```

![aos-soa](/assets/Caches/aos-soa.svg)

两种方法的延迟都随着 $$D$$ 的增长成线性趋势。但是在 $$N$$ 足够大时，AoS 需要获取的缓存行少了 16 倍。即使当 $$D=64$$ 时，处理其它 63 个值得开销仍然小于获取缓存行的开销。

我们也可以从图中看到 2 的幂次时产生的尖端。这种情况在 AoS 时会得到改善，因为此时可以使用 SIMD 来计算异或。相比之下 SoA 要糟得多，这并不是因为 $$D$$ 的大小，而是因为 $$M=\frac{N}{D}$$ 的大小。当 $$M$$ 为二次幂时，会导致缓存关联性的问题。

### 临时存储争用

乍看之下，这里似乎不应该存在任何缓存问题，因为当 $$N=2^{23}$$ 时，数组规模已经远超 L3 缓存的容量。但关键在于：当需要并行处理的元素分散在不同内存地址时，系统仍然需要临时的空间来暂存数据。由于寄存器数量不足，这些数据就不得不暂存于缓存中——即便它们在微秒级时间内就会被丢弃。

当 `N / D` 是较大的 2 的幂次，且我们沿着第一个维度遍历二维数组 `q[D][N / D]` 时，部分需要临时存储的内存地址会映射到同一缓存行。由于缓存空间不足，其中许多数据将被迫从更高层内存重新获取，这就导致了性能问题。

另一个令人费解的现象是：启用大页面后，虽然大多数 $$D$$ 值下的总延迟和预期一样降低了 10-15%，但当 $$D=64$$ 时，性能却急剧恶化了十倍：

![soa-hp](/assets/Caches/soa-hugepages.svg)

即便是设计内存控制器的工程师，恐怕也无法立即解释这种现象。

这里的本质区别在于：与每个核心独享的 L1/L2 缓存不同，L3 缓存需要采用物理内存地址（而非虚拟地址）来实现多核间的缓存同步。使用 4KB 的内存页时，相邻的虚拟地址也可能会映射到物理地址的各种地方，从而使其相对随机地分散在物理内存中，缓解缓存关联性的问题——此时物理地址上仅在几个随机的 4KB 上跳跃，而非虚拟地址中对 N / D 的步长。如果 L3 缓存足够大，4KB 中就不足以产生包含足够的缓存关联的缓存行（对于 16 路组相联，至少需要 $$16\times 64B=1KB$$ 且相互关联的内存才会产生竞争）。而当我们强制使用大页时，物理地址上的分页以 2MB 为单位，导致缓存行争用现象急剧增加。

这是我所知的唯一一例启用大页反而导致性能断崖式下跌的场景，而且其恶化程度甚至达到十倍之多。

如果 L3 缓存也是使用的虚拟内存地址，那么大页就不会引起这种特定的冲突问题。

### 填充时的 AoS

只要我们读取的缓存行数相同，它们在哪里并不重要，对吧？让我们测试一下。在 AoS 代码中切换为填充后的整数（一整个缓存行只有一个能用的数据）：

```cpp
struct padded_int {
    int val;
    int padding[15];
};

const int M = N / D / 16;
padded_int q[M][D];
```

我们仍然需要计算 $$D$$ 个整数的异或和，因此我们需要获取 $$D$$ 个缓存行，只是这次是 AoS 下的循环，也就是按内存顺序读取的。从这个角度来看，运行时间应该和 SoA 差不多，但是实际情况如下：

![aos-soa-padded](/assets/Caches/aos-soa-padded.svg)

当 $$D=63$$ 时，运行时间甚至可以减少到 $$1/3$$ 倍。但这个奇怪的“优化”现象只出现在超出 L3 缓存的数组。如果我们固定 $$D$$ 而修改 $$N$$，我们就能看到几个算法在更小的缓存上的表现，其中填充后的 AoS 和 不填充的 SoA 的性能相当：

![aos-soa-padded-n](/assets/Caches/aos-soa-padded-n.svg)

由于数组较小时没有这个现象，因此该现象显然与 RAM 有关。

### RAM 的时序特性

从性能分析的角度来看，RAM 中的所有数据实际上都存储在一个由微型电容器单元构成的二维阵列中，这个阵列被划分为行和列。要对任意存储单元进行读写操作，通常需要执行一至三个步骤：

1. **行激活**：将目标行的内容读取到行缓冲器中，这一过程会导致电容器暂时放电。
2. **列操作**：在行缓冲器中对特定单元进行读写操作。
3. **行回写**：将行缓冲器中的内容重新写入电容器阵列，以确保数据持久化，同时释放行缓冲器供其他的内存访问使用。

这里的重点在于：如果连续两次内存访问的目标是同一行，则可以省略步骤 1 和 3 ——此时行缓冲器就相当于临时缓存。由于这三个步骤耗时相近，因此这种优化能使连续的行内访问的速度达到分散访问模式的三倍。

![ram](/assets/Caches/ram.png)

不同硬件架构的中的行大小存在差异，但通常在 1024 至 8192 bytes 之间。在 AoS（数组结构）填充测试中，即使每个元素都独占一个缓存行，它们仍然有很大概率位于同一个内存行上，从而使得整个读取序列的耗时约为分散访问模式的三分之一（还需加上首次内存访问的延迟）。
