---
title: 高性能计算-第十章-SIMD
date: 2025-6-24 12:00:00 +0800
categories: [笔记, 高性能计算]
tags: [高性能计算]     # TAG names should always be lowercase
math: true
---
[原文](https://en.algorithmica.org/hpc/simd/)

## 序

考虑如下程序，我们计算一个整数数组的所有数之和：

```rust
pub fn sum(a: &[i32]) -> i32 {
    let mut result = 0;
    for &x in a {
        result += x;
    }
    result
}
```

使用 cargo 在 release 模式下编译后，这个函数计算 $$10^6$$ 个数之和的时间为 127μs。而在全局变量中指定了 CPU 允许 `avx2` 特性后 (`RUSTFLAGS="-C target-cpu=haswell"`)，所需要的时间变为 116μs。

> 原文中使用 c++，并在这里产生了两倍加速。尚不清楚原因。
{: .prompt-warning }

我们给编译器额外提供了一些最终程序会运行在什么机器上的信息。具体来说，我们告诉编译器我们会在支持 `AVX2` 的 CPU 上运行程序。其中 `AVX2` 是 x86 的一个扩展指令集，也是 x86 的众多所谓的 “SIMD 扩展” 中的一个。这些扩展包含了一些指令，它们可以使用 128、256、512 bits 的特殊寄存器并执行“单指令，多数据”的流程。SIMD 指令并不是一个值一个值处理，而是将寄存器分割成 8、16、32 或者 64 比特大小的块，然后并行地对它们进行相同的处理，从而提高运行效率[^heavy]。

这些扩展相对较新，因此它们一般是逐渐加入 CPU 并维护向后兼容性的[^backward]。除了增加了一些特殊指令外，更重要的是增加了对更宽的数据的支持。

例如，AVX2 具有使用 256 位寄存器的指令。而在默认情况下，GCC 假定 CPU 上没有比支持 128 位的 SSE2 更新的指令集。因此，在告诉优化器它可以使用一次对 8 个整数做加法而不是 4 个整数的指令后，性能提高了两倍。

![intel](/assets/simd/intel-extensions.webp)

一般而言编译器都可以很好的使用 SIMD 指令重写简单循环，比如之前的例子。这种优化被称作自动向量化，也是 SIMD 最常用的使用方式。

而问题在于，这样的自动向量化只适用于某些类型的循环，即使这样，产生的最终程序也往往不是最优而是次优的。为了理解编译器的极限，我们得自己上手探索这个技术在底层的情况。

[^heavy]: 在某些 CPU 上，SIMD 特别是更重的 SIMD 指令会消耗更多的能量，导致 CPU 需要降频来平衡总体的电力消耗，所以实际的加速并不总是成比例的。

[^backward]: 从 `AVX512` 起开始不再维护向后兼容性：针对数据压缩、加密、机器学习等特殊需求会产生不同的设计。

## 一 - 内置函数和向量类型

使用 SIMD 的最低层次的方法是直接使用汇编的向量指令——使用它们和使用它们的标准版本在方式上并没有什么区别。不过我们并不在这一章使用这种方法，而是使用现代编译器所提供的 _内置函数 (intrinsic functions)_。

### 准备

要使用 x86 的内置，我们需要做一些基础工作。

首先，我们得确认自己的硬件支持哪些扩展。在 Linux 上，我们可以调用 `cat /proc/cpuinfo`，而在其他平台上，最好是到 [WikiChip](https://en.wikichip.org/wiki/WikiChip) 上查找。

还有一个特殊的 `CPUID` 汇编指令，它允许查询有关 CPU 的各种信息，包括对特定向量扩展的支持。`CPUID` 经常被用于在运行时获取 CPU 信息，这样就不需要对各种不同的架构发布不同的二进制文件。它的输出信息以特征掩码的形式非常密集地存储，因此编译器提供了一些内置方法来理解它。下面是一个例子：

```rust
use std::arch;

fn main() {
    println!("Checking CPU support for SIMD instructions...");
    println!("SSE: {}", arch::is_x86_feature_detected!("sse"));
    println!("SSE2: {}", arch::is_x86_feature_detected!("sse2"));
    println!("AVX: {}", arch::is_x86_feature_detected!("avx"));
    println!("AVX2: {}", arch::is_x86_feature_detected!("avx2"));
    println!("AVX512: {}", arch::is_x86_feature_detected!("avx512f"));
}
```

在 rust 中，我们需要从 `std::arch` 中找出自己的目标架构，从而引入相应的函数。

最后，我们需要告诉编译器我们会将二进制文件运行在目标平台上。在 rust 中会使用 `#[cfg(target_arch = "x86_64")]`，从而仅在指定的平台上编译相应的代码。随后使用 `if cfg!(target_arch = "x86_64") {...} else {...}` 可以在特定的平台上运行相应的代码。

在本章中我们重点关注 AVX2 以及之前的 SIMD 扩展，它们可以在 95% 的电脑上运行。

### SIMD 寄存器

SIMD 扩展中最值得注意的版本是那些扩展了寄存器大小的版本：

- SSE (1999) 增加了 16 个 128 比特寄存器，从 `xmm0` 编号到 `xmm15`；
- AVX (2011) 增加了 16 个 256 比特寄存器，从 `ymm0` 编号到 `ymm15`；
- AVX512 (2017) 增加了[^AVX512] 16 个 512 比特寄存器，从 `zmm0` 编号到 `zmm15`。

[^AVX512]: AVX512 还增加了 8 个被叫做掩码寄存器的东西，从 `k0` 到 `k7`，用于遮罩和混合数据。而本文重点介绍 AVX2 及其之前的版本，因此不会包含这一部分。

从命名 (512) 和 512 位数据已经占满了整个缓存行来看，x86 架构的设计者近期并不打算再次增大寄存器。

C/C++ 编译器会使用一些特殊的向量类型来表示数据存储在这些特殊的寄存器中：

- 128 位的 `__m128`, `__m128d` 和 `__m128i` 分别使用于单精度浮点数，双精度浮点数和不同的整数类型；
- 256 位的 `__m256`, `__m256d` 和 `__m256i`；
- 512 位的 `__m512`, `__m512d` 和 `__m512i`。

寄存器中其实可以存储任何种类的数据，上面的类型只是用于类型检查。事实上，你可以直接将一个向量变量转化为其它向量类型，就像是你转换其它基本数据类型，这基本不会产生开销。

### SIMD 内建函数（内在函数、内部函数？）

编译器内建函数是一些 C 语言风格的函数，一般都是简单的调取一些相关的汇编指令来实现。

例如，我们可以使用 AVX 内建函数对两个 64 位浮点数的数组进行求和：

```rust
pub fn simd_add(a: &[f64], b: &[f64], c: &mut [f64]) {
    assert!(a.len() == b.len() && a.len() == c.len());
    let chunk_size = 4; // Number of f64 values processed in one SIMD operation
    let len = a.len();
    let simd_chunks = len / chunk_size * chunk_size;

    unsafe {
        for i in (0..simd_chunks).step_by(chunk_size) {
            let a_vec = arch::x86_64::_mm256_loadu_pd(a.as_ptr().add(i));
            let b_vec = arch::x86_64::_mm256_loadu_pd(b.as_ptr().add(i));
            let c_vec = arch::x86_64::_mm256_add_pd(a_vec, b_vec);
            arch::x86_64::_mm256_storeu_pd(c.as_mut_ptr().add(i), c_vec);
        }
    }

    // Handle remaining elements
    for i in simd_chunks..len {
        c[i] = a[i] + b[i];
    }
}
```

> 在实验中，该 `simd_sum` 求和 $$10^7$$ 个 64 位浮点数所需的时间为 19ms，而简单实现的 `sum` 则只需要 15ms。可能是编译器优化的太好了？但是在 `cargo-show-asm` 中并没有很明显的看到使用 SIMD 指令集。
{: .prompt-warning }
> 是的。在 `simd_sum` 中，rust 编译的结果产生了四个 call 指令调用对应的函数。而 `sum` 中，rust 编译时进行了 simd 优化，且如后文所说，只使用了三条向量指令。
{: .prompt-tip }
> rust 如果想要使用 AVX 指令集（256 位），需要指定 `--target-cpu=x86-64-v3`。而默认的选项是 `--target-cpu=x86-64`，这导致其只能使用 128 位的寄存器。修改 `target-cpu` 后，两者的速度都基本是 14.5ms。顺便一提，在每 4 个浮点数相加的情况下，编译器仍然对两个函数都做了循环展开优化。
{: .prompt-tip }
> 之前的某个章节其实提到过 `target-cpu=native` 表示本机编译本机运行，可以自动根据当前 cpu 进行优化来着，忘了这一点了。
{: .prompt-tip }

使用 SIMD 最主要的挑战之一是将数据切分为固定大小的块放入寄存器中。一般有两种解决方案：

- 给最后一些部分填充一些不会影响计算结果的数据，例如上述代码中应该给最后填 0。
- 留出最后一部分数据，用平常的方式计算它们。

### 指令引用

大多数 SIMD 指令的命名都是 `_mm<size>_<action>_<type>` 这种样子，并且对应于一个长得差不多的汇编指令。在习惯了这些命名后就能一眼看出来这个指令是做什么，但是还是有一些指令看上去是小猫踩到键盘上才出来的。（试着破译一下：`punpcklqdq`）

这里有一些更多的例子：

- `_mm_add_epi16`：将两个 128 位向量看作 16 位扩展打包整数 (extended packed integers) 相加。
- `_mm256_acos_pd`：计算 4 个打包 double 的 $$\arccos$$ 值。
- `_mm256_broadcast_sd`：广播（复制） 1 个 double 到结果向量中的 4 个元素。
- `_mm256_ceil_pd`：将 4 个 double 向上取整。
- `_mm256_cmpeq_epi32`：比较 2 个 256 位向量，每个向量有 $$256/32=8$$ 个整数，返回一个掩码，其中 1 表示对应的两个整数相等。
- `_mm256_blendv_ps`：根据传入的掩码获取对应的单精度浮点数。

你可能感觉到这里有非常多数量的内建函数，除此之外，有一些指令还包含立即数——因此这些内建函数需要编译期常数：例如，浮点数比较的指令有 32 个版本。

出于某些原因，有些操作对寄存器中存储的数据类型无特定要求，但仅支持特定向量类型（通常是 32 位浮点数）——若要使用这类内建函数，只需进行数据类型的转换即可。为简化本章示例，我们将主要使用 256 位 AVX2 寄存器中的32位整数 (epi32)。

x86 SIMD 内建函数的一个极佳参考是[《Intel 内建函数指南》](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)，该指南按类别和扩展集进行分类，包含功能描述、伪代码、对应汇编指令及其在英特尔微架构上的延迟与吞吐量数据。建议你将这个页面添加至书签。

当你确认某条特定指令存在，仅需查询其名称或性能信息时，英特尔参考手册非常实用。若不确定所需指令是否存在，这份[速查表](https://db.in.tum.de/~finis/x86%20intrinsics%20cheat%20sheet%20v1.0.pdf)或许能更高效地满足需求。

**指令选择**。注意编译器并不一定会选择你想要的特定指令。类似于我们之前讨论的标量加法并赋值 $$c = a + b$$，这里也有一个融合的向量加法指令。所以编译器并不是使用我们写的每个循环周期 $$2+1+1=4$$ 个指令，而是用 $$3$$ 个指令块重写上面的代码，[像这样](https://godbolt.org/z/dMz8E5Ye8)：

```nasm
vmovapd ymm1, YMMWORD PTR a[rax]
vaddpd  ymm0, ymm1, YMMWORD PTR b[rax]
vmovapd YMMWORD PTR c[rax], ymm0
```

有时候（虽然很少），这种编译器的行为会使事情变得更糟。所以仔细检查程序集并检查使用的向量指令总是一个好主意（AVX 指令通常以“v”开头，或者你可以查看寄存器的命名是否为 `*mm` 来判断是不是 SIMD）。

此外，一些内建函数也并不对应于单个指令，例如 broadcasts 和 extracts，会对应一小串指令。

### GCC 向量扩展

如果你觉得 C 语言的内建指令非常混乱，那你不是一个人。原作者花了几百个小时在写 SIMD 代码和阅读《Intel 内建函数指南》上，但是他还是不知道自己是要输入 `_mm256` 还是 `__m256`。

内建函数即难用，而且可移植性和可维护性也很差。在优秀的软件中，开发者并不希望为每款 CPU 维护不同的程序；而是希望以与架构无关的方式，仅实现一次即可。

有一天，GNU 项目的编译器工程师也产生了同样的想法，并研发出一种方法——允许用户定义专属的向量类型。这种类型使用起来更接近数组，且部分运算符经过重载，可匹配相关指令。

在 GCC 中，定义一个 “包含 8 个整数、打包存储在 256 位（32 字节）寄存器中” 的向量，方式如下：

```cpp
typedef int v8si __attribute__ (( vector_size(32) ));
// type ^   ^ typename          size in bytes ^ 
```

遗憾的是，这并非 C 或 C++ 标准的一部分，因此不同编译器会采用不同的语法来实现这一功能。

这里只存在一种大致的命名约定，即把元素的数量和类型纳入类型名称中：在上面的示例里，我们定义的是 “8 个有符号整数组成的向量”。不过你也可以自行选择任意名称，比如 vec、reg 之类均可。唯一需要避免的是将其命名为 vector——因为这会与 `std::vector` 造成极大混淆。

使用这种类型的主要优势在于，对于多数操作，你无需查阅对应的内置函数，直接使用常规的 C++ 运算符即可完成。

```cpp
v4si a = {1, 2, 3, 5};
v4si b = {8, 13, 21, 34};

v4si c = a + b;

for (int i = 0; i < 4; i++)
    printf("%d\n", c[i]);

c *= 2; // multiply by scalar

for (int i = 0; i < 4; i++)
    printf("%d\n", c[i]);
```

使用向量类型，我们可以极大地简化之前使用内建函数实现的 “a + b” 循环：

```cpp
typedef double v4d __attribute__ (( vector_size(32) ));
v4d a[100/4], b[100/4], c[100/4];

for (int i = 0; i < 100/4; i++)
    c[i] = a[i] + b[i];
```

如你所见，相较于内置函数带来的混乱局面，向量扩展的用法要简洁得多。但它也存在缺点：有些我们想要实现的功能，仅通过原生 C++ 的向量扩展是无法表达的，因此仍需借助内置函数来完成。幸运的是，这并非非此即彼的选择——因为向量类型与 `_mm` 系列类型可以进行零成本转换：

```cpp
v8f x;
int mask = _mm256_movemask_ps((__m256) x)
```

这里 `(__m256) x` 就是在类型转换。

此外，还有许多适用于不同语言的第三方库，它们不仅能提供类似的可移植 SIMD 代码编写能力，还实现了部分常用功能，且总体使用体验比内建函数和编译器内置向量类型都更友好。C++ 领域中值得关注的例子包括 Highway、Expressive Vector Engine、Vector Class Library 以及 xsimd。另外，c++26 中打算加入 `std::simd`，rust 的 `std::simd` 则到目前为止仍是实验性功能。

建议使用成熟的 SIMD 库，因为它能显著提升开发体验。不过，本书将尽量贴近硬件底层，主要直接使用内建函数；仅在条件允许且为了简化代码的情况下，偶尔切换到向量扩展。

## 二 - 移动数据

如果你查看了[参考文献](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)，你会注意到所有的向量操作本质上有两种：

1. 对向量的元素执行某些操作的指令（`+`，`*`，`<` 等）；
2. 加载、存储、掩码、打乱和移动数据的指令。

使用逐元素指令虽然简单，但 SIMD 面临的最大挑战在于如何高效地将数据载入向量寄存器——这需要足够低的开销，才能确保整个优化过程具有实际价值。

### 对齐加载和存储

将数据读写到 SIMD 寄存器的指令都有两个版本 `load`/`loadu` 和 `store`/`storeu`。这里的 u 表示 unaligned。 前一种版本只有在读写同一缓存行的数据时才会正常执行，而后一种在任何情况下都可以正确执行，只是有时会损失一点性能。

但是当内部操作非常轻量的时候，不同版本的性能差异就开始变得显著（因为每次计算都需要获取两次缓存行）。例如，将两个数组进行求和：

```rust
for i in (0..simd_chunks).step_by(chunk_size) {
    let a_vec = arch::x86_64::_mm256_loadu_pd(a.as_ptr().add(i));
    let b_vec = arch::x86_64::_mm256_loadu_pd(b.as_ptr().add(i));
    let c_vec = arch::x86_64::_mm256_add_pd(a_vec, b_vec);
    arch::x86_64::_mm256_storeu_pd(c.as_mut_ptr().add(i), c_vec);
}
```

在有时，相较于 aligned 版本会慢个 ~30%：

```rust
for i in (0..simd_chunks).step_by(chunk_size) {
    let a_vec = arch::x86_64::_mm256_load_pd(a.as_ptr().add(i));
    let b_vec = arch::x86_64::_mm256_load_pd(b.as_ptr().add(i));
    let c_vec = arch::x86_64::_mm256_add_pd(a_vec, b_vec);
    arch::x86_64::_mm256_store_pd(c.as_mut_ptr().add(i), c_vec);
}
```

在第一个版本中，倘若数组没有对齐，由于一个缓存行是 512 比特，这使得有一半的读写都是“坏的”，会占用更多的时间。

需要注意的是，这种性能差异是由缓存系统而非指令本身导致的。在大多数仙丹处理器上，`loadu`/`storeu` 指令的在读写一个缓存行内时的1性能应该和 `load`/`store` 的性能相近。后者的优势在于，它们能作为免费的运行时断言，确保所有读写操作都处于对齐状态。（否则报错）

因此，在分配内存时正确对齐数组及其他数据至关重要，这也是编译器无法始终高效实现自动向量化的原因之一。在多数应用场景中，我们只需要确保任意 32 字节的 SIMD 数据块不会跨越缓存行边界。在 cpp 中，这可以通过 alignas 说明符来指定对齐方式：

```cpp
alignas(32) float a[n];

for (int i = 0; i < n; i += 8) {
    __m256 x = _mm256_load_ps(&a[i]);
    // ...
}
```

内置的向量类型本身已具备相应的对齐要求，并默认内存读写操作是对齐的——因此当你分配 `v8si` 类型的数组时总是安全的，但若要从 `int*` 指针转换为该类型，则必须确保源数据本身是对齐的。

与标量运算类似，许多向量算术指令可以直接将内存地址作为操作数——例如向量加法指令。不过开发者无法直接将其作为显式内联函数调用，而需依赖编译器优化实现。此外还存在其他特定的内存读取指令，尤其是非临时性加载/存储操作，这类指令能避免在缓存层级中保留所访问的数据。（可以查看上一章节）

### 寄存器别名

初代 SIMD 扩展指令集 MMX 的起点相当简单，仅支持 64 位向量运算。其设计巧妙地复用了 80 位浮点数尾数部分的存储空间，从而避免了引入独立的寄存器组。随着后续扩展逐步增大向量尺寸，为维持向后兼容性，向量寄存器沿用了通用寄存器采用的同名寄存器分层机制：例如 xmm0 对应 ymm0 的前半部分（128位），xmm1 对应 ymm1 的前半部分，依此类推。

这一特性，加上向量寄存器位于浮点处理单元的事实，使得在向量寄存器与通用寄存器之间传输数据的过程略显复杂。

### 提取和插入

要从向量中提取特定数值，可以使用 `_mm256_extract_epi32` 及类似的内联函数。该函数以需要提取的整数索引作为第二参数，并根据索引值的不同生成相应的指令序列。

当需要提取首元素时，编译器会生成 `vmovd` 指令（操作对象为向量的前半部分xmm0）：

```nasm
vmovd eax, xmm0
```

如果需要从 AVX 向量的后半部分提取任何元素，则必须先分离出该后半部分向量，再从中提取目标标量值。例如，以下是提取最后一个（第八个）元素的具体实现：

```nasm
vextracti128 xmm0, ymm0, 0x1
vpextrd      eax, xmm0, 3
```

有一个类似的 `_mm256_insert_epi32` 内部函数用于覆盖特定的元素：

```nasm
mov          eax, 42

; v = _mm256_insert_epi32(v, 42, 0);
vpinsrd xmm2, xmm0, eax, 0
vinserti128     ymm0, ymm0, xmm2, 0x0

; v = _mm256_insert_epi32(v, 42, 7);
vextracti128 xmm1, ymm0, 0x1
vpinsrd      xmm2, xmm1, eax, 3
vinserti128  ymm0, ymm0, xmm2, 0x1
```

总的来说，在标量数据与向量寄存器之间的数据传输效率较低，尤其当操作对象不是首元素时，性能损耗更为显著。

### 制造常量

如果你需要填充的不仅仅是一个元素，而是整个向量，你可以使用 `_mm256_setr_epi32` 内部函数：

```cpp
__m256 iota = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);
```

此处 r 代表“反向”——这是从 CPU 视角而言的，对人类来说并不直观。另外还存在不带 r 的 `_mm256_set_epi32` 函数，其数据填充方向与之相反。这两种函数主要用于创建编译期常量，随后通过块加载指令将常量载入寄存器。若需将向量全部置零，应使用 `_mm256_setzero_si256` 函数：它通过让寄存器与自身进行异或运算实现归零。

对于内置的向量类型，直接使用常规花括号初始化即可：

```cpp
vec zero = {};
vec iota = {0, 1, 2, 3, 4, 5, 6, 7};
```

### 广播（Broadcast）

除了修改一个元素，你还可以将一个值广播到向量的所有位置：

```nasm
; __m256i v = _mm256_set1_epi32(42);
mov          eax, 42
vmovd        xmm0, eax
vpbroadcastd ymm0, xmm0
```

这是一个经常使用的操作，所以你也可以使用内存地址：

```nasm
; __m256 v = _mm256_broadcast_ss(&a[i]);
vbroadcastss ymm0, DWORD PTR [rdi]
```

在使用内部向量类型时，就可以使用零向量加上一个标量：

```cpp
vec v = 42 + vec{};
```

### 映射到数组

若希望规避上述所有复杂性，可以直接将向量数据转储至内存，再以标量形式读取其中的值。

```cpp
void print(__m256i v) {
    auto t = (unsigned*) &v;
    for (int i = 0; i < 8; i++)
        std::cout << std::bitset<32>(t[i]) << " ";
    std::cout << std::endl;
}
```

这种做法虽在执行效率或技术规范性上可能存在问题（C++ 标准未明确定义此类强制类型转换的行为），但其实现极为简洁。原作者在调试过程中就经常使用这段代码来输出向量内容。

### 不连续加载

后续的 SIMD 扩展指令集引入了特殊的“聚集”（gather）和“散射”（scatter）指令，能够通过任意数组索引实现非连续数据读写。虽然这些指令无法实现八倍速提升（通常受限于内存带宽而非 CPU 性能），但在稀疏线性代数等特定应用场景中仍极具价值。

聚集指令自 AVX2 架构开始支持，而各类散射指令则需 AVX512 架构才可实现。

![gather-scatter](/assets/simd/gather-scatter.png)

让我们测试一下它是否比标量读取更快一些。我们创建一个大小为 $$N$$ 的数组和 $$Q$$ 次随机查询：

```cpp
int a[N], q[Q];

for (int i = 0; i < N; i++)
    a[i] = rand();

for (int i = 0; i < Q; i++)
    q[i] = rand() % N;
```

在标量版本的代码中，我们将查询的指定元素逐个添加到校验和中：

```cpp
int s = 0;

for (int i = 0; i < Q; i++)
    s += a[q[i]];
```

在 SIMD 代码中，我们使用 gather 指令并行处理 8 个不同的索引：

```cpp
reg s = _mm256_setzero_si256();

for (int i = 0; i < Q; i += 8) {
    reg idx = _mm256_load_si256( (reg*) &q[i] );
    reg x = _mm256_i32gather_epi32(a, idx, 4);
    s = _mm256_add_epi32(s, x);
}
```

它们的性能大致相同，除了当数组大小可以放入 L1 缓存的时候：

![gather-scatter](/assets/simd/gather.svg)

> 可能是因为：在 L1 缓存上，一次提供多个地址读取比一个一个读取显然更快。而当缓存开销增加时，获取数据本身更加花费时间，时间瓶颈并不在到底是“一次提供多个地址”和“一个一个提供地址”上。
{: .prompt-warning }

`gather` 和 `scatter` 的设计目标并非加速内存操作，而是将数据高效载入寄存器以执行密集计算。对于计算成本超过单次加法操作的场景，这些指令都具有显著优势。

（高效）的聚集与散射指令的缺失，使得 CPU 上的 SIMD 编程模式与支持独立内存访问的真正并行计算环境截然不同（如 GPU）。开发者必须持续设计适配方案，通过多种数据重组策略将数据连续排列，才能将其有效载入寄存器。

## 三 - 归约

归约（或者叫做折叠）是一种在区间上计算具有结合律和交换律的运算的函数。

例如计算一个数组的和就是一个归约：

```rust
pub fn sum(a: &[i32]) -> i32 {
    let mut s = 0;
    for &val in a {
        s += val;
    }
    s
}
```

将求和进行向量化的方法并不只管，因为每个循环都依赖于上一次迭代。克服这个问题的方法是将 $$s$$ 拆分为 8 个累加器，这样 $$s_i$$ 就会包含数组中模 8 下第 $$i$$ 个元素之和：

$$
s_i = \sum_{j=0}^{n/8} a_{8j+i}
$$

如果将这 8 个累加器放入一个 256 比特向量，我们就可以指令一次性将数组中连续的 8 个元素加入到累加器中。通过使用向量扩展，这很容易实现：

```rust
pub fn simd_sum(a: &[i32]) -> i32 {
    use std::arch;
    
    unsafe {
        let (prefix, simd_chunks, suffix) = a.align_to::<arch::x86_64::__m256i>();
        let mut sum_vec = arch::x86_64::_mm256_setzero_si256();
        for &chunk in simd_chunks {
            sum_vec = arch::x86_64::_mm256_add_epi32(sum_vec, chunk);
        }
        let mut sum = 0;
        let mut temp = [0i32; 8];
        arch::x86_64::_mm256_storeu_si256(temp.as_mut_ptr() as *mut arch::x86_64::__m256i, sum_vec);
        for &val in &temp {
            sum += val;
        }
        for &val in prefix {
            sum += val;
        }
        for &val in suffix {
            sum += val;
        }
        sum
    }
}
```

这种方法本质上是基于结合律和交换律的性质，因此你可以用到其它运算上，例如找最小值或者求异或和。

### 指令级并行

我们的实现和编译器自动实现的向量化十分接近，但它实际上是次优的：向量加法需要等待 1 个周期才能得到结果，但是在这个微架构上，向量加法的吞吐量为 2。即一个周期可以同时处理两条向量加法。

通过将数组拆成 $$B\geq 2$$ 个部分并且内部独立使用累加器，我们就可以将向量加法的吞吐量提升两倍：

```rust
pub fn block_simd_sum<const B: usize>(a: &[i32]) -> i32 {
    use std::arch;

    unsafe {
        let mut block_sums = [arch::x86_64::_mm256_setzero_si256(); B];
        let (prefix, simd_chunks, suffix) = a.align_to::<[arch::x86_64::__m256i; B]>();
        for chunk_pair in simd_chunks {
            for i in 0..B {
                block_sums[i] = arch::x86_64::_mm256_add_epi32(block_sums[i], chunk_pair[i]);
            }
        }
        let mut total_sum = 0;
        for &block_sum in &block_sums {
            let mut temp = [0i32; 8];
            arch::x86_64::_mm256_storeu_si256(temp.as_mut_ptr() as *mut arch::x86_64::__m256i, block_sum);
            for &val in &temp {
                total_sum += val;
            }
        }
        for &val in prefix {
            total_sum += val;
        }
        for &val in suffix {
            total_sum += val;
        }
        total_sum
    }
}
```

若处理器具备两个以上相关执行端口，可相应提高 $$B$$ 常数，但这种 n 倍性能提升仅适用于能放入 L1 缓存的数组——对于更大规模的数据，内存带宽将成为性能瓶颈。

> 在我的机子上，并没有成功实现加速。反而在大于 L1 时各个方法都差不多，在小于 L1 时，编译器自行优化的最快，分成多个块的最慢。
{: .prompt-warning }

### 水平求和

将向量寄存器中存储的8个累加器合并为单个标量以求取总和的过程，被称为“水平求和”。

虽然逐个提取标量并相加只需要恒定的周期数，但通过使用特殊指令将寄存器中相邻元素两两相加，可以略微提升计算速度。

![hsum](/assets/simd/hsum.png)
_SSE/AVX 中的水平求和操作。请注意输出结果的存储方式：有些时候会使用 (a b a b) 的交错排列模式。具体需要查看[手册](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#techs=AVX,AVX2&ig_expand=3037,3009,5135,4870,4870,4872,4875,833,879,874,849,848,6715,4845&text=horizontal)_

由于这是一种高度特化的操作，必须通过SIMD内联函数实现——尽管编译器为标量代码生成的指令序列可能大致相同：

```rust
pub fn hsum(a: std::arch::x86_64::__m256i) -> i32 {
    use std::arch::x86_64::*;
    unsafe {
        let lo = _mm256_extracti128_si256(a, 0);
        let hi = _mm256_extracti128_si256(a, 1);
        let sum128 = _mm_add_epi32(lo, hi);
        let hsum128 = _mm_hadd_epi32(sum128, sum128);
        _mm_extract_epi32(hsum128, 0) + _mm_extract_epi32(hsum128, 1)
    }
}
```

存在[其他类似的指令](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#techs=AVX,AVX2&ig_expand=3037,3009,5135,4870,4870,4872,4875,833,879,874,849,848,6715,4845&text=horizontal)，例如用于整数乘法或计算相邻元素绝对差值的指令（常用于图像处理）。

还有一条特殊的指令 `_mm_minpos_epu16`，可一次性计算八个 16 位整数的水平最小值及其索引。这是唯一能单步完成的水平归约操作，其他所有归约运算都需要多步处理。

## 四 - 掩码与混合

SIMD编程的一大挑战在于其控制流选项极其有限——因为对向量施加的操作对其中所有元素都是一致的。  

这使得平时只需要一条 if 语句或其他分支结构就能轻松解决的问题变得复杂许多。在 SIMD 编程中，这类问题必须通过各种无分支编程技术来处理，而这些技术的应用并不简单。

### 掩码

一种实现无分支的主要方法是 _谓词_——同时计算两个分支的结果，然后通过算术技巧或者特殊的“条件移动”指令进行选择：

```cpp
for (int i = 0; i < N; i++)
    a[i] = rand() % 100;

int s = 0;

// branch:
for (int i = 0; i < N; i++)
    if (a[i] < 50)
        s += a[i];

// no branch:
for (int i = 0; i < N; i++)
    s += (a[i] < 50) * a[i];

// also no branch:
for (int i = 0; i < N; i++)
    s += (a[i] < 50 ? a[i] : 0);
```

为了向量化这个循环，我们需要介绍两个新的指令：

- `_mm256_cmpgt_epi32`：用于比较两个向量中的 `i32` 整数，当第一个元素大于第二个时生成全 1 掩码（32 位），否则生成全 0 掩码。
- `_mm256_blendv_epi8`：根据提供的掩码混合（组合）两个向量的值。

通过对向量元素进行掩码处理和混合，使得计算仅影响被选择的元素子集，我们可以实现类似于条件移动操作的谓词化计算：

```cpp
const reg c = _mm256_set1_epi32(49);
const reg z = _mm256_setzero_si256();
reg s = _mm256_setzero_si256();

for (int i = 0; i < N; i += 8) {
    reg x = _mm256_load_si256( (reg*) &a[i] );
    reg mask = _mm256_cmpgt_epi32(x, c);
    x = _mm256_blendv_epi8(x, z, mask);
    s = _mm256_add_epi32(s, x);
}
```

（这里省略了水平求和以及对数组剩余部分的处理）

这就是谓词在 SIMD 中经常被使用的方式，但这种方式并不总是最好的。既然 `z` 是全零值，而掩码是通过 0 和 1 来约定这个位置上是哪个元素，我们也可以使用位运算 `and` 来获取所有 1 上的对应元素：

```cpp
const reg c = _mm256_set1_epi32(50);
reg s = _mm256_setzero_si256();

for (int i = 0; i < N; i += 8) {
    reg x = _mm256_load_si256( (reg*) &a[i] );
    reg mask = _mm256_cmpgt_epi32(c, x);
    x = _mm256_and_si256(x, mask);
    s = _mm256_add_epi32(s, x);
}
```

这个循环的运行速度略有提升，因为在特定的 CPU 架构中，向量的与操作比混合操作少用一个时钟周期。

还有一些其他指令同样支持掩码作为输入参数，其中最值得注意的是：

- `_mm256_blend_epi32` 内部函数是一种混合操作，它接受 8 位整数掩码而非向量掩码（这也是其名称末尾没有 "v" 的原因）。
- `_mm256_maskload_epi32` 和 `_mm256_maskstore_epi32` 内部函数能够一次性完成从内存加载/存储 SIMD 数据块并与掩码进行与操作。

我们同样可以在内置向量类型上应用谓词化技术：

```cpp
vec *v = (vec*) a;
vec s = {};

for (int i = 0; i < N / 8; i++)
    s += (v[i] < 50 ? v[i] : 0);
```

所有这些版本的性能都维持在约 13 GFLOPS。因为这个示例过于简单，编译器完全能够自行完成循环的向量化。接下来我们将转向更复杂的、无法被自动向量化的案例。

### 查找

在这里，需要从数组中找出一个特定的值并返回：

```cpp
const int N = (1<<12);
int a[N];

int find(int x) {
    for (int i = 0; i < N; i++)
        if (a[i] == x)
            return i;
    return -1;
}
```

为了对 `find` 进行基准测试，原文用 $$[0,N-1]$$ 填充数组，然后不断地随机查找：

```cpp
for (int i = 0; i < N; i++)
    a[i] = i;

for (int t = 0; t < K; t++)
    checksum ^= find(rand() % N);
```

标量版本的性能约为 4 GFLOPS。这个数字包含了我们不需要处理的元素，所以在你的脑海中把这个数字除以 2（即我们实际需要检查元素的预期比例）。

要实现向量化，我们需要将向量中的元素与搜索值进行相等比较，生成掩码，然后通过某种方式检查该掩码是否为零。若非零，则说明目标元素位于这个 8 元素块内。

要检查掩码是否为零，可以使用 `_mm256_movemask_ps` 内部函数——该函数提取向量中每个 32 位元素的首位，并生成一个 8 位整数掩码。随后我们可以检查该掩码是否非零：若确实非零，还能立即通过 `ctz` 指令获取具体索引位置：

```cpp
int find(int needle) {
    reg x = _mm256_set1_epi32(needle);

    for (int i = 0; i < N; i += 8) {
        reg y = _mm256_load_si256( (reg*) &a[i] );
        reg m = _mm256_cmpeq_epi32(x, y);
        int mask = _mm256_movemask_ps((__m256) m);
        if (mask != 0)
            return i + __builtin_ctz(mask);
    }

    return -1;
}
```

这个版本大约是 20 GFLOPS，也即标量版本的 5 倍。其中在热点循环中只使用了三条指令：

```nasm
vpcmpeqd  ymm0, ymm1, YMMWORD PTR a[0+rdx*4]
vmovmskps eax, ymm0
test      eax, eax
je        loop
```

检查一个向量是否为零是一个常见的操作，在 SIMD 中有一个类似于 `test 的操作：

```cpp
int find(int needle) {
    reg x = _mm256_set1_epi32(needle);

    for (int i = 0; i < N; i += 8) {
        reg y = _mm256_load_si256( (reg*) &a[i] );
        reg m = _mm256_cmpeq_epi32(x, y);
        if (!_mm256_testz_si256(m, m)) {
            int mask = _mm256_movemask_ps((__m256) m);
            return i + __builtin_ctz(mask);
        }
    }

    return -1;
}
```

我们将 `movemask` 移动到了 `if` 中，这样在热点循环中的指令又少了一条：

```cpp
vpcmpeqd ymm0, ymm1, YMMWORD PTR a[0+rdx*4]
vptest   ymm0, ymm0
je       loop
```

可惜的是这并没有提高多少性能，因为 `vptest` 和 `vmovmskps` 的吞吐量都是 1，无论在循环中做什么都会成为计算瓶颈。而 `vpcmpeqd` 的延迟是 1，CPI 是 0.5，这意味着有半个周期被用于等待结果，产生了一定的闲置。

为突破这一限制，我们可以以 16 个元素为块进行迭代，并通过按位或运算合并两个 256 位 AVX2 寄存器的独立比较结果：

```cpp
int find(int needle) {
    reg x = _mm256_set1_epi32(needle);

    for (int i = 0; i < N; i += 16) {
        reg y1 = _mm256_load_si256( (reg*) &a[i] );
        reg y2 = _mm256_load_si256( (reg*) &a[i + 8] );
        reg m1 = _mm256_cmpeq_epi32(x, y1);
        reg m2 = _mm256_cmpeq_epi32(x, y2);
        reg m = _mm256_or_si256(m1, m2);
        if (!_mm256_testz_si256(m, m)) {
            int mask = (_mm256_movemask_ps((__m256) m2) << 8)
                     +  _mm256_movemask_ps((__m256) m1);
            return i + __builtin_ctz(mask);
        }
    }

    return -1;
}
```

产生的汇编指令中的循环部分为：

```nasm
vpcmpeqd ymm2, ymm1, YMMWORD PTR a[0+rdx*4]
vpcmpeqd ymm3, ymm1, YMMWORD PTR a[32+rdx*4]
vpor     ymm0, ymm3, ymm2
vptest   ymm0, ymm0
je       loop
```

现在的性能达到了 ~34 GFLOPS。但是为什么不是 40？性能不应该翻倍吗？

这是因为每个迭代需要执行 5 条指令（包括 `je`）。虽然相关执行端口的吞吐量理论上支持平均每周期完成这些操作，但由于该特定 CPU（Zen 2）的解码宽度仅为 4，我们无法实现理论峰值。因此，实际性能只能达到理想值的五分之四。

为缓解这一问题，我们可以再次将每次迭代处理的 SIMD 块数量翻倍：

```cpp
unsigned get_mask(reg m) {
    return _mm256_movemask_ps((__m256) m);
}

reg cmp(reg x, int *p) {
    reg y = _mm256_load_si256( (reg*) p );
    return _mm256_cmpeq_epi32(x, y);
}

int find(int needle) {
    reg x = _mm256_set1_epi32(needle);

    for (int i = 0; i < N; i += 32) {
        reg m1 = cmp(x, &a[i]);
        reg m2 = cmp(x, &a[i + 8]);
        reg m3 = cmp(x, &a[i + 16]);
        reg m4 = cmp(x, &a[i + 24]);
        reg m12 = _mm256_or_si256(m1, m2);
        reg m34 = _mm256_or_si256(m3, m4);
        reg m = _mm256_or_si256(m12, m34);
        if (!_mm256_testz_si256(m, m)) {
            unsigned mask = (get_mask(m4) << 24)
                          + (get_mask(m3) << 16)
                          + (get_mask(m2) << 8)
                          +  get_mask(m1);
            return i + __builtin_ctz(mask);
        }
    }

    return -1;
}
```

现在的吞吐量就达到了 43 GFLOPS——大约是原始版本的 10 倍。

尝试将每个周期处理的元素数量扩大到 64 个的时候则收效甚微：对于小规模数组，当触发条件判断时，所有额外的 `movemask` 操作会产生显著的开销；而对于大规模数组，内存带宽本身则成为瓶颈。

### 计数

```cpp
int count(int x) {
    int cnt = 0;
    for (int i = 0; i < N; i++)
        cnt += (a[i] == x);
    return cnt;
}
```

为了向量化它，我们只需要输出掩码中有多少个 1：

```cpp
const reg ones = _mm256_set1_epi32(1);

int count(int needle) {
    reg x = _mm256_set1_epi32(needle);
    reg s = _mm256_setzero_si256();

    for (int i = 0; i < N; i += 8) {
        reg y = _mm256_load_si256( (reg*) &a[i] );
        reg m = _mm256_cmpeq_epi32(x, y);
        m = _mm256_and_si256(m, ones);
        s = _mm256_add_epi32(s, m);
    }

    return hsum(s);
}
```

上述代码利用 `ones` 与掩码求与，从所有的 32 位全 1 掩码中只拿出了一个 1，随后累加到一个 8 元素寄存器中。

两种实现的性能都是 ~15 GFLOPS：因为编译器会自动将第一种进行向量化。

但是编译器未能发现的一个技巧是：全 1 掩码在被解释为 `i32` 时，其值为 -1.因此我们可以跳过和 `ones` 求与的步骤，直接使用掩码本身求和：

```cpp
int count(int needle) {
    reg x = _mm256_set1_epi32(needle);
    reg s = _mm256_setzero_si256();

    for (int i = 0; i < N; i += 8) {
        reg y = _mm256_load_si256( (reg*) &a[i] );
        reg m = _mm256_cmpeq_epi32(x, y);
        s = _mm256_add_epi32(s, m);
    }

    return -hsum(s);
}
```

现在的性能达到了 ~22 GFLOPS，这已经是能达到的最好了。

在将此代码适配至较短数据类型时，需注意累加器可能发生溢出（例如 8 位累加器只能加 255 次）。为解决此问题，可增加一个更大容量的辅助累加器，并定期暂停循环以将局部累加器的值累加到辅助累加器中，随后重置局部累加器。例如对于 8 位整数，这意味着需要创建另一个每 $$\left\lfloor\frac{256 - 1}{8}\right\rfloor =15$$ 次迭代执行一次水平求和的内层循环（水平求和的位数也是 8，因此次数的总和不能超过 255）。

## 五 - 寄存器内混洗

掩码操作可以让我们对向量中的部分元素进行操作。虽然这是一种极为高效且常用的技术，但在许多场景中，我们还要对数据进行更加复杂的操作。这类操作涉及对向量寄存器内部数据的重排列，而不仅仅是和其它向量的混合。

问题在于，在硬件中为每一种情况都加入一个元素重排指令是不现实的。不过，我们可以添加一种通用的置换指令，该指令接收置换索引，并通过预先计算好的查找表来生成这些索引。

### Shuffles 和 Popcount

_Population count_，也被叫做 _Hamming weight_，是一个二进制字符串中 1 的数量。

这是一个非常常用的操作，以至于在 `x86` 上专门有一条指令计算一个字的 popcount：

```cpp
const int N = (1<<12);
int a[N];

int popcnt() {
    int res = 0;
    for (int i = 0; i < N; i++)
        res += __builtin_popcount(a[i]);
    return res;
}
```

它还支持 64 位（2 字）整数，从而将总吞吐量提升至两倍：

```cpp
int popcnt_ll() {
    long long *b = (long long*) a;
    int res = 0;
    for (int i = 0; i < N / 2; i++)
        res += __builtin_popcountl(b[i]);
    return res;
}
```

通过使用融合加载的位计数指令与加法指令两条指令就可以实现这个算法。这两条指令都具备很高的吞吐量，因此代码每周期可处理约 $$8+8=16$$ 字节数据，瓶颈主要受限于该 CPU 的解码宽度 4。

这些指令大约在 2008 年随着 SSE4 指令集引入 x86 架构。让我们暂时回溯到向量化技术尚未普及的时代，尝试通过其他方式实现位计数功能。

最直接的方法是逐位遍历二进制字符串：

```cpp
__attribute__ (( optimize("no-tree-vectorize") ))
int popcnt() {
    int res = 0;
    for (int i = 0; i < N; i++)
        for (int l = 0; l < 32; l++)
            res += (a[i] >> l & 1);
    return res;
}
```

和预期一致，这样的实现只比每周期处理 $$1/8$$ 个字节好上一点点——大约 0.2。

我们可以尝试以字节为单位进行处理，而非逐位计算。具体方法是预先计算一个包含 256 个元素的表，对 256 个 8 比特的所有情况计算其 1 的数量，然后在遍历数组原始字节时查询该表：

```cpp
struct Precalc {
    alignas(64) char counts[256];

    constexpr Precalc() : counts{} {
        for (int m = 0; m < 256; m++)
            for (int i = 0; i < 8; i++)
                counts[m] += (m >> i & 1);
    }
};

constexpr Precalc P;

int popcnt() {
    auto b = (unsigned char*) a; // careful: plain "char" is signed
    int res = 0;
    for (int i = 0; i < 4 * N; i++)
        res += P.counts[b[i]];
    return res;
}
```

现在每个周期能处理约 2 个字节，如果切换到 16 位（`unsigned short`），更屎可以提高到 ~2.7 字节。

相较于 `popcnt` 指令，这个解决方案还是有点慢，不过现在它就可以被向量化了。这里我们不使用 `gather` 来加速获取，或是用另一种方法：将查找表压缩的足够小以至于可以放到寄存器里，然后使用 `pshufb` 来并行查找。

原始的 `pshufb` 在 128 位 SSE3 中被引入，它需要两个寄存器：包含 16 个字节值的查找表，以及一个由 16 个 4 比特索引（0 到 15）组成的向量，指定每个位置所查询的对应字节值的索引。而在 256 位 AVX2 中，并没有使用包含 32 字节查找表和笨拙的 5 位索引的方案，而是采用了一种在两个 128 位通道上独立执行相同混洗操作的指令。

在应用中，我们创建一个 16 字节的查找表，由于我们要使用 256 位的双通道指令，因此需要把表重复一遍：

```cpp
const reg lookup = _mm256_setr_epi8(
    /* 0 */ 0, /* 1 */ 1, /* 2 */ 1, /* 3 */ 2,
    /* 4 */ 1, /* 5 */ 2, /* 6 */ 2, /* 7 */ 3,
    /* 8 */ 1, /* 9 */ 2, /* a */ 2, /* b */ 3,
    /* c */ 2, /* d */ 3, /* e */ 3, /* f */ 4,

    /* 0 */ 0, /* 1 */ 1, /* 2 */ 1, /* 3 */ 2,
    /* 4 */ 1, /* 5 */ 2, /* 6 */ 2, /* 7 */ 3,
    /* 8 */ 1, /* 9 */ 2, /* a */ 2, /* b */ 3,
    /* c */ 2, /* d */ 3, /* e */ 3, /* f */ 4
);
```

现在要计算一个向量的 population count，我们可以把它的每个字节分成高位和低位，然后使用查找表统计数量。最后再小心地把它们求和起来：

```cpp
const reg low_mask = _mm256_set1_epi8(0x0f);

int popcnt() {
    int k = 0;

    reg t = _mm256_setzero_si256();

    for (; k + 15 < N; k += 15) {
        reg s = _mm256_setzero_si256();
        
        // 内部循环，防止 s 的水平求和超出 8 位的表示范围（255）
        for (int i = 0; i < 15; i += 8) {
            reg x = _mm256_load_si256( (reg*) &a[k + i] );
            
            // 取出每个 8 位整数的低 4 位和高 4 位
            // 因为查找表只能接受 4 位索引
            reg l = _mm256_and_si256(x, low_mask);
            // _mm256_srli_epi16 对每个 16 位整数进行逻辑右移操作
            reg h = _mm256_and_si256(_mm256_srli_epi16(x, 4), low_mask);

            reg pl = _mm256_shuffle_epi8(lookup, l);
            reg ph = _mm256_shuffle_epi8(lookup, h);

            s = _mm256_add_epi8(s, pl);
            s = _mm256_add_epi8(s, ph);
        }

        t = _mm256_add_epi64(t, _mm256_sad_epu8(s, _mm256_setzero_si256()));
    }

    int res = hsum(t);

    while (k < N)
        res += __builtin_popcount(a[k++]);

    return res;
}
```

现在的代码每周期可以处理约 30 字节。

`pshufb` 指令在某些 SIMD 算法中扮演着至关重要的角色，以至于提出该算法的 Wojciech Muła 甚至将其用作自己的推特账号名。实际上，位计数还能计算得更快：您可以查看他在 [GitHub](https://github.com/WojciechMula/sse-popcount) 上开源的各种向量化位计数实现库，以及他近期发表的[论文](https://arxiv.org/pdf/1611.07612)，其中对前沿技术方案进行了详细阐述。

### 排序和查找表

本章最后一个重要案例是 filter 操作。作为一种至关重要的数据处理原语，filter 操作接收数组作为输入，并按原始顺序仅输出满足特定谓词条件的元素。

在单线程标量场景下，只需通过维护一个在每次写入时递增的计数器即可轻松实现：

```cpp
int a[N], b[N];

int filter() {
    int k = 0;

    for (int i = 0; i < N; i++)
        if (a[i] < P)
            b[k++] = a[i];

    return k;
}
```

为了向量化这个过程，我们需要使用 `_mm256_permutevar8x32_epi32`。该函数接收一个向量，并通过索引向量逐个选择元素。尽管名称中含有“置换”字样，它并不进行真正的置换操作，而是通过复制元素来形成新的向量（因此结果中允许存在重复值）。

算法的核心思路如下：

1. 对数据向量计算谓词条件——本例中使用比较操作以获取掩码；
2. 使用 `movemask` 指令获取标量 8 位掩码；
3. 用该掩码索引查找表，该表返回一个置换模式，将满足谓词条件的元素按原始顺序移动到向量前端；
4. 使用 `_mm256_permutevar8x32_epi32` 内部函数执行置换操作；
5. 将整个置换后的向量写入缓冲区——尾部可能包含无效数据，但其前缀部分是正确的；
6. 计算标量掩码的位计数，并依此移动缓冲区指针。

首先，我们需要预先计算置换模式表。

```cpp
struct Precalc {
    alignas(64) int permutation[256][8];

    constexpr Precalc() : permutation{} {
        for (int m = 0; m < 256; m++) {
            int k = 0;
            for (int i = 0; i < 8; i++)
                if (m >> i & 1)
                    permutation[m][k++] = i;
        }
    }
};

constexpr Precalc T;
```

该表给出掩码 $$m$$ 下，第 $$k$$ 个保留下来的元素的索引是 $$i$$。随后我们就可以实现算法：

```cpp
const reg p = _mm256_set1_epi32(P);

int filter() {
    int k = 0;

    for (int i = 0; i < N; i += 8) {
        reg x = _mm256_load_si256( (reg*) &a[i] );
        
        reg m = _mm256_cmpgt_epi32(p, x);
        int mask = _mm256_movemask_ps((__m256) m);
        reg permutation = _mm256_load_si256( (reg*) &T.permutation[mask] );
        
        x = _mm256_permutevar8x32_epi32(x, permutation);
        _mm256_storeu_si256((reg*) &b[k], x);
        
        k += __builtin_popcount(mask);
    }

    return k;
}
```

向量化版本虽然需要一定的工作量才能实现，但其性能比标量版本快 6-7 倍（当 P 值极低或极高时，由于分支预测成功率提升，加速比会略有降低）。

该循环的性能仍相对有限——每次迭代耗时约 4 个 CPU 周期，这是因为在此特定 CPU（Zen 2）架构中，movemask、permute 和 store 指令的吞吐量较低，且都需经过相同的执行端口（P2）。在大多数其他 x86 CPU 上，预期性能可提升约2倍。

在 AVX-512 架构上，filter 操作还能实现显著加速：它提供专用的 [`compress`](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=7395,7392,7269,4868,7269,7269,1820,1835,6385,5051,4909,4918,5051,7269,6423,7410,150,2138,1829,1944,3009,1029,7077,519,5183,4462,4490,1944,1395&text=_mm512_mask_compress_epi32) 指令，可接收数据向量和掩码，并将未掩码元素连续写入。这对于依赖各种筛选子程序的算法（如快速排序）能产生巨大性能提升。

## 六 - 自动向量化和 SPMD

SIMD 并行化最常用于高度并行的计算场景：即对数组所有元素施加相同的逐元素运算并将结果写回其他位置的操作模式。在这种场景下，甚至无需了解 SIMD 的工作原理：编译器完全能够自行优化此类循环——开发者只需意识到这种优化机制的存在，并了解其通常能带来 5-10 倍的性能提升。

实际上，最主流的 SIMD 应用方式正是无为而治：依赖自动向量化。在许多情况下，考虑到代码简洁性与可维护性，坚持使用普通标量代码甚至更受推崇。

然而，对于一些看似容易向量化的循环，也常因某些技术细节而未能优化。对于这些场景，编译器可能需要程序员提供额外信息——毕竟程序员对具体问题的了解往往比静态分析所能推断的更为深入。

### 潜在的问题

考虑最开始的 “a+b” 样例：

```cpp
void sum(int *a, int *b, int *c, int n) {
    for (int i = 0; i < n; i++)
        c[i] = a[i] + b[i];
}
```

让我们想想在编译器视角下为什么这个函数不能向量化。

**数组大小**。若数组大小无法预先确定，可能导致数组规模过小使得向量化失去意义。即便数组足够大，我们也需要额外添加循环余数处理的标量计算部分，这会引入分支开销。

为消除这些运行时检查，应使用编译期常量作为数组大小，并最好将数组填充至 SIMD 块大小的整数倍。

**内存别名**。即便不考虑数组大小问题，这种类型的向量化也往往不可行。例如，数组 a 和 c 可能是同一个数组的只差 1 个元素的位置——毕竟程序员的意图难以揣测，比方说有人想通过这种卷积方式计算斐波那契数列。在这种情况下，SIMD 块中的数据将出现交叉，导致观察到的行为与标量情况不同。

当编译器无法证明函数可能被用于重叠数组时，它必须生成两个版本的实现——向量化版本与“安全”版本——并增加运行时检查来选择具体的版本。为避免这种情况，我们可以通过添加 `__restrict__` 关键字告知编译器内存区域不存在别名冲突。（Rust 不需要，因为 Rust 的引用检查提供了更强的保障。）

```cpp
void add(int * __restrict__ a, const int * __restrict__ b, int n) {
    for (int i = 0; i < n; i++)
        a[i] += b[i];
}
```

另一种特定于 SIMD 的方法是 “忽略向量依赖关系”（ignore vector dependencies）标记。这是通知编译器这个循环迭代之间没有依赖关系的通用方法：

```cpp
#pragma GCC ivdep
for (int i = 0; i < n; i++)
    // ...
```

**内存对齐**。编译器同样对这些数组的对齐情况一无所知，因此必须在向量化开始前先处理数组起始的部分元素，或者接受使用未对齐内存访问可能带来的性能损失。

为帮助编译器消除这种边界情况，我们可以 `alignas` 说明符标记静态数组，或者通过`std::assume_aligned` 函数标记指针的对齐。

**检查是否向量化**。在任何情况下，检查编译器是否按预期进行向量化都很有必要。可以将代码编译为汇编语言后查找以 “v” 开头的指令块，或添加 `-fopt-info-vec-optimized` 编译器标志，使编译器标注自动向量化的位置及使用的 SIMD 位宽。若将 `optimized` 替换为 `missed` 或 `all`，还能获取其他位置未发生向量化的原因说明。

虽然存在多种向编译器精确传达意图的[方法](https://www.intel.com/content/dam/develop/external/us/en/documents/31848-compilerautovectorizationguide.pdf)，但在特别复杂的情况下（例如循环内包含大量分支或函数调用时），更简便的做法是降低一个抽象层级，直接进行手动向量化。

### SPMD

在自动向量化与手动使用 SIMD 内部函数之间，存在一种精巧的折中方案：单程序多数据（SPMD）模型。这是一种编程人员编写看似常规串行程序，但实际在硬件上并行执行的计算模式。

其编程体验基本保持不变，但依然受限于计算必须具备数据并行性的根本约束。不过， SPMD 能确保无论编译器与目标 CPU 架构如何，向量化都会确定性地发生。它还能让计算自动跨多个核心并行化，在某些情况下甚至可卸载到其他类型的并行硬件上执行。

虽然现代编程语言（如 Julia）、多进程 API（如 OpenMP）以及专用编译器（如 Intel ISPC）都支持 SPMD 模型，但其最成功的应用领域仍是大规模并行问题与硬件高度匹配的 GPU 编程场景。

我们将在本书第二部分（有生之年）对这个计算模型进行更深入的探讨。
