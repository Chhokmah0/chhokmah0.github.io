---
title: CUDA 概念笔记
date: 2026-1-31 2:00:00 +0800
categories: [笔记, GPU]
tags: [GPU, CUDA]     # TAG names should always be lowercase
math: true
---

（先记录到 CUDA Programming Guide 的 2.2 结束，CUDA Programming Guide 比想象中的要多得多）

## 大概念

异构计算：多个不同的计算设备构成的系统。

CPU：中央处理器。擅长执行串行代码，拥有分支预测、指令流水线、指令重排等功能。

GPU：图形处理器。擅长执行并行代码，拥有大量的计算单元，并以单指令多线程（SIMT）的方式运行。现在的 GPU 由于可编程能力的增加，所计算的范围已经远远超出了“图形”的概念。

动态随机存取存储器 (DRAM)：即内存。CPU 的 DRAM 被称为系统内存或主机内存，GPU 的 DRAM 被称为显存或全局内存（因为 GPU 的所有 SM 都可以访问 GPU 的 DRAM）。

主机 (Host)：一般指 CPU。

设备 (Device)：一般指 GPU。

核函数 (Kernel)：指在 GPU 上运行的函数。也被叫做设备代码，只是习惯于称呼为核函数。核函数启动时需要得知 Grid 和 Block 大小，同时还有一些可选的集群大小、流和 SM 配置设置。CPU 启动核函数是异步的，也就是说 CPU 启动核函数后，不会等待 GPU 运行结束。最基本的版本是通过 `cudaDeviceSynchronize()` 来同步。

SIMT：单指令多线程，多个线程同时执行一条指令来并行处理多组数据。

SIMD：单指令多数据，使用一条并行处理多组数据的指令。

## 硬件概念

图形处理集群 (Graphics Processing Clusters, GPCs)：GPC 中包含多个 SM。从 CC 9.0 的 GPU 开始有这一概念。

流式多处理器 (Streaming Multiprocessors, SMs)：SM 中包含一个本地寄存器页（存储线程的局部变量）、一个同一数据缓存（L1 缓存 or 共享内存，存储一个 Block 的共享变量）、一个常量缓存 (Constant Cache)、大量 CUDA Core (计算单元，旧称 SP)。可以看到 CUDA Core 并不像 CPU Core 一样包含各种功能，其缓存、寄存器、调度功能都是由 SM 中别的硬软件同一管理。SM 是调度 Block 的基本单位。一个 SM 上能加载的 Block 数量取决于“SM 最大能调度的 Block 数量”、“SM 最大可运行的线程数量”、“SM 的共享内存限制”、“SM 的寄存器限制”，对于不同 GPU 的可能需要进行特殊对待才能充分利用 SM 的性能，可以使用 `cudaGetDeviceProperties` 获得这些信息。如果考虑优化一个 SM 上加载的线程数量，则还需要考虑 Block 本身的“共享内存限制”、“寄存器限制”、“最大线程数限制”。

全局内存：GPU 的 DRAM，也就是显存。

共享内存：同一个 Block 中的所有线程都可以访问的资源。和 L1 cache 一块分配在统一数据缓存上。共享内存使用得越多，L1 cache 的资源也会越少。CUDA 支持一个核函数动态地分配“一个”共享内存。

寄存器：每个 SM 都有一个寄存器页，用于放置每个线程的寄存器。NVCC 允许通过 `-maxrregcount` 修改一个 SM 可以使用的寄存器数量，这允许一个 SM 中调度更多的 Block，但是也会导致更多的寄存器溢出（寄存器不够用了）。

本地内存：本地内存是类似于寄存器的线程本地存储，由 NVCC 管理，但是本地内存的物理位置在全局内存空间中。“本地”指的是其逻辑范围（单个线程），而不是其物理位置。当：

- 无法确定一个数组是否总是使用编译期常量进行索引
- 过大的结构体或数组
- 寄存器溢出

时，会将数据放在本地内存中。由于本地内存在全局内存中，因此也有合并访存优化。好消息是，NVCC 会将连续的线程的数据每 32bit 放入本地内存中，这意味着只要 warp 中的所有线程访问相同的相对地址，例如数组变量中的相同索引或结构变量中的相同成员，访问就可以完全合并。

常量内存：常量内存可以被 GPU 的各处读取到，且具有和整个应用相同的生命周期。常量内存只能由 Host 修改 (`cudaMemcpyToSymbol()`)，对核函数是只读的。每个设备上都有一个常量内存且较小，一般只有 64KB。

纹理内存：老的 GPU 使用纹理内存会带来加载/存储上的优势。现在已经无所谓了，直接加载/存储即可。

分布式共享内存：同一个 Cluster 内的线程可以访问所在的图形处理集群 (GPC) 的共享内存，原理是使用 `cluster.map_shared_rank(smem, dst_block_rank)` 将本地的 `smem` 地址转化为目标的共享内存地址。使用 `cluster.sync()` 可以保证同一个 Cluster 的 Block 都同步，一方面防止 Block 还没有开始运行，另一方面某个 Block 卸载了自己的共享内存。分布式共享内存可以在直方图统计优化上使用，如果直方图的大小超过了 Block 的共享内存，就可以通过 Cluster 的分布式共享内存申请更大的共享内存空间放置直方图，统计整个 Cluster 的数据。如果再大的话那也没招了，用全局内存 or 分段直方图多遍历几次吧。

L2 cache：GPU 上的 cache，供给所有 SM 使用。

L1 cache：SM 里的 cache，供给单个 SM 使用。和共享内存一起分布在统一数据缓存中。L1 和 L2 cache 的缓存行为都可以通过函数来控制。如果不使用这些函数，编译器和运行时会尽可能利用这些 cache。

Warp Scheduler：SM 中调度 Warp 的调度单元，会将 Warp 在 32 个 CUDA Core 上进行调度，当一个 Warp 阻塞时会换一个 Warp。

PCIe or NVLINK：GPU 和 CPU 之间的总线，用于传输数据。

## 模型概念

Grid：包含多个 Block。启动一个核函数时就会生成一个 Grid，其中包含许多 Block，Block 中又包含许多线程。Grid 可以无限大，可以是 1、2、3 维。

Cluster：包含多个 Block。是 Grid 中 Block 的一个分组。也可以是多维的。Cluster 与 GPC 对应，从而使得 GPC 中的大量 Block 在 Grid 中是相邻的。由于这些 Block 同处于一个 GPC，因此同一个集群的不同块可以同步、可以用  Cooperative Group 接口进行通讯。同一个集群的线程可以访问集群中任意 Block 的共享内存，这叫分布式共享内存。集群的最大大小取决于硬件，与具体的设备有关。使用了 Cluster 的函数需要标记 `__cluster_dims__(X,Y,Z)` 或者使用 `cudaLaunchKernelEx` 调用。

Block：包含多个线程。Block 是 SM 的调度单位，一个 SM 可以同时调度多个（个位数大小）Block。Block 可以是 1、2、3 维。一个 SM 上能加载的 Block 数量取决于“SM 最大能调度的 Block 数量”、“SM 最大可运行的线程数量”、“SM 的共享内存限制”、“SM 的寄存器限制”，对于不同 GPU 的可能需要进行特殊对待才能充分利用 SM 的性能。如果考虑优化一个 SM 上加载的线程数量，则还需要考虑 Block 本身的“共享内存限制”、“寄存器限制”、“最大线程数限制”。由于 Block 内部的线程在同一个 SM 中运行，这允许这些线程共享 shared memory、进行同步 (`__syncthreads()`)。

Warp：包含 32 个线程，以单指令多线程的方式运行，相互之间自然是同步的。Block 中每 32 个线程组织成一个 Warp，这要求 Block 的大小最好是 32 的倍数。

## CUDA 平台概念

计算能力 (Compute Capability, CC)：每个 NVIDIA GPU 都有个 CC 编号，用于表示 GPU 支持哪些功能，以及设置一些硬件参数。CC 编号直接与 SM 的版本号对应，例如 CC 12.0 的 GPU 具有 `sm_120` 的 SM。

NVIDIA 驱动：可以被认为是GPU的操作系统，必须安装在主机系统的操作系统上。除了 CUDA 之外，NVIDIA Driver 还提供了使用 GPU 的所有其他方法，例如 Vulkan 和 Direct3D。

CUDA Toolkit：一组库、头文件、以及用于编写、构建、分析 CUDA 软件的工具。

CUDA 兼容性：GPU、NVIDIA 驱动、CUDA 工具集之间应该相互兼容。

CUDA API：CUDA 运行时 API 基于 CUDA 驱动 API 开发的。理论上，所有代码都可以使用 CUDA 驱动 API 来完成。有一些特性只能通过驱动 API 来利用。

CUDA 运行时：CUDA 运行时会对每一个 device 创建一个 CUDA 上下文，这个上下文对所有的 Host 线程可见。因此你可以通过 `cudaSetDevice` 来让多线程程序使用不同的 GPU，通过 `cudaInitDevice` 手动创建某个 GPU 的上下文从而保证正确的计时。（在 CUDA 12.0 后，`cudaSetDevice` 会自动初始化对应的 GPU）

```cpp
// Runtime API 示例代码 - 更简洁
cudaMalloc(&devPtr, size);
cudaMemcpy(devPtr, hostPtr, size, cudaMemcpyHostToDevice);
kernel<<<grid, block>>>(devPtr, N);
cudaDeviceSynchronize();

// Driver API 示例代码 - 更底层但更灵活
cuInit(0);
cuDeviceGet(&device, 0);
cuCtxCreate(&context, 0, device);
cuModuleLoad(&module, "kernel.ptx");
cuModuleGetFunction(&kernel, module, "myKernel");
cuLaunchKernel(kernel, ...);
```

Parallel Thread Execution (PTX)：CUDA 平台上平时基本不可见的层，PTX 是一个虚拟指令集，是 NVIDIA GPU 上的一个 high-level 的汇编语言。作为一个中间表示，这使得除了 nvcc 外，也可以使用其它编译器来编译 CUDA 程序的 PTX，然后通过 JIT 编译工具运行。PTX 版本也对应着计算能力，例如 `compute_80` 对应 CC 8.0。

Cubin：CUDA 二进制文件。对于特定的 SM 版本，cubin 有特殊的格式，例如 `sm_120`。

Fatbin：Fatbin 可以包含多个不同 SM 版本的 cubin 和多个不同 PTX 版本的 PTX。因此一个可执行程序或者库里面会包含 CPU 二进制代码和一个 fatbin。

二进制兼容性：在同一个大版本内（例如 8.x），CC 版本的 GPU 可以加载 SM 版本等于或小于 CC 版本的 cubin。不同大版本之间不能兼容。

PTX 兼容性：PTX 可以在运行时被 JIT 编译为 SM 版本等于或更高的 cubin。

JIT (just-in-time) 编译：运行时加载 PTX 代码会将其编译成二进制代码。JIT 会保存编码的程序以便后续使用。虽然牺牲了一些加载的耗时，但是 JIT 技术允许一个程序在当时并未出现的新设备上运行。

## 编程概念

统一内存 (Unified Memory)：CUDA 的一个特性叫做统一内存，它允许应用程序分配可以从 CPU 和 GPU 访问的内存。CUDA 运行时或底层硬件可以在需要时访问或重新定位数据到正确的位置（数据复制依然存在）。即使使用统一内存，通过保持内存迁移到最小并尽可能多地访问直接连接到内存所在的处理器的数据，也可以获得最佳性能。按需迁移也使得 GPU 的显存压力可以缓解（虚拟内存的感觉）。

### 函数说明符

`__global__`：在 GPU 上，只能由 CPU 调用的函数。即核函数。

`__host__`：在 CPU 上，由 CPU 调用的函数，默认的函数就是 `__host__` 的。

`__device__`：在 GPU 上，只能被 GPU 调用的函数。只能由 GPU 中的单个线程调用，一般是辅助计算的函数。

### 变量说明符

`__device__`：标记变量存储在全局内存。

`__constant__`：标记变量存储在常量内存。

`__managed__`：标记变量作为统一内存存储。

`__shared__`：标记变量存储在共享内存。

当一个 `__device__` 或 `__global__` 函数中的变量没有被说明符标识时，变量优先会被分配到寄存器，只在必要时被分配到本地内存。其他地方声明的变量默认放到系统内存里。

### 错误检查

宏：在实际生产中经常被使用的宏定义：

```cpp
#define CUDA_CHECK(expr_to_check) do {            \
    cudaError_t result  = expr_to_check;          \
    if(result != cudaSuccess)                     \
    {                                             \
        fprintf(stderr,                           \
                "CUDA Runtime Error: %s:%i:%d = %s\n", \
                __FILE__,                         \
                __LINE__,                         \
                result,\
                cudaGetErrorString(result));      \
    }                                             \
} while(0)
```

`cudaError_t`：CUDA 运行时为每一个 Host 线程维护一个 `cudaError_t`，这个变量默认是 `cudaSuccess`。`cudaGetLastError` 可以获取当前错误状态并重置为 Success。`cudaPeekLastError` 则不重置。使用 `<<<>>>` 执行的核函数不会返回 `cudaError_t`，因此需要通过这种方式进行错误处理。

异步错误：CUDA 内核启动和许多运行时 api 都是异步的，这使得它们的错误仅在下一次错误检查时返回。例如 `cudaGetLastError`、`cudaPeekLastError` 或者某个返回 `cudaError_t` 的 CUDA API。CUDA API 返回错误时不会清除错误状态。

`CUDA_LOG_FILE`：当设置此环境变量时，CUDA 驱动程序将把遇到的错误消息写入环境变量中指定路径的文件中。会输出比 `CUDA_CHECK` 能获取到的更多更详细的信息。同时对于没有进行错误处理的代码，`CUDA_LOG_FILE` 也是一种不需要大幅度修改代码就能进行调试的手段。

## 性能优化

线程束分化 (Warp Divergence)：由于一个 Warp 是以 SIMT 运行的，因此当 Warp 中的线程在某个条件分支中的判定不一致，就会使得 Warp 不得不将两个分支的指令都运行一遍。优化需要保证一个 Warp 中的线程尽可能执行相同的路径。

合并访存：GPU 访问全局内存时以 32 字节为单位发送事务请求。当一个 Warp 的线程们访问全局内存中相邻的连续数据时，就可以充分利用返回的数据。例如一个 Warp 访问 `int` 数组的连续 32 个索引时，只需要通过四个事务请求就可以完成。在实现中优化时，需要保证一个 Warp 中的线程尽可能同时访问全局内存的相邻位置，按照线性顺序、按照某个排列都是可以的，重点在“局部”上。

Bank Conflict：GPU 访问共享内存时由 bank 代理。每个 bank 的带宽都是 32 bit，共享内存以 32 bit 为单位按顺序循环分配给 bank。当一个 warp 中的不同线程访问同一个 bank 的不同 32 bit 地址时，就会发生 bank 冲突。但是，如果同一个 warp 中的不同线程访问的是同一个 bank 的同一个地址，此时会进行广播，因此没有 bank 冲突。

二维访问消除 Bank Conflict：在二维数组的情况下，经常需要让一个 warp 横向和纵向访问连续的索引，然而当纵向访问时会产生大量的 Bank Conflict，这就需要通过以下手段消除（以下都以 4 字节/32 位内存为基本单位）：

- 填充。通过填充适当的字节，让列上连续的 32 个索引分散到 32 个 Bank 中。假设每行之间的差距为 $$a \pmod 32$$，则需要填充 $$b = q - a \pmod {32}$$ 个 4 字节，其中 $$q$$ 和 $$32$$ 互素，才能使得按列访问时，访问的 32 位数据分散到不同的 bank 中。（此时对于任意 $$c$$ 和连续的 32 个 $$i$$，$$(c+i(a+b)) \bmod 32 = (c+iq) \bmod 32$$ 各不相同）考虑到 $$32=2^5$$，这其实意味着当 $$a$$ 为偶数时，列访问就会发生 Bank Confilct，此时应该将行填充到下一个奇数，这也是为什么填充经常是 1。
- Swizzled Layout。将列索引与行号进行异或。这个方法申请的内存大小不变，可以让内存保持比 4 字节更大的内存对齐。缺点是逻辑上相邻的数据在共享内存中不一定相邻，代码库可能缺少对这种布局的支持。异或后，由于行访问时的列索引不同，而它们会一起异或相同的值，因此仍属于不同的 Bank。如果列访问的索引在原本二维布局的情况下会访问相同的 Bank，此时会异或上连续的不同行号，因此恰好访问不同的 32 个 Bank。在更一般的情况下，假设每行的 4 字节数量为 $$a \pmod {32}$$。此时逻辑索引 $$(r,c)$$ 应当对应为内存 $$(r,c+\left\lfloor\frac{r \gcd(a,32)}{32}\right\rfloor)$$ 的内存地址。由于 $$\frac{32}{\gcd(a,32)}$$ 肯定是二的幂次，这个转换操作可以通过一次右移完成。

注：Swizzled Layout 是相对于行优先布局（Row-Major Layout）而言的。

占用率：SM 中活动的 warp 和实际最大可运行的 warp 数量之比。对于不同 GPU 需要特别设置 Block 中的线程数量才能充分利用 SM 的性能，可以使用 `cudaGetDeviceProperties` 获得这些信息。基本上取决于“SM 最大能调度的 Block 数量”、“SM 最大可运行的线程数量”、“SM 的共享内存限制”、“SM 的寄存器限制”、Block 本身的“共享内存限制”、“寄存器限制”、“最大线程数限制”。除了 Block 的线程数外，程序员还可以通过 `--maxrregcount` 设置每个线程块所使用的寄存器数量来调整占用率。

隐式同步：来自不同阻塞流的两个操作，当它们的提交时间中间存在一个提交到默认流的 CUDA 操作时，这两个操作不能同步运行。这意味着在编程中应该遵循以下守则：

- 所有的独立操作应该在存在依赖的操作之前提交。
- 任何形式的同步都应该尽可能地往后放置。
