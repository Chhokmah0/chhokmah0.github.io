---
title: CUDA 异步执行
date: 2026-2-4 12:00:00 +0800
categories: [笔记, GPU]
tags: [GPU, CUDA]     # TAG names should always be lowercase
math: true
---

CUDA 允许以下任务并行执行：

- 主机上的计算
- 设备上的计算
- 从主机到设备的数据传递
- 从设备到主机的数据传递
- 在设备内进行数据传递
- 在设备间进行数据传递

一般异步接口会提供三种方法：

- 阻塞。`cudaDeviceSynchronize()` 就是一个阻塞接口。
- 非阻塞，或者叫轮询方法。通过调用一个函数查看异步任务的完成情况。
- 回调。当异步任务完成后执行回调函数。

## CUDA Streams

总的来说，CUDA stream 是对一系列操作的抽象。CUDA 运行时会自动从多个流中找出能执行的任务执行。CUDA 有一个默认流，如果没有在函数中做任何设置，就会将任务塞到这个默认流中。

### 流的启动与摧毁

```cpp
cudaStream_t stream;        // Stream handle
cudaStreamCreate(&stream);  // Create a new stream

// stream based operations ...

cudaStreamDestroy(stream);  // Destroy the stream
```

### 在流中启动核函数

```cpp
kernel<<<grid, block, shared_mem_size, stream>>>(...);
```

可以通过 stream 设置在哪个流中启动，如之前所说，如果不设置使用的就是默认流。

### 在流中启动内存传递

```cpp
// Copy `size` bytes from `src` to `dst` in stream `stream`
cudaMemcpyAsync(dst, src, size, cudaMemcpyHostToDevice, stream);
```

同上，也是通过 `stream` 设置。为了能够异步执行，CPU 缓存必须被固定且分页锁定，否则 `cudaMemcpyAsync()` 会退化为阻塞版本。使用 `cudaMallocHost()` 来分配页锁定缓冲区可以让更好地与 GPU 传递数据。

分页锁定内存是稀缺资源，应该只用于频繁传输的数据缓冲区，不应过度使用。

### 流同步

- 阻塞的 `cudaStreamSynchronize()`
- 非阻塞的 `cudaStreamQuery()`

```cpp
// Wait for the stream to be empty of tasks
cudaStreamSynchronize(stream);

// At this point the stream is done
// and we can access the results of stream operations safely
```

```cpp
// Have a peek at the stream
// returns cudaSuccess if the stream is empty
// returns cudaErrorNotReady if the stream is not empty
cudaError_t status = cudaStreamQuery(stream);

switch (status) {
    case cudaSuccess:
        // The stream is empty
        std::cout << "The stream is empty" << std::endl;
        break;
    case cudaErrorNotReady:
        // The stream is not empty
        std::cout << "The stream is not empty" << std::endl;
        break;
    default:
        // An error occurred - we should handle this
        break;
};
```

## CUDA Events

CUDA event 是流中的事件标记，可以通过 event 查询流中的任务是否完成到了这一步。如果没有 event 的话，我们就只能判断流是不是空的来进行同步。

CUDA 流还会保留时间信息，可以用来计时内核启动和内存传输。

### 事件的启动与摧毁

```cpp
cudaEvent_t event;

// Create the event
cudaEventCreate(&event);

// do some work involving the event

// Once the work is done and the event is no longer needed
// we can destroy the event
cudaEventDestroy(event);
```

### 在流中插入事件

```cpp
// Insert the event into the stream
cudaEventRecord(event, stream);
```

### 通过流进行计时

CUDA 事件可用于计算包括内核在内的各种流操作的执行时间。当事件到达流的前端时，它会记录一个时间戳，因此可以通过在某个操作前后插入事件来计时。

```cpp
cudaStream_t stream;
cudaStreamCreate(&stream);

cudaEvent_t start;
cudaEvent_t stop;

// create the events
cudaEventCreate(&start);
cudaEventCreate(&stop);

 // record the start event
cudaEventRecord(start, stream);

// launch the kernel
kernel<<<grid, block, 0, stream>>>(...);

// record the stop event
cudaEventRecord(stop, stream);

// wait for the stream to complete
// both events will have been triggered
cudaStreamSynchronize(stream);

// get the timing
float elapsedTime;
cudaEventElapsedTime(&elapsedTime, start, stop);
std::cout << "Kernel execution time: " << elapsedTime << " ms" << std::endl;

// clean up
cudaEventDestroy(start);
cudaEventDestroy(stop);
cudaStreamDestroy(stream);
```

### 通过事件进行同步

同样的：

- 阻塞的 `cudaEventSynchronize()`
- 非阻塞的 `cudaEventQuery()`

```cpp
cudaEvent_t event;
cudaStream_t stream;

// create the stream
cudaStreamCreate(&stream);

// create the event
cudaEventCreate(&event);

// launch a kernel into the stream
kernel<<<grid, block, 0, stream>>>(...);

// Record the event
cudaEventRecord(event, stream);

// launch a kernel into the stream
kernel2<<<grid, block, 0, stream>>>(...);

// Wait for the event to complete
// Kernel 1 will be  guaranteed to have completed
// and we can launch the dependent task.
cudaEventSynchronize(event);
dependentCPUtask();

// Wait for the stream to be empty
// Kernel 2 is guaranteed to have completed
cudaStreamSynchronize(stream);

// destroy the event
cudaEventDestroy(event);

// destroy the stream
cudaStreamDestroy(stream);
```

也可以使用非阻塞的方式，如下代码的 CPU 不断进行一个工作循环，偶尔检查一下任务是否完成：

```cpp
cudaEvent_t event;
cudaStream_t stream1;
cudaStream_t stream2;

size_t size = LARGE_NUMBER;
float *d_data;

// Create some data
cudaMalloc(&d_data, size);
float *h_data = (float *)malloc(size);

// create the streams
cudaStreamCreate(&stream1);   // Processing stream
cudaStreamCreate(&stream2);   // Copying stream
bool copyStarted = false;

//  create the event
cudaEventCreate(&event);

// launch kernel1 into the stream
kernel1<<<grid, block, 0, stream1>>>(d_data, size);
// enqueue an event following kernel1
cudaEventRecord(event, stream1);

// launch kernel2 into the stream
kernel2<<<grid, block, 0, stream1>>>();

// while the kernels are running do some work on the CPU
// but check if kernel1 has completed because then we will start
// a device to host copy in stream2
while ( not allCPUWorkDone() || not copyStarted ) {
    doNextChunkOfCPUWork();

    // peek to see if kernel 1 has completed
    // if so enqueue a non-blocking copy into stream2
    if ( not copyStarted ) {
        if( cudaEventQuery(event) == cudaSuccess ) {
            cudaMemcpyAsync(h_data, d_data, size, cudaMemcpyDeviceToHost, stream2);
            copyStarted = true;
        }
    }
}

// wait for both streams to be done
cudaStreamSynchronize(stream1);
cudaStreamSynchronize(stream2);

// destroy the event
cudaEventDestroy(event);

// destroy the streams and free the data
cudaStreamDestroy(stream1);
cudaStreamDestroy(stream2);
cudaFree(d_data);
free(h_data);
```

## 流回调函数

CUDA 提供了一种通过流在主机上运行函数的方式。`cudaLaunchHostFunc()`。虽然还有一个叫 `cudaAddCallback()` 的，但是已经被弃用。

```cpp
cudaError_t cudaLaunchHostFunc(cudaStream_t stream, void (*func)(void *), void *data);
```

主机函数的签名应该为：

```cpp
void hostFunction(void *data);
```

可以通过 `void *data` 任意解释传入的数据。在回调函数中不应该使用任何 CUDA API。流回调函数（旧时 `cudaAddCallback()` 的概念）和这个主机函数在后文统称为主机回调函数。

CUDA 运行时会维护一个线程池用于运行这些回调函数，不会阻塞主线程。为了保证统一内存能在回调函数里正常使用，CUDA 提供如下保证：

- 主机回调函数执行期间，所在的流被视为空闲。从而回调函数始终可以使用所在流所绑定的内存。
- 主机回调函数开始执行的效果，等同于放入一个 event，并通过 event 同步，即回调函数任务在流中是顺序执行的。
- 向任何流添加核函数不会使该流立即变成活动状态。只有这个流之前的所有的主机回调函数都已经执行完毕才会开始活动。这样保证 GPU 和 CPU 不会同时修改数据。
- 主机回调函数结束时，如果流的后面有核函数才会继续执行。换句话说，连续的主机回调函数并不会激活流，只有核函数才会激活流让 GPU 开始并行它。

### 异步错误处理

在 cuda 流中，错误可能源于流中的任何操作，包括内核启动和内存传输。在流被同步之前 (`cudaStreamSynchronize()`)，这些错误可能不会在运行时传播回用户。除了同步外还有两种方式：

- `cudaGetLastError()`：获取当前的错误状态并重置。如果 `cudaGetLastError()` 返回 `cudaSuccess`，说明两次获取状态之间没有发生错误。
- `cudaPeekAtLastError()`：获取当前的错误状态且不重置。

在调试时，可以通过设置 `CUDA_LAUNCH_BLOCKING=1` 来让每次核函数结束时都进行同步，从而方便定位错误位置。缺点是运行变慢和掩盖了异步错误。

## CUDA 流顺序

在某些特殊情况下，为了性能优化的目的，流的全序语义可能被放宽。例如：

- 在程序化依赖内核启动 (Programmatic Dependent Kernel Launch) 场景中，通过使用特殊属性和内核启动机制，可以实现两个内核的部分重叠执行；
- 在使用 `cudaMemcpyBatchAsync()` 函数进行批量内存传输时，若 CUDA 运行时能够并发执行非重叠的批量复制，则可能允许操作重叠。

## 阻塞流、非阻塞流、默认流

CUDA 中的阻塞流和非阻塞流只表示这些流和默认流之间的同步方式，并不是说非阻塞流的内部任务就不同步了。默认情况 (`cudaStreamCreate()`) 下重建的都是阻塞流，除非特别设置：

```cpp
cudaStream_t stream;
cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);
```

### 传统默认流

默认流，也被称为 NULL 流或者编号为 0 的流。默认流与其它的阻塞流同步。例如：

```cpp
cudaStream_t stream1, stream2;
cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);

kernel1<<<grid, block, 0, stream1>>>(...);
kernel2<<<grid, block>>>(...);
kernel3<<<grid, block, 0, stream2>>>(...);

cudaDeviceSynchronize();
```

`kernel2` 在阻塞流中，与阻塞流中的核函数 `kernel1` 和 `kernel3` 都同步，导致 `kernel1` 和 `kernel3` 虽然不在同一个流中，但是也产生了隐式同步。

通过创建非阻塞流就可以避免这种问题，默认流不会与非阻塞流进行同步，从而三个核函数之间没有任何同步机制。

```cpp
cudaStream_t stream1, stream2;
cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking);
cudaStreamCreateWithFlags(&stream2, cudaStreamNonBlocking);

kernel1<<<grid, block, 0, stream1>>>(...);
kernel2<<<grid, block>>>(...);
kernel3<<<grid, block, 0, stream2>>>(...);

cudaDeviceSynchronize();
```

### 每线程默认流

从 CUDA-7 开始，CUDA 允许每个主机线程拥有自己独立的默认流，而不是共享的传统默认流。为了启用此行为，必须使用 nvcc 的编译选项 `--default-stream per-thread` 或者定义 `CUDA_API_PER_THREAD_DEFAULT_STREAM` 宏。

一个比较明显的用处是，在多线程编程环境下，多个线程之间往往不需要传统默认流这么多的同步。如果使用传统默认流，就不得不给每个线程都创建一个非阻塞流，这会加重编程的思维负担（流的释放，同步的管理等）。

## 显式同步

- `cudaDeviceSynchronize()`：等待所有主机线程上的所有流在这条同步指令之前的所有指令完成。
- `cudaStreamSynchronize()`：等待某个流在这条同步指令之前的所有指令完成。这允许其他流在设备上继续运行。
- `cudaStreamWaitEvent()`：以一个流和一个事件为参数，使该流中所有在该同步指令之后加入流中的任务延迟到事件结束才能执行。`cudaStreamWaitEvent()` 不会阻塞主机。
- `cudaStreamQuery()`：查询流的所有任务是否全部完成。

## 隐式同步

来自不同阻塞流的两个操作，当它们的提交时间中间存在一个提交到默认流的 CUDA 操作时，这两个操作不能同步运行。

这意味着在编程中应该遵循以下守则：

- 所有的独立操作应该在存在依赖的操作之前提交。
- 任何形式的同步都应该尽可能地往后放置。

## 杂项

### 流优先级

开发人员可以通过 `cudaStreamCreateWithPriority()` 给创建的流设置优先级，较低的数字有着较高的优先级，可以使用 `cudaDeviceGetStreamPriorityRange()` 查询某个设备的优先级范围。

默认创建的流的优先级为 0。

优先级只是对运行时的提示，不保证任何顺序。

### CUDA 图简介与流捕获

CUDA 流可以通过事件进行同步，整体的依赖呈现为一个有向无环图 (DAG)。在实践中，某些应用会反复执行同一个 DAG。通过捕获或创建 DAG 所对应的 CUDA 图，就能减少主机线程重复调用同一串 API 带来的延迟和开销。

使用 CUDA 图的工作流程如下：

- 图的捕获：可以由软件完成该步骤，且仅会在 CUDA 图首次执行时进行一次。也可以手动通过 CUDA 图的专用 API 构建图结构。
- 图的实例化：在图捕获完成后执行一次该步骤。这个步骤会初始化执行 CUDA 图所需的各类运行时结构，确保图中各组件的启动效率达到最优。
- 图的重复执行：由于执行图中操作所需的所有运行时结构均已提前就绪，CUDA 图执行过程中的 CPU 开销能被降至最低。

一个简单的例子为：

```cpp
#define N 500000 // tuned such that kernel takes a few microseconds

// A very lightweight kernel
__global__ void shortKernel(float * out_d, float * in_d){
    int idx=blockIdx.x*blockDim.x+threadIdx.x;
    if(idx<N) out_d[idx]=1.23*in_d[idx];
}

bool graphCreated=false;
cudaGraph_t graph;
cudaGraphExec_t instance;

// The graph will be executed NSTEP times
for(int istep=0; istep<NSTEP; istep++){
    if(!graphCreated){
        // 1. Capture the graph
        cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);

        // Launch NKERNEL kernels
        for(int ikrnl=0; ikrnl<NKERNEL; ikrnl++){
            shortKernel<<<blocks, threads, 0, stream>>>(out_d, in_d);
        }

        // End the capture
        cudaStreamEndCapture(stream, &graph);
        
        // 2. Instantiate the graph
        cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);
        graphCreated=true;
    }

    // 3. Launch the graph
    cudaGraphLaunch(instance, stream);

    // Synchronize the stream
    cudaStreamSynchronize(stream);
}
```

对 CUDA 图更加详细的介绍请参考之后的笔记。
